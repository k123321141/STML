{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285789, 256) (88074, 256)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# emb\n",
    "with open(join('./','GCN.emb')) as f:\n",
    "# with open(join('./','t1.emb')) as f:\n",
    "    num_nodes, D = f.readline().strip().split(' ')\n",
    "    num_nodes = int(num_nodes)\n",
    "    D = int(D)\n",
    "    \n",
    "    ls = f.readlines()\n",
    "node_emb_dict = {}\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    node_id, emb = int(buf[0]), buf[1:]\n",
    "    x = np.asarray([float(i) for i in emb], dtype=np.float32)\n",
    "    node_emb_dict[node_id] = x\n",
    "    \n",
    "# training data\n",
    "with open(join(data_path,'t1-merge.txt')) as f:\n",
    "    ls = f.readlines()\n",
    "node_set = get_node_set(join(data_path,'t1-merge.txt'))\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "X = []\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    src, dst = int(buf[0]), int(buf[1])\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "    fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "    X.append(fea)\n",
    "X = np.vstack(X)\n",
    "\n",
    "# test data\n",
    "with open(join(data_path,'t1-test.txt')) as f:\n",
    "    ls = f.readlines()\n",
    "N2 = len(ls)\n",
    "test_X = []\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    src, dst = int(buf[0]), int(buf[1])\n",
    "    if src not in node_emb_dict:\n",
    "        src = 37019\n",
    "    if dst not in node_emb_dict:\n",
    "        dst = 37482\n",
    "    fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "    \n",
    "    test_X.append(fea)\n",
    "test_X = np.vstack(test_X)\n",
    "print(X.shape, test_X.shape)\n",
    "# print 'done'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256) (256, 1)\n",
      "(140, 256) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 128\n",
    "def naive_bootsrap_generator(X, adj_mat, idx_map, node_emb_dict, train_node_set, batch_size=128, neg_rate=1. ):\n",
    "    train_node_list = list(train_node_set)\n",
    "    train_N = len(train_node_list)\n",
    "    num_edge = X.shape[0]\n",
    "        \n",
    "    while True:\n",
    "        idx = np.random.choice(num_edge, batch_size)\n",
    "        pos_X = X[idx, :]\n",
    "        \n",
    "        neg_count = int(batch_size*neg_rate)\n",
    "        neg_idx = np.random.randint(train_N, size=[neg_count, 2])\n",
    "        neg_X = []\n",
    "        for i in range(neg_count):\n",
    "            src, dst = neg_idx[i]\n",
    "            src = train_node_list[src]\n",
    "            dst = train_node_list[dst]\n",
    "            if src != dst and adj_mat[idx_map[src], idx_map[dst]] == 0:\n",
    "                fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "                neg_X.append(fea)\n",
    "        neg_X = np.vstack(neg_X)\n",
    "\n",
    "        ret_X = np.vstack([pos_X, neg_X])\n",
    "        ret_Y = np.zeros([ret_X.shape[0], 1])\n",
    "        ret_Y[:batch_size, 0] = 1\n",
    "        yield ret_X, ret_Y\n",
    "\n",
    "N = X.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "train_idx = idx[N//10:]\n",
    "val_idx = idx[:N//10]\n",
    "\n",
    "train_X = X[train_idx,:]\n",
    "val_X = X[val_idx,:]\n",
    "\n",
    "train_node_set = get_node_set('./kaggle/t1-train.txt')\n",
    "G = naive_bootsrap_generator(train_X, adj_mat, idx_map, node_emb_dict, train_node_set, batch_size=batch_size)\n",
    "val_G = naive_bootsrap_generator(val_X, adj_mat, idx_map, node_emb_dict, train_node_set,batch_size=batch_size, neg_rate=0.1)\n",
    "x,y = next(G)\n",
    "print x.shape,y.shape\n",
    "x,y = next(val_G)\n",
    "print x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256) (256, 1)\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.8518 - acc: 0.5117\n",
      "Epoch 1/1000\n",
      "2009/2009 [==============================] - 9s 5ms/step - loss: 0.6458 - acc: 0.6301 - val_loss: 0.6623 - val_acc: 0.5192\n",
      "Epoch 2/1000\n",
      "2009/2009 [==============================] - 9s 5ms/step - loss: 0.6397 - acc: 0.6397 - val_loss: 0.6148 - val_acc: 0.4992\n",
      "Epoch 3/1000\n",
      "2009/2009 [==============================] - 9s 5ms/step - loss: 0.6339 - acc: 0.6466 - val_loss: 0.6229 - val_acc: 0.5274\n",
      "Epoch 4/1000\n",
      "2009/2009 [==============================] - 9s 5ms/step - loss: 0.6300 - acc: 0.6512 - val_loss: 0.6469 - val_acc: 0.5906\n",
      "Epoch 5/1000\n",
      "2009/2009 [==============================] - 9s 5ms/step - loss: 0.6276 - acc: 0.6533 - val_loss: 0.6635 - val_acc: 0.5560\n",
      "Epoch 6/1000\n",
      "2009/2009 [==============================] - 9s 5ms/step - loss: 0.6257 - acc: 0.6547 - val_loss: 0.6333 - val_acc: 0.6142\n",
      "Epoch 7/1000\n",
      "1760/2009 [=========================>....] - ETA: 1s - loss: 0.6237 - acc: 0.6578"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bb2cde3dd77f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mck\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                     )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "epochs = 100\n",
    "def build_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='selu', input_shape=(256,)))\n",
    "    model.add(Dense(256, activation='selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='selu'))\n",
    "    model.add(Dense(128, activation='selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='selu'))\n",
    "    model.add(Dense(64, activation='selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='selu'))\n",
    "    model.add(Dense(32, activation='selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, activation='selu'))\n",
    "    model.add(Dense(16, activation='selu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "x,y = next(G)\n",
    "print x.shape, y.shape\n",
    "np.random.seed(1337)\n",
    "model = build_model()\n",
    "model.fit(x,y)\n",
    "ck = keras.callbacks.ModelCheckpoint('./weights.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "tfb = keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "model.fit_generator(G,\n",
    "                    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "                    epochs=1000, verbose=1,\n",
    "                    validation_data=val_G,\n",
    "                    validation_steps=val_X.shape[0]//batch_size,\n",
    "                    callbacks=[ck,tfb]\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "(88074, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "model = load_model('./weights.hdf5')\n",
    "z = model.predict(test_X)\n",
    "with open('pred.txt', 'w') as f:\n",
    "    for i in range(z.shape[0]):\n",
    "        p = z[i,0]\n",
    "        ans = 1 if p >= 0.5 else 0\n",
    "        f.write('%d\\n' % ans)\n",
    "pred_file = 'pred.txt'\n",
    "with open(pred_file, 'r') as f, open(pred_file + '.csv', 'w') as g:\n",
    "    g.write('query_id,prediction\\n')\n",
    "    for idx, line in enumerate(f):\n",
    "        g.write('%d,%d\\n' % (1 + idx, int(line)))\n",
    "\n",
    "\n",
    "print 'done'\n",
    "print z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714296, 256)\n",
      "prob done\n"
     ]
    }
   ],
   "source": [
    "from svm import *\n",
    "from svmutil import *\n",
    "# prepare data for SVM\n",
    "def generate_neg_X(X, adj_mat, node_emb_dict,neg_rate=1.5):\n",
    "    exist_node_list = node_emb_dict.keys()\n",
    "    exist_N = len(exist_node_list)\n",
    "    num_edge = X.shape[0]\n",
    "        \n",
    "    pos_X = X\n",
    "\n",
    "    neg_count = int(num_edge*neg_rate)\n",
    "    neg_idx = np.random.randint(exist_N, size=[neg_count, 2])\n",
    "    neg_X = []\n",
    "    for i in range(neg_count):\n",
    "        src, dst = neg_idx[i]\n",
    "        src = exist_node_list[src]\n",
    "        dst = exist_node_list[dst]\n",
    "        if src != dst and adj_mat[src, dst] == 0:\n",
    "            fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "            neg_X.append(fea)\n",
    "    neg_X = np.vstack(neg_X)\n",
    "\n",
    "    ret_X = np.vstack([pos_X, neg_X])\n",
    "    ret_Y = np.ones([ret_X.shape[0], 1])\n",
    "    ret_Y[pos_X.shape[0]:, 0] = -1\n",
    "    return ret_X, ret_Y\n",
    "train_X, train_Y = generate_neg_X(X, adj_mat, node_emb_dict,neg_rate=1.5)\n",
    "print train_X.shape\n",
    "prob = svm_problem(train_Y.flatten(),train_X)\n",
    "print 'prob done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-s 0 -t 2 -m 3000')\n",
    "m = svm_train(prob, param)\n",
    "p_labels, p_acc, p_vals = svm_predict([], test_X, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file ./kaggle/t1-merge.txt ...\n",
      "torch.Size([29402, 1])\n",
      "Epoch: 0011 loss_train: 591.0887 loss_val: 487.7440 acc1_val: 0.1408 acc2_val: 0.0183 time: 14.8307s\n",
      "Epoch: 0021 loss_train: 588.7114 loss_val: 485.4294 acc1_val: 0.1406 acc2_val: 0.0197 time: 14.9518s\n",
      "Epoch: 0031 loss_train: 588.4152 loss_val: 485.2107 acc1_val: 0.1397 acc2_val: 0.0197 time: 14.7854s\n",
      "Epoch: 0041 loss_train: 588.4144 loss_val: 485.2620 acc1_val: 0.1399 acc2_val: 0.0201 time: 14.8511s\n",
      "Epoch: 0051 loss_train: 588.0554 loss_val: 484.9533 acc1_val: 0.1399 acc2_val: 0.0197 time: 15.1941s\n",
      "Epoch: 0061 loss_train: 587.6581 loss_val: 484.6120 acc1_val: 0.1398 acc2_val: 0.0202 time: 14.8462s\n",
      "Epoch: 0071 loss_train: 587.0911 loss_val: 484.1024 acc1_val: 0.1391 acc2_val: 0.0193 time: 14.9412s\n",
      "Epoch: 0081 loss_train: 586.4742 loss_val: 483.5817 acc1_val: 0.1388 acc2_val: 0.0185 time: 14.8397s\n",
      "Epoch: 0091 loss_train: 585.7252 loss_val: 482.8959 acc1_val: 0.1400 acc2_val: 0.0188 time: 14.8448s\n",
      "Epoch: 0101 loss_train: 584.6797 loss_val: 482.0273 acc1_val: 0.1386 acc2_val: 0.0179 time: 14.8886s\n",
      "Epoch: 0111 loss_train: 583.2974 loss_val: 480.8962 acc1_val: 0.1376 acc2_val: 0.0176 time: 14.8398s\n",
      "Epoch: 0121 loss_train: 581.7091 loss_val: 479.5878 acc1_val: 0.1351 acc2_val: 0.0184 time: 14.8000s\n",
      "Epoch: 0131 loss_train: 579.6393 loss_val: 477.9559 acc1_val: 0.1317 acc2_val: 0.0176 time: 14.9058s\n",
      "Epoch: 0141 loss_train: 577.2838 loss_val: 476.1123 acc1_val: 0.1309 acc2_val: 0.0178 time: 14.8025s\n",
      "Epoch: 0151 loss_train: 574.3482 loss_val: 473.8473 acc1_val: 0.1272 acc2_val: 0.0169 time: 14.7422s\n",
      "Epoch: 0161 loss_train: 571.2813 loss_val: 471.5282 acc1_val: 0.1246 acc2_val: 0.0178 time: 14.7929s\n",
      "Epoch: 0171 loss_train: 567.6671 loss_val: 468.8672 acc1_val: 0.1218 acc2_val: 0.0168 time: 15.0523s\n",
      "Epoch: 0181 loss_train: 563.7971 loss_val: 465.9951 acc1_val: 0.1178 acc2_val: 0.0169 time: 15.0402s\n",
      "Epoch: 0191 loss_train: 559.5431 loss_val: 462.9387 acc1_val: 0.1179 acc2_val: 0.0169 time: 15.3408s\n",
      "Epoch: 0201 loss_train: 555.0620 loss_val: 459.7419 acc1_val: 0.1143 acc2_val: 0.0170 time: 15.0902s\n",
      "Epoch: 0211 loss_train: 550.3787 loss_val: 456.3828 acc1_val: 0.1106 acc2_val: 0.0171 time: 15.1911s\n",
      "Epoch: 0221 loss_train: 545.7058 loss_val: 453.1259 acc1_val: 0.1080 acc2_val: 0.0163 time: 15.1815s\n",
      "Epoch: 0231 loss_train: 540.6553 loss_val: 449.7205 acc1_val: 0.1055 acc2_val: 0.0167 time: 15.1420s\n",
      "Epoch: 0241 loss_train: 535.8098 loss_val: 446.4155 acc1_val: 0.1032 acc2_val: 0.0169 time: 15.5407s\n",
      "Epoch: 0251 loss_train: 530.8343 loss_val: 443.0320 acc1_val: 0.0991 acc2_val: 0.0164 time: 15.3447s\n",
      "Epoch: 0261 loss_train: 525.6824 loss_val: 439.6574 acc1_val: 0.1002 acc2_val: 0.0165 time: 15.4912s\n",
      "Epoch: 0271 loss_train: 521.1077 loss_val: 436.7924 acc1_val: 0.0953 acc2_val: 0.0165 time: 15.0936s\n",
      "Epoch: 0281 loss_train: 516.5621 loss_val: 433.9203 acc1_val: 0.0951 acc2_val: 0.0167 time: 15.0398s\n",
      "Epoch: 0291 loss_train: 511.3212 loss_val: 430.8108 acc1_val: 0.0938 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 0301 loss_train: 506.5257 loss_val: 427.8568 acc1_val: 0.0908 acc2_val: 0.0165 time: 14.9268s\n",
      "Epoch: 0311 loss_train: 501.7146 loss_val: 425.1350 acc1_val: 0.0894 acc2_val: 0.0164 time: 15.1432s\n",
      "Epoch: 0321 loss_train: 496.6795 loss_val: 422.2970 acc1_val: 0.0868 acc2_val: 0.0164 time: 15.0369s\n",
      "Epoch: 0331 loss_train: 491.3264 loss_val: 419.4598 acc1_val: 0.0867 acc2_val: 0.0164 time: 16.1458s\n",
      "Epoch: 0341 loss_train: 486.0397 loss_val: 416.5239 acc1_val: 0.0865 acc2_val: 0.0164 time: 15.0927s\n",
      "Epoch: 0351 loss_train: 481.2193 loss_val: 414.2021 acc1_val: 0.0848 acc2_val: 0.0163 time: 15.1853s\n",
      "Epoch: 0361 loss_train: 476.0504 loss_val: 411.4795 acc1_val: 0.0835 acc2_val: 0.0164 time: 14.9544s\n",
      "Epoch: 0371 loss_train: 470.9374 loss_val: 408.7559 acc1_val: 0.0817 acc2_val: 0.0164 time: 15.0208s\n",
      "Epoch: 0381 loss_train: 465.5901 loss_val: 406.0274 acc1_val: 0.0800 acc2_val: 0.0164 time: 15.1432s\n",
      "Epoch: 0391 loss_train: 459.4224 loss_val: 403.0328 acc1_val: 0.0799 acc2_val: 0.0164 time: 14.9862s\n",
      "Epoch: 0401 loss_train: 453.7343 loss_val: 400.0969 acc1_val: 0.0768 acc2_val: 0.0164 time: 14.8395s\n",
      "Epoch: 0411 loss_train: 449.0525 loss_val: 398.4204 acc1_val: 0.0759 acc2_val: 0.0164 time: 15.1403s\n",
      "Epoch: 0421 loss_train: 443.4866 loss_val: 395.3731 acc1_val: 0.0752 acc2_val: 0.0164 time: 14.9896s\n",
      "Epoch: 0431 loss_train: 439.8324 loss_val: 394.3822 acc1_val: 0.0743 acc2_val: 0.0164 time: 15.0401s\n",
      "Epoch: 0441 loss_train: 434.7673 loss_val: 392.5441 acc1_val: 0.0741 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 0451 loss_train: 431.1328 loss_val: 391.2455 acc1_val: 0.0751 acc2_val: 0.0163 time: 15.0516s\n",
      "Epoch: 0461 loss_train: 426.1658 loss_val: 388.7736 acc1_val: 0.0743 acc2_val: 0.0164 time: 14.8896s\n",
      "Epoch: 0471 loss_train: 420.5060 loss_val: 386.3643 acc1_val: 0.0727 acc2_val: 0.0164 time: 14.9392s\n",
      "Epoch: 0481 loss_train: 418.1747 loss_val: 386.0801 acc1_val: 0.0738 acc2_val: 0.0164 time: 14.9941s\n",
      "Epoch: 0491 loss_train: 413.9042 loss_val: 384.6035 acc1_val: 0.0735 acc2_val: 0.0164 time: 15.0266s\n",
      "Epoch: 0501 loss_train: 409.0733 loss_val: 382.3300 acc1_val: 0.0710 acc2_val: 0.0164 time: 15.1938s\n",
      "Epoch: 0511 loss_train: 405.4355 loss_val: 381.6417 acc1_val: 0.0714 acc2_val: 0.0164 time: 15.1030s\n",
      "Epoch: 0521 loss_train: 399.8536 loss_val: 379.5944 acc1_val: 0.0702 acc2_val: 0.0164 time: 14.8723s\n",
      "Epoch: 0531 loss_train: 396.4731 loss_val: 378.4123 acc1_val: 0.0696 acc2_val: 0.0164 time: 15.0903s\n",
      "Epoch: 0541 loss_train: 392.5637 loss_val: 377.0108 acc1_val: 0.0658 acc2_val: 0.0164 time: 15.0432s\n",
      "Epoch: 0551 loss_train: 390.0014 loss_val: 377.2164 acc1_val: 0.0694 acc2_val: 0.0164 time: 15.1093s\n",
      "Epoch: 0561 loss_train: 385.8992 loss_val: 375.4014 acc1_val: 0.0655 acc2_val: 0.0164 time: 15.1445s\n",
      "Epoch: 0571 loss_train: 382.0768 loss_val: 373.8465 acc1_val: 0.0661 acc2_val: 0.0164 time: 15.1875s\n",
      "Epoch: 0581 loss_train: 378.6303 loss_val: 373.3641 acc1_val: 0.0641 acc2_val: 0.0164 time: 15.1403s\n",
      "Epoch: 0591 loss_train: 373.0800 loss_val: 370.7807 acc1_val: 0.0611 acc2_val: 0.0164 time: 15.0400s\n",
      "Epoch: 0601 loss_train: 374.0972 loss_val: 372.6006 acc1_val: 0.0652 acc2_val: 0.0163 time: 14.9457s\n",
      "Epoch: 0611 loss_train: 370.8846 loss_val: 372.1914 acc1_val: 0.0686 acc2_val: 0.0163 time: 15.0875s\n",
      "Epoch: 0621 loss_train: 366.0581 loss_val: 369.8775 acc1_val: 0.0609 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 0631 loss_train: 365.8344 loss_val: 370.2378 acc1_val: 0.0665 acc2_val: 0.0164 time: 15.0329s\n",
      "Epoch: 0641 loss_train: 360.5398 loss_val: 368.6659 acc1_val: 0.0634 acc2_val: 0.0164 time: 15.0108s\n",
      "Epoch: 0651 loss_train: 359.4643 loss_val: 368.2342 acc1_val: 0.0614 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 0661 loss_train: 355.6015 loss_val: 367.7013 acc1_val: 0.0617 acc2_val: 0.0164 time: 14.9159s\n",
      "Epoch: 0671 loss_train: 354.2952 loss_val: 368.3294 acc1_val: 0.0659 acc2_val: 0.0163 time: 15.2907s\n",
      "Epoch: 0681 loss_train: 350.6408 loss_val: 367.5056 acc1_val: 0.0639 acc2_val: 0.0164 time: 15.0428s\n",
      "Epoch: 0691 loss_train: 347.2260 loss_val: 366.4744 acc1_val: 0.0613 acc2_val: 0.0164 time: 15.1473s\n",
      "Epoch: 0701 loss_train: 343.6624 loss_val: 365.9881 acc1_val: 0.0616 acc2_val: 0.0164 time: 15.1403s\n",
      "Epoch: 0711 loss_train: 341.9254 loss_val: 365.1715 acc1_val: 0.0625 acc2_val: 0.0163 time: 15.0929s\n",
      "Epoch: 0721 loss_train: 337.8931 loss_val: 364.6974 acc1_val: 0.0612 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 0731 loss_train: 335.6180 loss_val: 364.0188 acc1_val: 0.0580 acc2_val: 0.0164 time: 14.9409s\n",
      "Epoch: 0741 loss_train: 333.4096 loss_val: 363.9049 acc1_val: 0.0588 acc2_val: 0.0163 time: 15.0932s\n",
      "Epoch: 0751 loss_train: 332.4197 loss_val: 365.2625 acc1_val: 0.0642 acc2_val: 0.0164 time: 15.0910s\n",
      "Epoch: 0761 loss_train: 330.1385 loss_val: 364.1071 acc1_val: 0.0601 acc2_val: 0.0164 time: 14.9899s\n",
      "Epoch: 0771 loss_train: 328.2151 loss_val: 364.0980 acc1_val: 0.0613 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 0781 loss_train: 325.3968 loss_val: 363.6445 acc1_val: 0.0605 acc2_val: 0.0164 time: 15.0163s\n",
      "Epoch: 0791 loss_train: 323.0288 loss_val: 363.1289 acc1_val: 0.0591 acc2_val: 0.0164 time: 15.0902s\n",
      "Epoch: 0801 loss_train: 320.0201 loss_val: 362.3462 acc1_val: 0.0573 acc2_val: 0.0164 time: 14.9898s\n",
      "Epoch: 0811 loss_train: 319.4368 loss_val: 362.7866 acc1_val: 0.0610 acc2_val: 0.0164 time: 14.9733s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0821 loss_train: 316.9384 loss_val: 361.8237 acc1_val: 0.0594 acc2_val: 0.0163 time: 15.4442s\n",
      "Epoch: 0831 loss_train: 315.0992 loss_val: 363.2898 acc1_val: 0.0589 acc2_val: 0.0163 time: 14.9397s\n",
      "Epoch: 0841 loss_train: 312.8734 loss_val: 361.7029 acc1_val: 0.0609 acc2_val: 0.0163 time: 15.0882s\n",
      "Epoch: 0851 loss_train: 310.4143 loss_val: 362.5493 acc1_val: 0.0607 acc2_val: 0.0163 time: 15.0976s\n",
      "Epoch: 0861 loss_train: 307.3359 loss_val: 362.6578 acc1_val: 0.0585 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 0871 loss_train: 306.6123 loss_val: 361.9621 acc1_val: 0.0574 acc2_val: 0.0164 time: 14.9796s\n",
      "Epoch: 0881 loss_train: 302.2325 loss_val: 362.1649 acc1_val: 0.0559 acc2_val: 0.0163 time: 15.3409s\n",
      "Epoch: 0891 loss_train: 301.8675 loss_val: 362.4531 acc1_val: 0.0593 acc2_val: 0.0164 time: 15.1403s\n",
      "Epoch: 0901 loss_train: 300.3988 loss_val: 363.4804 acc1_val: 0.0580 acc2_val: 0.0164 time: 14.8650s\n",
      "Epoch: 0911 loss_train: 298.0191 loss_val: 362.5672 acc1_val: 0.0561 acc2_val: 0.0163 time: 14.9705s\n",
      "Epoch: 0921 loss_train: 296.3365 loss_val: 362.0331 acc1_val: 0.0573 acc2_val: 0.0163 time: 15.0430s\n",
      "Epoch: 0931 loss_train: 293.3801 loss_val: 363.1552 acc1_val: 0.0583 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 0941 loss_train: 292.3085 loss_val: 363.4263 acc1_val: 0.0558 acc2_val: 0.0164 time: 15.3891s\n",
      "Epoch: 0951 loss_train: 290.0844 loss_val: 362.7776 acc1_val: 0.0566 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 0961 loss_train: 287.3617 loss_val: 364.7560 acc1_val: 0.0583 acc2_val: 0.0163 time: 14.9544s\n",
      "Epoch: 0971 loss_train: 288.4441 loss_val: 364.5867 acc1_val: 0.0585 acc2_val: 0.0164 time: 15.1427s\n",
      "Epoch: 0981 loss_train: 286.8456 loss_val: 363.6012 acc1_val: 0.0552 acc2_val: 0.0164 time: 15.0903s\n",
      "Epoch: 0991 loss_train: 284.4734 loss_val: 363.3647 acc1_val: 0.0546 acc2_val: 0.0163 time: 14.9898s\n",
      "Epoch: 1001 loss_train: 283.0136 loss_val: 363.1308 acc1_val: 0.0523 acc2_val: 0.0164 time: 15.0741s\n",
      "Epoch: 1011 loss_train: 279.6425 loss_val: 364.2888 acc1_val: 0.0543 acc2_val: 0.0164 time: 15.1027s\n",
      "Epoch: 1021 loss_train: 279.1020 loss_val: 364.8671 acc1_val: 0.0529 acc2_val: 0.0164 time: 15.1931s\n",
      "Epoch: 1031 loss_train: 277.4240 loss_val: 364.6047 acc1_val: 0.0568 acc2_val: 0.0163 time: 15.1912s\n",
      "Epoch: 1041 loss_train: 275.4026 loss_val: 363.7744 acc1_val: 0.0535 acc2_val: 0.0164 time: 15.0373s\n",
      "Epoch: 1051 loss_train: 273.8955 loss_val: 364.6422 acc1_val: 0.0552 acc2_val: 0.0164 time: 15.3408s\n",
      "Epoch: 1061 loss_train: 271.6765 loss_val: 364.7163 acc1_val: 0.0529 acc2_val: 0.0164 time: 15.0873s\n",
      "Epoch: 1071 loss_train: 268.0711 loss_val: 366.6795 acc1_val: 0.0509 acc2_val: 0.0163 time: 15.1437s\n",
      "Epoch: 1081 loss_train: 267.5736 loss_val: 366.0389 acc1_val: 0.0521 acc2_val: 0.0164 time: 15.3079s\n",
      "Epoch: 1091 loss_train: 270.6301 loss_val: 364.5317 acc1_val: 0.0522 acc2_val: 0.0164 time: 15.1375s\n",
      "Epoch: 1101 loss_train: 267.1794 loss_val: 365.8016 acc1_val: 0.0558 acc2_val: 0.0164 time: 15.1455s\n",
      "Epoch: 1111 loss_train: 263.0745 loss_val: 366.5300 acc1_val: 0.0532 acc2_val: 0.0163 time: 15.1904s\n",
      "Epoch: 1121 loss_train: 263.0856 loss_val: 366.7959 acc1_val: 0.0539 acc2_val: 0.0163 time: 14.9207s\n",
      "Epoch: 1131 loss_train: 259.9862 loss_val: 368.4394 acc1_val: 0.0503 acc2_val: 0.0164 time: 15.0902s\n",
      "Epoch: 1141 loss_train: 260.3809 loss_val: 367.5046 acc1_val: 0.0528 acc2_val: 0.0164 time: 15.1944s\n",
      "Epoch: 1151 loss_train: 259.3953 loss_val: 367.1416 acc1_val: 0.0528 acc2_val: 0.0164 time: 15.1825s\n",
      "Epoch: 1161 loss_train: 257.9350 loss_val: 369.0079 acc1_val: 0.0533 acc2_val: 0.0164 time: 14.8896s\n",
      "Epoch: 1171 loss_train: 256.9080 loss_val: 368.9666 acc1_val: 0.0534 acc2_val: 0.0163 time: 15.1402s\n",
      "Epoch: 1181 loss_train: 253.9501 loss_val: 370.5902 acc1_val: 0.0518 acc2_val: 0.0164 time: 15.0927s\n",
      "Epoch: 1191 loss_train: 254.3278 loss_val: 369.2370 acc1_val: 0.0500 acc2_val: 0.0164 time: 15.0935s\n",
      "Epoch: 1201 loss_train: 249.8281 loss_val: 371.8874 acc1_val: 0.0500 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 1211 loss_train: 249.9861 loss_val: 371.3036 acc1_val: 0.0497 acc2_val: 0.0164 time: 15.1088s\n",
      "Epoch: 1221 loss_train: 249.5128 loss_val: 370.8982 acc1_val: 0.0485 acc2_val: 0.0163 time: 15.1429s\n",
      "Epoch: 1231 loss_train: 249.1045 loss_val: 372.3441 acc1_val: 0.0545 acc2_val: 0.0164 time: 15.0509s\n",
      "Epoch: 1241 loss_train: 248.4663 loss_val: 371.5040 acc1_val: 0.0518 acc2_val: 0.0164 time: 15.0400s\n",
      "Epoch: 1251 loss_train: 243.6151 loss_val: 373.0686 acc1_val: 0.0494 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 1261 loss_train: 242.9014 loss_val: 374.0300 acc1_val: 0.0492 acc2_val: 0.0163 time: 15.0902s\n",
      "Epoch: 1271 loss_train: 241.7020 loss_val: 374.9169 acc1_val: 0.0517 acc2_val: 0.0163 time: 15.1148s\n",
      "Epoch: 1281 loss_train: 242.2242 loss_val: 373.4473 acc1_val: 0.0515 acc2_val: 0.0164 time: 14.9411s\n",
      "Epoch: 1291 loss_train: 242.7210 loss_val: 374.2068 acc1_val: 0.0549 acc2_val: 0.0164 time: 14.9370s\n",
      "Epoch: 1301 loss_train: 243.6216 loss_val: 371.8227 acc1_val: 0.0510 acc2_val: 0.0164 time: 15.0278s\n",
      "Epoch: 1311 loss_train: 237.4855 loss_val: 376.1509 acc1_val: 0.0495 acc2_val: 0.0163 time: 14.8906s\n",
      "Epoch: 1321 loss_train: 238.0225 loss_val: 373.6799 acc1_val: 0.0470 acc2_val: 0.0164 time: 14.9398s\n",
      "Epoch: 1331 loss_train: 235.0687 loss_val: 376.3405 acc1_val: 0.0484 acc2_val: 0.0164 time: 14.8896s\n",
      "Epoch: 1341 loss_train: 234.8159 loss_val: 376.0041 acc1_val: 0.0494 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 1351 loss_train: 235.5269 loss_val: 377.0321 acc1_val: 0.0506 acc2_val: 0.0165 time: 14.9180s\n",
      "Epoch: 1361 loss_train: 232.5453 loss_val: 377.8043 acc1_val: 0.0491 acc2_val: 0.0164 time: 14.8896s\n",
      "Epoch: 1371 loss_train: 228.1965 loss_val: 384.9056 acc1_val: 0.0441 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 1381 loss_train: 227.8875 loss_val: 381.4325 acc1_val: 0.0474 acc2_val: 0.0164 time: 15.5413s\n",
      "Epoch: 1391 loss_train: 227.9501 loss_val: 380.7762 acc1_val: 0.0457 acc2_val: 0.0164 time: 15.0600s\n",
      "Epoch: 1401 loss_train: 227.4597 loss_val: 380.6280 acc1_val: 0.0486 acc2_val: 0.0165 time: 14.9898s\n",
      "Epoch: 1411 loss_train: 226.4717 loss_val: 382.3736 acc1_val: 0.0485 acc2_val: 0.0162 time: 15.2379s\n",
      "Epoch: 1421 loss_train: 228.6375 loss_val: 377.4096 acc1_val: 0.0484 acc2_val: 0.0164 time: 15.0379s\n",
      "Epoch: 1431 loss_train: 223.8329 loss_val: 383.0262 acc1_val: 0.0487 acc2_val: 0.0163 time: 15.1940s\n",
      "Epoch: 1441 loss_train: 223.2640 loss_val: 385.3629 acc1_val: 0.0441 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 1451 loss_train: 222.3930 loss_val: 383.8698 acc1_val: 0.0494 acc2_val: 0.0164 time: 14.9899s\n",
      "Epoch: 1461 loss_train: 220.7038 loss_val: 385.8624 acc1_val: 0.0481 acc2_val: 0.0164 time: 15.1120s\n",
      "Epoch: 1471 loss_train: 220.5961 loss_val: 382.9191 acc1_val: 0.0466 acc2_val: 0.0165 time: 15.0417s\n",
      "Epoch: 1481 loss_train: 218.0716 loss_val: 388.9790 acc1_val: 0.0435 acc2_val: 0.0165 time: 15.0892s\n",
      "Epoch: 1491 loss_train: 218.1432 loss_val: 386.2070 acc1_val: 0.0481 acc2_val: 0.0165 time: 14.9398s\n",
      "Epoch: 1501 loss_train: 217.5676 loss_val: 385.2417 acc1_val: 0.0472 acc2_val: 0.0164 time: 15.1903s\n",
      "Epoch: 1511 loss_train: 215.5284 loss_val: 388.5311 acc1_val: 0.0464 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 1521 loss_train: 214.4783 loss_val: 387.0697 acc1_val: 0.0491 acc2_val: 0.0164 time: 15.0342s\n",
      "Epoch: 1531 loss_train: 216.5609 loss_val: 387.0637 acc1_val: 0.0454 acc2_val: 0.0166 time: 14.9899s\n",
      "Epoch: 1541 loss_train: 214.4368 loss_val: 386.6029 acc1_val: 0.0489 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 1551 loss_train: 213.4811 loss_val: 389.5251 acc1_val: 0.0464 acc2_val: 0.0164 time: 15.4416s\n",
      "Epoch: 1561 loss_train: 211.6030 loss_val: 390.6281 acc1_val: 0.0466 acc2_val: 0.0164 time: 15.2908s\n",
      "Epoch: 1571 loss_train: 211.6743 loss_val: 390.4487 acc1_val: 0.0470 acc2_val: 0.0164 time: 15.0400s\n",
      "Epoch: 1581 loss_train: 210.7864 loss_val: 392.0026 acc1_val: 0.0486 acc2_val: 0.0164 time: 15.0399s\n",
      "Epoch: 1591 loss_train: 210.8548 loss_val: 388.6979 acc1_val: 0.0474 acc2_val: 0.0164 time: 14.8897s\n",
      "Epoch: 1601 loss_train: 211.3320 loss_val: 390.9450 acc1_val: 0.0491 acc2_val: 0.0165 time: 15.0397s\n",
      "Epoch: 1611 loss_train: 207.7551 loss_val: 393.4390 acc1_val: 0.0433 acc2_val: 0.0164 time: 15.0934s\n",
      "Epoch: 1621 loss_train: 206.2136 loss_val: 396.3962 acc1_val: 0.0474 acc2_val: 0.0165 time: 15.1920s\n",
      "Epoch: 1631 loss_train: 203.1969 loss_val: 398.2409 acc1_val: 0.0457 acc2_val: 0.0164 time: 15.2770s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1641 loss_train: 202.2951 loss_val: 398.9748 acc1_val: 0.0448 acc2_val: 0.0164 time: 14.9870s\n",
      "Epoch: 1651 loss_train: 204.3381 loss_val: 396.5375 acc1_val: 0.0449 acc2_val: 0.0163 time: 15.2906s\n",
      "Epoch: 1661 loss_train: 201.7617 loss_val: 395.2855 acc1_val: 0.0452 acc2_val: 0.0164 time: 15.1136s\n",
      "Epoch: 1671 loss_train: 201.4439 loss_val: 397.5036 acc1_val: 0.0448 acc2_val: 0.0164 time: 15.0747s\n",
      "Epoch: 1681 loss_train: 199.7089 loss_val: 400.7058 acc1_val: 0.0475 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 1691 loss_train: 198.7265 loss_val: 402.2788 acc1_val: 0.0466 acc2_val: 0.0164 time: 14.9898s\n",
      "Epoch: 1701 loss_train: 199.6071 loss_val: 397.7327 acc1_val: 0.0464 acc2_val: 0.0163 time: 14.9878s\n",
      "Epoch: 1711 loss_train: 198.8903 loss_val: 397.4721 acc1_val: 0.0452 acc2_val: 0.0163 time: 15.1448s\n",
      "Epoch: 1721 loss_train: 195.9617 loss_val: 405.5149 acc1_val: 0.0454 acc2_val: 0.0164 time: 15.1911s\n",
      "Epoch: 1731 loss_train: 196.5045 loss_val: 404.0506 acc1_val: 0.0451 acc2_val: 0.0164 time: 15.1126s\n",
      "Epoch: 1741 loss_train: 193.5614 loss_val: 407.4387 acc1_val: 0.0441 acc2_val: 0.0164 time: 15.0400s\n",
      "Epoch: 1751 loss_train: 193.5822 loss_val: 406.0781 acc1_val: 0.0443 acc2_val: 0.0164 time: 15.1403s\n",
      "Epoch: 1761 loss_train: 194.3101 loss_val: 401.5712 acc1_val: 0.0451 acc2_val: 0.0164 time: 14.9994s\n",
      "Epoch: 1771 loss_train: 195.0390 loss_val: 401.3040 acc1_val: 0.0466 acc2_val: 0.0164 time: 15.3555s\n",
      "Epoch: 1781 loss_train: 192.7664 loss_val: 404.9686 acc1_val: 0.0452 acc2_val: 0.0164 time: 15.0903s\n",
      "Epoch: 1791 loss_train: 191.3865 loss_val: 408.4845 acc1_val: 0.0454 acc2_val: 0.0164 time: 15.1907s\n",
      "Epoch: 1801 loss_train: 188.7143 loss_val: 410.7137 acc1_val: 0.0449 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 1811 loss_train: 188.2072 loss_val: 408.9445 acc1_val: 0.0444 acc2_val: 0.0162 time: 15.0654s\n",
      "Epoch: 1821 loss_train: 188.4073 loss_val: 412.9101 acc1_val: 0.0431 acc2_val: 0.0164 time: 15.0400s\n",
      "Epoch: 1831 loss_train: 187.6334 loss_val: 409.3600 acc1_val: 0.0444 acc2_val: 0.0166 time: 14.8896s\n",
      "Epoch: 1841 loss_train: 187.0900 loss_val: 406.9972 acc1_val: 0.0457 acc2_val: 0.0163 time: 14.8395s\n",
      "Epoch: 1851 loss_train: 186.8978 loss_val: 412.7346 acc1_val: 0.0447 acc2_val: 0.0164 time: 14.8895s\n",
      "Epoch: 1861 loss_train: 183.9635 loss_val: 415.3900 acc1_val: 0.0455 acc2_val: 0.0164 time: 15.1450s\n",
      "Epoch: 1871 loss_train: 184.6173 loss_val: 416.1191 acc1_val: 0.0448 acc2_val: 0.0165 time: 14.8425s\n",
      "Epoch: 1881 loss_train: 184.5199 loss_val: 413.5673 acc1_val: 0.0429 acc2_val: 0.0163 time: 14.7914s\n",
      "Epoch: 1891 loss_train: 183.0279 loss_val: 416.3749 acc1_val: 0.0443 acc2_val: 0.0165 time: 15.0880s\n",
      "Epoch: 1901 loss_train: 180.6893 loss_val: 420.9788 acc1_val: 0.0439 acc2_val: 0.0164 time: 14.9215s\n",
      "Epoch: 1911 loss_train: 182.5479 loss_val: 418.2594 acc1_val: 0.0413 acc2_val: 0.0165 time: 15.0958s\n",
      "Epoch: 1921 loss_train: 182.3564 loss_val: 417.8711 acc1_val: 0.0416 acc2_val: 0.0164 time: 14.9402s\n",
      "Epoch: 1931 loss_train: 180.9473 loss_val: 418.6234 acc1_val: 0.0427 acc2_val: 0.0164 time: 15.1781s\n",
      "Epoch: 1941 loss_train: 179.1406 loss_val: 426.6170 acc1_val: 0.0404 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 1951 loss_train: 177.8535 loss_val: 426.0931 acc1_val: 0.0460 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 1961 loss_train: 178.9319 loss_val: 422.1887 acc1_val: 0.0418 acc2_val: 0.0164 time: 14.9899s\n",
      "Epoch: 1971 loss_train: 177.3314 loss_val: 424.9823 acc1_val: 0.0415 acc2_val: 0.0165 time: 15.0400s\n",
      "Epoch: 1981 loss_train: 176.0325 loss_val: 424.5590 acc1_val: 0.0442 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 1991 loss_train: 176.2800 loss_val: 424.6756 acc1_val: 0.0431 acc2_val: 0.0164 time: 15.0903s\n",
      "Epoch: 2001 loss_train: 175.9062 loss_val: 422.5287 acc1_val: 0.0414 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 2011 loss_train: 175.4675 loss_val: 426.6037 acc1_val: 0.0409 acc2_val: 0.0164 time: 15.1422s\n",
      "Epoch: 2021 loss_train: 172.2764 loss_val: 436.8136 acc1_val: 0.0392 acc2_val: 0.0164 time: 14.8896s\n",
      "Epoch: 2031 loss_train: 170.6291 loss_val: 437.3422 acc1_val: 0.0433 acc2_val: 0.0164 time: 14.8896s\n",
      "Epoch: 2041 loss_train: 174.4228 loss_val: 427.0577 acc1_val: 0.0418 acc2_val: 0.0164 time: 15.0872s\n",
      "Epoch: 2051 loss_train: 171.1197 loss_val: 434.4535 acc1_val: 0.0428 acc2_val: 0.0163 time: 15.0400s\n",
      "Epoch: 2061 loss_train: 174.4559 loss_val: 429.6363 acc1_val: 0.0430 acc2_val: 0.0164 time: 15.2405s\n",
      "Epoch: 2071 loss_train: 172.2063 loss_val: 427.4758 acc1_val: 0.0428 acc2_val: 0.0164 time: 15.1869s\n",
      "Epoch: 2081 loss_train: 169.2486 loss_val: 432.7872 acc1_val: 0.0399 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 2091 loss_train: 168.6076 loss_val: 432.6194 acc1_val: 0.0418 acc2_val: 0.0164 time: 15.0422s\n",
      "Epoch: 2101 loss_train: 172.6948 loss_val: 424.9467 acc1_val: 0.0426 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 2111 loss_train: 173.5585 loss_val: 426.7682 acc1_val: 0.0434 acc2_val: 0.0164 time: 15.0377s\n",
      "Epoch: 2121 loss_train: 167.6088 loss_val: 435.9427 acc1_val: 0.0429 acc2_val: 0.0164 time: 14.9385s\n",
      "Epoch: 2131 loss_train: 165.5894 loss_val: 439.4312 acc1_val: 0.0439 acc2_val: 0.0167 time: 15.0784s\n",
      "Epoch: 2141 loss_train: 166.5919 loss_val: 432.3637 acc1_val: 0.0429 acc2_val: 0.0163 time: 15.1904s\n",
      "Epoch: 2151 loss_train: 162.7413 loss_val: 452.6621 acc1_val: 0.0402 acc2_val: 0.0164 time: 14.9935s\n",
      "Epoch: 2161 loss_train: 165.6187 loss_val: 446.7454 acc1_val: 0.0393 acc2_val: 0.0164 time: 14.9877s\n",
      "Epoch: 2171 loss_train: 163.8906 loss_val: 443.0276 acc1_val: 0.0389 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 2181 loss_train: 164.2985 loss_val: 440.9229 acc1_val: 0.0441 acc2_val: 0.0164 time: 14.9379s\n",
      "Epoch: 2191 loss_train: 167.4822 loss_val: 432.7146 acc1_val: 0.0420 acc2_val: 0.0165 time: 15.0442s\n",
      "Epoch: 2201 loss_train: 160.9481 loss_val: 447.1602 acc1_val: 0.0426 acc2_val: 0.0164 time: 14.9272s\n",
      "Epoch: 2211 loss_train: 161.9855 loss_val: 440.9852 acc1_val: 0.0424 acc2_val: 0.0164 time: 15.0400s\n",
      "Epoch: 2221 loss_train: 161.6209 loss_val: 442.9514 acc1_val: 0.0421 acc2_val: 0.0165 time: 15.0582s\n",
      "Epoch: 2231 loss_train: 161.7673 loss_val: 441.0493 acc1_val: 0.0416 acc2_val: 0.0163 time: 15.1049s\n",
      "Epoch: 2241 loss_train: 157.9819 loss_val: 453.7162 acc1_val: 0.0396 acc2_val: 0.0164 time: 14.8614s\n",
      "Epoch: 2251 loss_train: 158.6380 loss_val: 452.8957 acc1_val: 0.0390 acc2_val: 0.0164 time: 14.7893s\n",
      "Epoch: 2261 loss_train: 158.0729 loss_val: 451.6001 acc1_val: 0.0406 acc2_val: 0.0162 time: 14.8895s\n",
      "Epoch: 2271 loss_train: 157.1337 loss_val: 451.0956 acc1_val: 0.0409 acc2_val: 0.0166 time: 15.2405s\n",
      "Epoch: 2281 loss_train: 155.7201 loss_val: 456.5803 acc1_val: 0.0409 acc2_val: 0.0165 time: 15.0437s\n",
      "Epoch: 2291 loss_train: 155.1117 loss_val: 459.0224 acc1_val: 0.0396 acc2_val: 0.0164 time: 15.1904s\n",
      "Epoch: 2301 loss_train: 155.7468 loss_val: 457.2943 acc1_val: 0.0415 acc2_val: 0.0165 time: 14.9898s\n",
      "Epoch: 2311 loss_train: 156.7599 loss_val: 449.8761 acc1_val: 0.0409 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 2321 loss_train: 157.5800 loss_val: 446.8235 acc1_val: 0.0397 acc2_val: 0.0164 time: 15.1402s\n",
      "Epoch: 2331 loss_train: 157.2609 loss_val: 449.5483 acc1_val: 0.0407 acc2_val: 0.0164 time: 15.2373s\n",
      "Epoch: 2341 loss_train: 152.1750 loss_val: 465.9716 acc1_val: 0.0378 acc2_val: 0.0164 time: 15.1369s\n",
      "Epoch: 2351 loss_train: 152.6266 loss_val: 460.4468 acc1_val: 0.0415 acc2_val: 0.0164 time: 14.9397s\n",
      "Epoch: 2361 loss_train: 153.4907 loss_val: 454.0275 acc1_val: 0.0379 acc2_val: 0.0164 time: 14.9898s\n",
      "Epoch: 2371 loss_train: 152.6059 loss_val: 461.5405 acc1_val: 0.0387 acc2_val: 0.0163 time: 15.0928s\n",
      "Epoch: 2381 loss_train: 152.9132 loss_val: 455.5876 acc1_val: 0.0399 acc2_val: 0.0163 time: 14.9887s\n",
      "Epoch: 2391 loss_train: 151.5631 loss_val: 464.9939 acc1_val: 0.0405 acc2_val: 0.0163 time: 15.1402s\n",
      "Epoch: 2401 loss_train: 150.7180 loss_val: 464.2242 acc1_val: 0.0411 acc2_val: 0.0164 time: 14.9899s\n",
      "Epoch: 2411 loss_train: 150.6423 loss_val: 466.8438 acc1_val: 0.0390 acc2_val: 0.0164 time: 15.2374s\n",
      "Epoch: 2421 loss_train: 152.8290 loss_val: 453.4825 acc1_val: 0.0419 acc2_val: 0.0164 time: 14.9899s\n",
      "Epoch: 2431 loss_train: 151.5630 loss_val: 460.6138 acc1_val: 0.0387 acc2_val: 0.0165 time: 15.1904s\n",
      "Epoch: 2441 loss_train: 150.6947 loss_val: 460.6288 acc1_val: 0.0404 acc2_val: 0.0162 time: 15.0113s\n",
      "Epoch: 2451 loss_train: 149.7370 loss_val: 463.3678 acc1_val: 0.0393 acc2_val: 0.0165 time: 14.9397s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2461 loss_train: 148.5095 loss_val: 467.8900 acc1_val: 0.0380 acc2_val: 0.0164 time: 15.0961s\n",
      "Epoch: 2471 loss_train: 148.8033 loss_val: 468.5698 acc1_val: 0.0373 acc2_val: 0.0164 time: 15.0874s\n",
      "Epoch: 2481 loss_train: 146.6314 loss_val: 471.4716 acc1_val: 0.0385 acc2_val: 0.0164 time: 15.1905s\n",
      "Epoch: 2491 loss_train: 146.2769 loss_val: 474.3130 acc1_val: 0.0393 acc2_val: 0.0164 time: 14.9920s\n",
      "Epoch: 2501 loss_train: 145.0467 loss_val: 478.4380 acc1_val: 0.0372 acc2_val: 0.0163 time: 14.9844s\n",
      "Epoch: 2511 loss_train: 141.6173 loss_val: 484.0628 acc1_val: 0.0406 acc2_val: 0.0164 time: 15.1719s\n",
      "Epoch: 2521 loss_train: 143.6904 loss_val: 477.2281 acc1_val: 0.0393 acc2_val: 0.0163 time: 15.1913s\n",
      "Epoch: 2531 loss_train: 142.7814 loss_val: 481.2914 acc1_val: 0.0381 acc2_val: 0.0163 time: 15.0448s\n",
      "Epoch: 2541 loss_train: 147.8294 loss_val: 466.7887 acc1_val: 0.0379 acc2_val: 0.0165 time: 15.2903s\n",
      "Epoch: 2551 loss_train: 141.2921 loss_val: 483.5156 acc1_val: 0.0384 acc2_val: 0.0163 time: 15.2906s\n",
      "Epoch: 2561 loss_train: 144.3770 loss_val: 474.9619 acc1_val: 0.0364 acc2_val: 0.0164 time: 14.9899s\n",
      "Epoch: 2571 loss_train: 141.9933 loss_val: 474.9735 acc1_val: 0.0396 acc2_val: 0.0164 time: 15.1048s\n",
      "Epoch: 2581 loss_train: 144.1939 loss_val: 471.2905 acc1_val: 0.0410 acc2_val: 0.0164 time: 15.0938s\n",
      "Epoch: 2591 loss_train: 142.0627 loss_val: 478.6552 acc1_val: 0.0397 acc2_val: 0.0164 time: 14.9497s\n",
      "Epoch: 2601 loss_train: 141.4641 loss_val: 476.1432 acc1_val: 0.0386 acc2_val: 0.0164 time: 15.4410s\n",
      "Epoch: 2611 loss_train: 139.3329 loss_val: 486.8033 acc1_val: 0.0390 acc2_val: 0.0164 time: 14.9366s\n",
      "Epoch: 2621 loss_train: 135.7504 loss_val: 495.4084 acc1_val: 0.0364 acc2_val: 0.0163 time: 15.0901s\n",
      "Epoch: 2631 loss_train: 137.5913 loss_val: 490.5859 acc1_val: 0.0366 acc2_val: 0.0163 time: 15.0790s\n",
      "Epoch: 2641 loss_train: 139.2580 loss_val: 483.0857 acc1_val: 0.0387 acc2_val: 0.0164 time: 14.8886s\n",
      "Epoch: 2651 loss_train: 135.9076 loss_val: 487.2570 acc1_val: 0.0373 acc2_val: 0.0166 time: 15.4913s\n",
      "Epoch: 2661 loss_train: 136.2997 loss_val: 490.6628 acc1_val: 0.0388 acc2_val: 0.0164 time: 15.0980s\n",
      "Epoch: 2671 loss_train: 135.7992 loss_val: 491.8861 acc1_val: 0.0359 acc2_val: 0.0164 time: 15.0901s\n",
      "Epoch: 2681 loss_train: 136.9925 loss_val: 491.0514 acc1_val: 0.0372 acc2_val: 0.0163 time: 15.6417s\n",
      "Epoch: 2691 loss_train: 136.6874 loss_val: 488.0726 acc1_val: 0.0375 acc2_val: 0.0164 time: 15.3409s\n",
      "Epoch: 2701 loss_train: 135.4601 loss_val: 490.9189 acc1_val: 0.0399 acc2_val: 0.0163 time: 14.9878s\n",
      "Epoch: 2711 loss_train: 136.2809 loss_val: 502.0558 acc1_val: 0.0373 acc2_val: 0.0164 time: 15.1403s\n",
      "Epoch: 2721 loss_train: 134.9763 loss_val: 498.9083 acc1_val: 0.0368 acc2_val: 0.0164 time: 14.6730s\n",
      "Epoch: 2731 loss_train: 140.1200 loss_val: 475.9822 acc1_val: 0.0396 acc2_val: 0.0163 time: 14.8395s\n",
      "Epoch: 2741 loss_train: 136.8950 loss_val: 488.0768 acc1_val: 0.0393 acc2_val: 0.0164 time: 14.9064s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-354b28a2a89e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msym_adj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_1\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                 \u001b[1;36m0.\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[0ml1_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1_criterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\env\\python3.6.5\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-354b28a2a89e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ml1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\env\\python3.6.5\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\git\\STML\\hw1\\task1\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0msupport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GCN version\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path, idx_map):\n",
    "    print('Loading from file {} ...'.format(path))\n",
    "\n",
    "\n",
    "    # build graph\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    N = len(idx_map)\n",
    "    \n",
    "#     print edges_unordered.shape\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "#     print edges.shape\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(N, N),\n",
    "                        dtype=np.float32)\n",
    "    src_adj = adj\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    return src_adj, adj\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "def accuracy_mse(output, labels):\n",
    "    correct = torch.abs(output - labels) < 0.5\n",
    "    return torch.sum(correct).item() / len(labels)\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "adj, sym_adj = load_data('./kaggle/t1-merge.txt', idx_map)\n",
    "# print adj.shape\n",
    "\n",
    "\n",
    "# GCN version\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, node_num, nhid, nclass, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "        self.emb = nn.Embedding(node_num, nhid)\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.mid = GraphConvolution(nhid, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.emb(x)\n",
    "        x = F.selu(self.gc1(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        x = l1 = F.selu(self.mid(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "        return x, l1\n",
    "\n",
    "\n",
    "def count_degree(adj):\n",
    "    node_num = adj.shape[0]\n",
    "    return \n",
    "def count_neighbor(adj):\n",
    "    N = adj.shape[0]\n",
    "    m = {i:[] for i in range(N)}\n",
    "    rows, cols, values = sp.find(adj)\n",
    "    for src,dst,v in zip(rows, cols, values):\n",
    "        if v == 1:\n",
    "            m[src].append(dst)\n",
    "            m[dst].append(src)\n",
    "    ret = []\n",
    "    for i in range(N):\n",
    "        neighbor_set = set(m[i])\n",
    "        ret.append(len(neighbor_set))\n",
    "    return np.array(ret).reshape([-1, 1])\n",
    "\n",
    "# Load data\n",
    "node_num = adj.shape[0]\n",
    "features = np.arange(node_num)\n",
    "\n",
    "\n",
    "in_degree = np.sum(adj, axis=0).flatten()\n",
    "out_degree = np.sum(adj, axis=1).flatten()\n",
    "neighbor_count = count_neighbor(adj)\n",
    "degree = in_degree + out_degree\n",
    "second_order_adj = np.dot(adj, adj)\n",
    "second_order_degree = np.sum(second_order_adj, axis=0).flatten() + np.sum(second_order_adj, axis=1).flatten()\n",
    "\n",
    "\n",
    "labels_1 = in_degree.reshape([-1, 1])\n",
    "labels_2 = second_order_degree.reshape([-1, 1])\n",
    "idx = np.random.permutation(node_num)\n",
    "train_idx, val_idx = idx[:node_num//10], idx[node_num//10:]\n",
    "\n",
    "# \n",
    "features = torch.LongTensor(features)\n",
    "    \n",
    "labels_1 = torch.FloatTensor(labels_1)\n",
    "labels_2 = torch.FloatTensor(labels_2)\n",
    "label_neighbor_count = torch.FloatTensor(neighbor_count)\n",
    "# labels_1 = label_neighbor_count\n",
    "print(label_neighbor_count.size())\n",
    "# Model and optimizer\n",
    "model = GCN(\n",
    "            node_num=node_num,\n",
    "            nhid=128,\n",
    "            nclass=2,\n",
    "            dropout_rate=0.3)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "# \n",
    "model.cuda()\n",
    "features = features.cuda()\n",
    "sym_adj = sym_adj.cuda()\n",
    "labels_1 = labels_1.cuda()\n",
    "labels_2 = labels_2.cuda()\n",
    "# \n",
    "for i in range(1,1000000):\n",
    "        \n",
    "    t = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    output, l1 = model(features, sym_adj)\n",
    "    loss_train = 1. * criterion(output[train_idx,0], labels_1[train_idx,0]) + \\\n",
    "                0.*criterion(output[train_idx,1], labels_2[train_idx,0]) \n",
    "    l1_loss = l1_criterion(l1, torch.zeros_like(l1).cuda())\n",
    "    l2_loss = criterion(l1, torch.zeros_like(l1).cuda())\n",
    "#     loss_train = criterion(output[train_idx,0], labels_1[train_idx,0])\n",
    "    a = 500\n",
    "    b = 3000\n",
    "    (loss_train + a*l1_loss + b*l2_loss).backward()\n",
    "#     print(a*l1_loss.item() ,b*l2_loss.item())\n",
    "#     (loss_train).backward()\n",
    "    optimizer.step()\n",
    "#     val\n",
    "    loss_val = 1. * criterion(output[val_idx,0], labels_1[val_idx,0]) + \\\n",
    "                0.*criterion(output[val_idx,1], labels_2[val_idx,0])\n",
    "#     loss_val = criterion(output[val_idx,0], labels_1[val_idx,0])\n",
    "\n",
    "    acc1 = accuracy_mse(output[val_idx,0], labels_1[val_idx,0])\n",
    "    acc2 = accuracy_mse(output[val_idx,1], labels_2[val_idx,0])\n",
    "    if i % 10 ==0:\n",
    "        print('Epoch: {:04d}'.format(i+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc1_val: {:.4f}'.format(acc1),\n",
    "              'acc2_val: {:.4f}'.format(acc2),\n",
    "              'time: {:.4f}s'.format((time.time() - t)*100))\n",
    "\n",
    "    nn.init.xavier_uniform_\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# output GCN emb\n",
    "with torch.no_grad():\n",
    "    output, l1 = model(features, sym_adj)\n",
    "l1 = l1.cpu().numpy()\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "with open('GCN.emb','w' ) as f:\n",
    "    f.write('%d %d\\n' % (node_num, l1.shape[1]) )\n",
    "    for i in range(l1.shape[0]):\n",
    "        f.write('%d' % rev_map[i])\n",
    "        for x in l1[i,:].flatten().tolist():\n",
    "            f.write(' %f' % x)\n",
    "        f.write('\\n')\n",
    "print 'done'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 (virtualenv)",
   "language": "python",
   "name": "python3.6.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
