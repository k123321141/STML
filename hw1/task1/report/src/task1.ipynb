{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((285789, 256), (88074, 256))\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# emb\n",
    "with open(join('./','t1.emb')) as f:\n",
    "\n",
    "    num_nodes, D = f.readline().strip().split(' ')\n",
    "    num_nodes = int(num_nodes)\n",
    "    D = int(D)\n",
    "    \n",
    "    ls = f.readlines()\n",
    "node_emb_dict = {}\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    node_id, emb = int(buf[0]), buf[1:]\n",
    "    x = np.asarray([float(i) for i in emb], dtype=np.float32)\n",
    "    node_emb_dict[node_id] = x\n",
    "    \n",
    "# training data\n",
    "with open(join(data_path,'t1-merge.txt')) as f:\n",
    "    ls = f.readlines()\n",
    "node_set = get_node_set(join(data_path,'t1-merge.txt'))\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "X = []\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    src, dst = int(buf[0]), int(buf[1])\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "    fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "    X.append(fea)\n",
    "X = np.vstack(X)\n",
    "\n",
    "# test data\n",
    "with open(join(data_path,'t1-test.txt')) as f:\n",
    "    ls = f.readlines()\n",
    "N2 = len(ls)\n",
    "test_X = []\n",
    "for i,l in enumerate(ls):\n",
    "    buf = l.strip().split(' ')\n",
    "    src, dst = int(buf[0]), int(buf[1])\n",
    "    \n",
    "    if src not in node_emb_dict:\n",
    "        src = 6188\n",
    "        dst = 31366\n",
    "    if dst not in node_emb_dict:\n",
    "        src = 6188\n",
    "        dst = 31366\n",
    "    fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "    \n",
    "    test_X.append(fea)\n",
    "test_X = np.vstack(test_X)\n",
    "print(X.shape, test_X.shape)\n",
    "print 'done'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256) (256, 1)\n",
      "(140, 256) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 128\n",
    "def naive_bootsrap_generator(X, adj_mat, idx_map, node_emb_dict, train_node_set, batch_size=128, neg_rate=1. ):\n",
    "    train_node_list = list(train_node_set)\n",
    "    train_N = len(train_node_list)\n",
    "    num_edge = X.shape[0]\n",
    "        \n",
    "    while True:\n",
    "        idx = np.random.choice(num_edge, batch_size)\n",
    "        pos_X = X[idx, :]\n",
    "        \n",
    "        neg_count = int(batch_size*neg_rate)\n",
    "        neg_idx = np.random.randint(train_N, size=[neg_count, 2])\n",
    "        neg_X = []\n",
    "        for i in range(neg_count):\n",
    "            src, dst = neg_idx[i]\n",
    "            src = train_node_list[src]\n",
    "            dst = train_node_list[dst]\n",
    "            if src != dst and adj_mat[idx_map[src], idx_map[dst]] == 0:\n",
    "                fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "                neg_X.append(fea)\n",
    "        neg_X = np.vstack(neg_X)\n",
    "\n",
    "        ret_X = np.vstack([pos_X, neg_X])\n",
    "        ret_Y = np.zeros([ret_X.shape[0], 1])\n",
    "        ret_Y[:batch_size, 0] = 1\n",
    "        yield ret_X, ret_Y\n",
    "\n",
    "N = X.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "train_idx = idx[N//10:]\n",
    "val_idx = idx[:N//10]\n",
    "\n",
    "train_X = X[train_idx,:]\n",
    "val_X = X[val_idx,:]\n",
    "\n",
    "train_node_set = get_node_set('./kaggle/t1-train.txt')\n",
    "G = naive_bootsrap_generator(train_X, adj_mat, idx_map, node_emb_dict, train_node_set, batch_size=batch_size)\n",
    "val_G = naive_bootsrap_generator(val_X, adj_mat, idx_map, node_emb_dict, train_node_set,batch_size=batch_size, neg_rate=0.1)\n",
    "x,y = next(G)\n",
    "print x.shape,y.shape\n",
    "x,y = next(val_G)\n",
    "print x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256) (256, 1)\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 1s 2ms/step - loss: 0.8596 - acc: 0.5820\n",
      "Epoch 1/1000\n",
      "1473/2009 [====================>.........] - ETA: 3s - loss: 0.1995 - acc: 0.9217"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "                                    # (nothing gets printed in Jupyter, only if you run it standalone)\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "epochs = 100\n",
    "def build_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='selu', input_shape=(256,)))\n",
    "    model.add(Dense(256, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='selu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "x,y = next(G)\n",
    "print x.shape, y.shape\n",
    "np.random.seed(1337)\n",
    "model = build_model()\n",
    "model.fit(x,y)\n",
    "ck = keras.callbacks.ModelCheckpoint('./weights.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "tfb = keras.callbacks.TensorBoard(log_dir='./logs', )\n",
    "model.fit_generator(G,\n",
    "                    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "                    epochs=1000, verbose=1,\n",
    "                    validation_data=val_G,\n",
    "                    validation_steps=val_X.shape[0]//batch_size,\n",
    "                    callbacks=[ck,tfb]\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "done\n",
      "(88074, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "model = load_model('./weights.hdf5')\n",
    "\n",
    "z = model.predict(test_X)\n",
    "with open('pred.txt', 'w') as f:\n",
    "    for i in range(z.shape[0]):\n",
    "        p = z[i,0]\n",
    "        ans = 1 if p >= 0.5 else 0\n",
    "        f.write('%d\\n' % ans)\n",
    "pred_file = 'pred.txt'\n",
    "with open(pred_file, 'r') as f, open(pred_file + '.csv', 'w') as g:\n",
    "    g.write('query_id,prediction\\n')\n",
    "    for idx, line in enumerate(f):\n",
    "        g.write('%d,%d\\n' % (1 + idx, int(line)))\n",
    "#         print idx\n",
    "        \n",
    "\n",
    "\n",
    "print 'done'\n",
    "print z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714296, 256)\n",
      "prob done\n"
     ]
    }
   ],
   "source": [
    "from svm import *\n",
    "from svmutil import *\n",
    "# prepare data for SVM\n",
    "def generate_neg_X(X, adj_mat, node_emb_dict,neg_rate=1.5):\n",
    "    exist_node_list = node_emb_dict.keys()\n",
    "    exist_N = len(exist_node_list)\n",
    "    num_edge = X.shape[0]\n",
    "        \n",
    "    pos_X = X\n",
    "\n",
    "    neg_count = int(num_edge*neg_rate)\n",
    "    neg_idx = np.random.randint(exist_N, size=[neg_count, 2])\n",
    "    neg_X = []\n",
    "    for i in range(neg_count):\n",
    "        src, dst = neg_idx[i]\n",
    "        src = exist_node_list[src]\n",
    "        dst = exist_node_list[dst]\n",
    "        if src != dst and adj_mat[src, dst] == 0:\n",
    "            fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "            neg_X.append(fea)\n",
    "    neg_X = np.vstack(neg_X)\n",
    "\n",
    "    ret_X = np.vstack([pos_X, neg_X])\n",
    "    ret_Y = np.ones([ret_X.shape[0], 1])\n",
    "    ret_Y[pos_X.shape[0]:, 0] = -1\n",
    "    return ret_X, ret_Y\n",
    "train_X, train_Y = generate_neg_X(X, adj_mat, node_emb_dict,neg_rate=1.5)\n",
    "print train_X.shape\n",
    "prob = svm_problem(train_Y.flatten(),train_X)\n",
    "print 'prob done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-s 0 -t 2 -m 3000')\n",
    "m = svm_train(prob, param)\n",
    "p_labels, p_acc, p_vals = svm_predict([], test_X, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Link Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 475946/571578 [00:01<00:00, 360325.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 475946\n"
     ]
    }
   ],
   "source": [
    "# randomly sample test link\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# training data\n",
    "train_node_set = get_node_set(join(data_path,'t1-train.txt'))\n",
    "test_node_set = get_node_set(join(data_path,'t1-test-seen.txt'))\n",
    "node_set = set.union(train_node_set, test_node_set)\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t1-merge.txt'), dtype=np.int32)\n",
    "for i in range(links.shape[0]):\n",
    "    \n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "out_degree = np.sum(adj_mat, axis=1).flatten()\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "total_link_num = links.shape[0] + int(np.sum(out_degree))\n",
    "with tqdm(total=total_link_num) as pbar:\n",
    "    with open(join(data_path,'t1-fake.txt'), 'w') as f:\n",
    "        for i in range(links.shape[0]):\n",
    "            src, dst = links[i].tolist()\n",
    "            s = '%d %d\\n' % (src, dst)\n",
    "            f.write(s)\n",
    "            pbar.update(1)\n",
    "        train_node_list = list(train_node_set)\n",
    "        for node_id in list(test_node_set):\n",
    "            i = idx_map[node_id]\n",
    "            d = out_degree[i]\n",
    "            \n",
    "            for j in range(d):\n",
    "                idx = np.random.randint(len(train_node_list))\n",
    "                dst = idx_map[train_node_list[idx]]\n",
    "                while adj_mat[i, dst] == 1 or dst == i:\n",
    "                    idx = np.random.randint(len(train_node_list))\n",
    "                    dst = idx_map[train_node_list[idx]]\n",
    "                \n",
    "                adj_mat[i, dst] = 1\n",
    "                s = '%d %d\\n' % (rev_map[i], rev_map[dst])\n",
    "                f.write(s)\n",
    "            \n",
    "                pbar.update(1)\n",
    "    \n",
    "print 'done', np.sum(adj_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file ./kaggle/t1-fake.txt ...\n",
      "torch.Size([29402, 128]) 29402\n",
      "('Epoch: 0011', 'loss_train: 546.4982', 'loss_val: 527.1210', 'acc1_val: 0.0000', 'time: 1.9512s')\n",
      "('Epoch: 0021', 'loss_train: 517.0976', 'loss_val: 503.0954', 'acc1_val: 0.0000', 'time: 3.8818s')\n",
      "('Epoch: 0031', 'loss_train: 518.0991', 'loss_val: 505.3470', 'acc1_val: 0.0000', 'time: 5.8127s')\n",
      "('Epoch: 0041', 'loss_train: 516.2365', 'loss_val: 501.8302', 'acc1_val: 0.0000', 'time: 7.7438s')\n",
      "('Epoch: 0051', 'loss_train: 514.1308', 'loss_val: 500.7548', 'acc1_val: 0.0000', 'time: 9.6746s')\n",
      "('Epoch: 0061', 'loss_train: 511.6733', 'loss_val: 499.1846', 'acc1_val: 0.0000', 'time: 11.6054s')\n",
      "('Epoch: 0071', 'loss_train: 507.1001', 'loss_val: 495.6954', 'acc1_val: 0.0000', 'time: 13.5363s')\n",
      "('Epoch: 0081', 'loss_train: 501.9738', 'loss_val: 494.1317', 'acc1_val: 0.0000', 'time: 15.4673s')\n",
      "('Epoch: 0091', 'loss_train: 498.0366', 'loss_val: 494.4380', 'acc1_val: 0.0000', 'time: 17.3982s')\n",
      "('Epoch: 0101', 'loss_train: 494.9583', 'loss_val: 494.1017', 'acc1_val: 0.0000', 'time: 19.3290s')\n",
      "('Epoch: 0111', 'loss_train: 492.4157', 'loss_val: 493.7599', 'acc1_val: 0.0000', 'time: 21.2598s')\n",
      "('Epoch: 0121', 'loss_train: 491.3024', 'loss_val: 492.8986', 'acc1_val: 0.0000', 'time: 23.1906s')\n",
      "('Epoch: 0131', 'loss_train: 489.7607', 'loss_val: 493.6067', 'acc1_val: 0.0000', 'time: 25.1217s')\n",
      "('Epoch: 0141', 'loss_train: 488.7442', 'loss_val: 495.6129', 'acc1_val: 0.0000', 'time: 27.0528s')\n",
      "('Epoch: 0151', 'loss_train: 487.3207', 'loss_val: 494.2771', 'acc1_val: 0.0000', 'time: 28.9837s')\n",
      "('Epoch: 0161', 'loss_train: 485.6447', 'loss_val: 495.2579', 'acc1_val: 0.0000', 'time: 30.9148s')\n",
      "('Epoch: 0171', 'loss_train: 485.1468', 'loss_val: 496.8433', 'acc1_val: 0.0000', 'time: 32.8456s')\n",
      "('Epoch: 0181', 'loss_train: 483.0627', 'loss_val: 494.8748', 'acc1_val: 0.0000', 'time: 34.7764s')\n",
      "('Epoch: 0191', 'loss_train: 482.8929', 'loss_val: 495.1649', 'acc1_val: 0.0000', 'time: 36.7071s')\n",
      "('Epoch: 0201', 'loss_train: 480.1739', 'loss_val: 497.7747', 'acc1_val: 0.0000', 'time: 38.6384s')\n",
      "('Epoch: 0211', 'loss_train: 487.1826', 'loss_val: 497.1972', 'acc1_val: 0.0000', 'time: 40.5694s')\n",
      "('Epoch: 0221', 'loss_train: 480.6018', 'loss_val: 496.5762', 'acc1_val: 0.0000', 'time: 42.4998s')\n",
      "('Epoch: 0231', 'loss_train: 475.5695', 'loss_val: 499.5504', 'acc1_val: 0.0000', 'time: 44.4309s')\n",
      "('Epoch: 0241', 'loss_train: 473.2014', 'loss_val: 499.9448', 'acc1_val: 0.0000', 'time: 46.3621s')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-03a3d5c2b747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;31m#     print output.size(), labels.size(),  'out'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-03a3d5c2b747>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#         x = self.emb(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw1/task1/layers.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GCN version\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path, idx_map):\n",
    "    print('Loading from file {} ...'.format(path))\n",
    "\n",
    "\n",
    "    # build graph\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    N = len(idx_map)\n",
    "    \n",
    "#     print edges_unordered.shape\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "#     edges = np.vstack([edges, np.flip(edges, axis=1)])\n",
    "#     print edges.shape\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(N, N),\n",
    "                        dtype=np.float32)\n",
    "    src_adj = adj\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    return src_adj, adj\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "def accuracy_mse(output, labels):\n",
    "    correct = torch.abs(output - labels) < 0.5\n",
    "    return torch.sum(correct).item() / len(labels)\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "# print adj.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, node_num, nhid, nclass, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "#         self.emb = nn.Embedding(node_num, nhid)\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.mid = GraphConvolution(nhid, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x = self.emb(x)\n",
    "        x = F.selu(self.gc1(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        l1 = x = F.selu(self.mid(x, adj))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "        return x, l1\n",
    "\n",
    "\n",
    "def count_degree(adj):\n",
    "    node_num = adj.shape[0]\n",
    "    return \n",
    "def count_neighbor(adj):\n",
    "    N = adj.shape[0]\n",
    "    m = {i:[] for i in range(N)}\n",
    "    rows, cols, values = sp.find(adj)\n",
    "    for src,dst,v in zip(rows, cols, values):\n",
    "        if v == 1:\n",
    "            m[src].append(dst)\n",
    "            m[dst].append(src)\n",
    "    ret = []\n",
    "    for i in range(N):\n",
    "        neighbor_set = set(m[i])\n",
    "        ret.append(len(neighbor_set))\n",
    "    return np.array(ret).reshape([-1, 1])\n",
    "def sparse_diag(degree):\n",
    "    v = degree.flatten()\n",
    "    N = v.size()[0]\n",
    "    i = torch.cat([torch.arange(N).view([1,-1]), torch.arange(N).view([1,-1])], dim=0)\n",
    "    sp = torch.sparse.FloatTensor(i, v, torch.Size([N,N]))\n",
    "    return sp\n",
    "\n",
    "# Load data\n",
    "adj, sym_adj = load_data('./kaggle/t1-fake.txt', idx_map)\n",
    "node_num = adj.shape[0]\n",
    "\n",
    "\n",
    "in_degree = np.sum(adj, axis=0).flatten()\n",
    "out_degree = np.sum(adj, axis=1).flatten()\n",
    "neighbor_count = count_neighbor(adj)\n",
    "degree = in_degree + out_degree\n",
    "second_order_adj = np.dot(adj, adj)\n",
    "second_order_degree = np.sum(second_order_adj, axis=0).flatten() + np.sum(second_order_adj, axis=1).flatten()\n",
    "\n",
    "\n",
    "degree = degree.reshape([-1, 1])\n",
    "\n",
    "second_order_degree = second_order_degree.reshape([-1, 1])\n",
    "idx = np.random.permutation(node_num)\n",
    "train_idx, val_idx = idx[:node_num//10], idx[node_num//10:]\n",
    "\n",
    "# \n",
    "# features = torch.LongTensor(features)\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "features = torch.FloatTensor(np.vstack([node_emb_dict[rev_map[i]] for i in range(node_num)]))\n",
    "# features = torch.arange(node_num)\n",
    "\n",
    "degree = torch.FloatTensor(degree)\n",
    "out_degree = torch.FloatTensor(out_degree)\n",
    "label_neighbor_count = torch.FloatTensor(neighbor_count)\n",
    "\n",
    "# labels = label_neighbor_count\n",
    "labels = out_degree\n",
    "# labels = torch.FloatTensor(in_degree.reshape([-1,1]))\n",
    "# Model and optimizer\n",
    "model = GCN(\n",
    "            node_num=node_num,\n",
    "            nhid=128,\n",
    "            nclass=1,\n",
    "            dropout_rate=0.15)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.0087)\n",
    "criterion = nn.MSELoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "# \n",
    "model.cuda()\n",
    "features = features.cuda()\n",
    "sym_adj = sym_adj.cuda()\n",
    "labels = labels.view([-1,1]).cuda()\n",
    "\n",
    "reg_adj = sparse_diag(degree).cuda()\n",
    "print features.shape, node_num\n",
    "# \n",
    "t = time.time()\n",
    "    \n",
    "for i in range(1,1000):\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    output,_ = model(features, sym_adj)\n",
    "#     print output.size(), labels.size(),  'out'\n",
    "    loss_train = criterion(output[train_idx,:], labels[train_idx,:] )\n",
    "#     gcn_reg_loss = torch.sum( torch.mm(output.t(), torch.spmm(reg_adj, output)) )\n",
    "#     l1_loss = l1_criterion(l1, torch.zeros_like(l1).cuda())\n",
    "#     l2_loss = criterion(l1, torch.zeros_like(l1).cuda())\n",
    "#     loss_train = criterion(output[train_idx,0], labels_1[train_idx,0])\n",
    "#     a = 0.000\n",
    "#     (loss_train + a*gcn_reg_loss).backward()\n",
    "    (loss_train).backward()\n",
    "    optimizer.step()\n",
    "#     break\n",
    "#     val\n",
    "    model.eval()\n",
    "    loss_val = criterion(output[val_idx,:], labels[val_idx,:]) \n",
    "\n",
    "    acc1 = accuracy_mse(output[val_idx,:], labels[val_idx,:])\n",
    "    if i % 10 ==0:\n",
    "#         print( a*gcn_reg_loss.item())\n",
    "        print('Epoch: {:04d}'.format(i+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc1_val: {:.4f}'.format(acc1),\n",
    "              'time: {:.4f}s'.format((time.time() - t)))\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# output GCN emb\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, l1 = model(features, sym_adj)\n",
    "l1 = l1.cpu().numpy()\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "with open('GCN.emb','w' ) as f:\n",
    "    f.write('%d %d\\n' % (node_num, l1.shape[1]) )\n",
    "    for i in range(l1.shape[0]):\n",
    "        f.write('%d' % rev_map[i])\n",
    "        for x in l1[i,:].flatten().tolist():\n",
    "            f.write(' %f' % x)\n",
    "        f.write('\\n')\n",
    "print 'done'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
