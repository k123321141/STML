{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((285789, 256), (88074, 256))\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# emb\n",
    "with open(join('./','t1.emb')) as f:\n",
    "\n",
    "    num_nodes, D = f.readline().strip().split(' ')\n",
    "    num_nodes = int(num_nodes)\n",
    "    D = int(D)\n",
    "    \n",
    "    ls = f.readlines()\n",
    "node_emb_dict = {}\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    node_id, emb = int(buf[0]), buf[1:]\n",
    "    x = np.asarray([float(i) for i in emb], dtype=np.float32)\n",
    "    node_emb_dict[node_id] = x\n",
    "    \n",
    "# training data\n",
    "with open(join(data_path,'t1-merge.txt')) as f:\n",
    "    ls = f.readlines()\n",
    "node_set = get_node_set(join(data_path,'t1-merge.txt'))\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "X = []\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "for l in ls:\n",
    "    buf = l.strip().split(' ')\n",
    "    src, dst = int(buf[0]), int(buf[1])\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "    fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "    X.append(fea)\n",
    "X = np.vstack(X)\n",
    "\n",
    "# test data\n",
    "with open(join(data_path,'t1-test.txt')) as f:\n",
    "    ls = f.readlines()\n",
    "N2 = len(ls)\n",
    "test_X = []\n",
    "for i,l in enumerate(ls):\n",
    "    buf = l.strip().split(' ')\n",
    "    src, dst = int(buf[0]), int(buf[1])\n",
    "    \n",
    "    if src not in node_emb_dict:\n",
    "        src = 6188\n",
    "        dst = 31366\n",
    "    if dst not in node_emb_dict:\n",
    "        src = 6188\n",
    "        dst = 31366\n",
    "    fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "    \n",
    "    test_X.append(fea)\n",
    "test_X = np.vstack(test_X)\n",
    "print(X.shape, test_X.shape)\n",
    "print 'done'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256) (256, 1)\n",
      "(140, 256) (140, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_size = 128\n",
    "def naive_bootsrap_generator(X, adj_mat, idx_map, node_emb_dict, train_node_set, batch_size=128, neg_rate=1. ):\n",
    "    train_node_list = list(train_node_set)\n",
    "    train_N = len(train_node_list)\n",
    "    num_edge = X.shape[0]\n",
    "        \n",
    "    while True:\n",
    "        idx = np.random.choice(num_edge, batch_size)\n",
    "        pos_X = X[idx, :]\n",
    "        \n",
    "        neg_count = int(batch_size*neg_rate)\n",
    "        neg_idx = np.random.randint(train_N, size=[neg_count, 2])\n",
    "        neg_X = []\n",
    "        for i in range(neg_count):\n",
    "            src, dst = neg_idx[i]\n",
    "            src = train_node_list[src]\n",
    "            dst = train_node_list[dst]\n",
    "            if src != dst and adj_mat[idx_map[src], idx_map[dst]] == 0:\n",
    "                fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "                neg_X.append(fea)\n",
    "        neg_X = np.vstack(neg_X)\n",
    "\n",
    "        ret_X = np.vstack([pos_X, neg_X])\n",
    "        ret_Y = np.zeros([ret_X.shape[0], 1])\n",
    "        ret_Y[:batch_size, 0] = 1\n",
    "        yield ret_X, ret_Y\n",
    "\n",
    "N = X.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "train_idx = idx[N//10:]\n",
    "val_idx = idx[:N//10]\n",
    "\n",
    "train_X = X[train_idx,:]\n",
    "val_X = X[val_idx,:]\n",
    "\n",
    "train_node_set = get_node_set('./kaggle/t1-train.txt')\n",
    "G = naive_bootsrap_generator(train_X, adj_mat, idx_map, node_emb_dict, train_node_set, batch_size=batch_size)\n",
    "val_G = naive_bootsrap_generator(val_X, adj_mat, idx_map, node_emb_dict, train_node_set,batch_size=batch_size, neg_rate=0.1)\n",
    "x,y = next(G)\n",
    "print x.shape,y.shape\n",
    "x,y = next(val_G)\n",
    "print x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256) (256, 1)\n",
      "Epoch 1/1\n",
      "256/256 [==============================] - 1s 3ms/step - loss: 0.9252 - acc: 0.5430\n",
      "Epoch 1/1000\n",
      "2009/2009 [==============================] - 9s 4ms/step - loss: 0.3847 - acc: 0.8205 - val_loss: 0.2580 - val_acc: 0.8988\n",
      "Epoch 2/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.2668 - acc: 0.8902 - val_loss: 0.1711 - val_acc: 0.9381\n",
      "Epoch 3/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.2185 - acc: 0.9130 - val_loss: 0.1955 - val_acc: 0.9232\n",
      "Epoch 4/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1857 - acc: 0.9279 - val_loss: 0.1372 - val_acc: 0.9506\n",
      "Epoch 5/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1659 - acc: 0.9362 - val_loss: 0.1224 - val_acc: 0.9597\n",
      "Epoch 6/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1489 - acc: 0.9437 - val_loss: 0.1197 - val_acc: 0.9564\n",
      "Epoch 7/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1379 - acc: 0.9483 - val_loss: 0.1187 - val_acc: 0.9570\n",
      "Epoch 8/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1278 - acc: 0.9524 - val_loss: 0.1365 - val_acc: 0.9519\n",
      "Epoch 9/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1195 - acc: 0.9560 - val_loss: 0.1141 - val_acc: 0.9661\n",
      "Epoch 10/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1138 - acc: 0.9581 - val_loss: 0.1190 - val_acc: 0.9570\n",
      "Epoch 11/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1087 - acc: 0.9604 - val_loss: 0.1087 - val_acc: 0.9628\n",
      "Epoch 12/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.1030 - acc: 0.9626 - val_loss: 0.0908 - val_acc: 0.9705\n",
      "Epoch 13/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0992 - acc: 0.9640 - val_loss: 0.0821 - val_acc: 0.9714\n",
      "Epoch 14/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0941 - acc: 0.9659 - val_loss: 0.0921 - val_acc: 0.9711\n",
      "Epoch 15/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0923 - acc: 0.9668 - val_loss: 0.1068 - val_acc: 0.9648\n",
      "Epoch 16/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0871 - acc: 0.9689 - val_loss: 0.0929 - val_acc: 0.9690\n",
      "Epoch 17/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0847 - acc: 0.9698 - val_loss: 0.0905 - val_acc: 0.9685\n",
      "Epoch 18/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0828 - acc: 0.9705 - val_loss: 0.0884 - val_acc: 0.9729\n",
      "Epoch 19/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0799 - acc: 0.9716 - val_loss: 0.0956 - val_acc: 0.9706\n",
      "Epoch 20/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0780 - acc: 0.9723 - val_loss: 0.1048 - val_acc: 0.9664\n",
      "Epoch 21/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0748 - acc: 0.9738 - val_loss: 0.0969 - val_acc: 0.9686\n",
      "Epoch 22/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0736 - acc: 0.9739 - val_loss: 0.0927 - val_acc: 0.9728\n",
      "Epoch 23/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0714 - acc: 0.9750 - val_loss: 0.0951 - val_acc: 0.9657\n",
      "Epoch 24/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0697 - acc: 0.9758 - val_loss: 0.0900 - val_acc: 0.9710\n",
      "Epoch 25/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0679 - acc: 0.9765 - val_loss: 0.0796 - val_acc: 0.9731\n",
      "Epoch 26/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0665 - acc: 0.9767 - val_loss: 0.0930 - val_acc: 0.9693\n",
      "Epoch 27/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0647 - acc: 0.9777 - val_loss: 0.0958 - val_acc: 0.9691\n",
      "Epoch 28/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0633 - acc: 0.9784 - val_loss: 0.0819 - val_acc: 0.9727\n",
      "Epoch 29/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0629 - acc: 0.9784 - val_loss: 0.0620 - val_acc: 0.9810\n",
      "Epoch 30/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0615 - acc: 0.9789 - val_loss: 0.0987 - val_acc: 0.9701\n",
      "Epoch 31/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0598 - acc: 0.9794 - val_loss: 0.1100 - val_acc: 0.9695\n",
      "Epoch 32/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0585 - acc: 0.9802 - val_loss: 0.0823 - val_acc: 0.9756\n",
      "Epoch 33/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0579 - acc: 0.9801 - val_loss: 0.0911 - val_acc: 0.9725\n",
      "Epoch 34/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0574 - acc: 0.9807 - val_loss: 0.0827 - val_acc: 0.9760\n",
      "Epoch 35/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0557 - acc: 0.9810 - val_loss: 0.0779 - val_acc: 0.9768\n",
      "Epoch 36/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0547 - acc: 0.9816 - val_loss: 0.1001 - val_acc: 0.9712\n",
      "Epoch 37/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0543 - acc: 0.9818 - val_loss: 0.0990 - val_acc: 0.9712\n",
      "Epoch 38/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0539 - acc: 0.9817 - val_loss: 0.0695 - val_acc: 0.9796\n",
      "Epoch 39/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0529 - acc: 0.9824 - val_loss: 0.0751 - val_acc: 0.9772\n",
      "Epoch 40/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0522 - acc: 0.9825 - val_loss: 0.1039 - val_acc: 0.9700\n",
      "Epoch 41/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0515 - acc: 0.9827 - val_loss: 0.0680 - val_acc: 0.9782\n",
      "Epoch 42/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0508 - acc: 0.9831 - val_loss: 0.0956 - val_acc: 0.9735\n",
      "Epoch 43/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0496 - acc: 0.9834 - val_loss: 0.0709 - val_acc: 0.9785\n",
      "Epoch 44/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0486 - acc: 0.9838 - val_loss: 0.1141 - val_acc: 0.9664\n",
      "Epoch 45/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0486 - acc: 0.9838 - val_loss: 0.0681 - val_acc: 0.9796\n",
      "Epoch 46/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0480 - acc: 0.9843 - val_loss: 0.0713 - val_acc: 0.9794\n",
      "Epoch 47/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0476 - acc: 0.9843 - val_loss: 0.1016 - val_acc: 0.9745\n",
      "Epoch 48/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0477 - acc: 0.9845 - val_loss: 0.0904 - val_acc: 0.9748\n",
      "Epoch 49/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0467 - acc: 0.9846 - val_loss: 0.0848 - val_acc: 0.9768\n",
      "Epoch 50/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0456 - acc: 0.9851 - val_loss: 0.0892 - val_acc: 0.9750\n",
      "Epoch 51/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0459 - acc: 0.9849 - val_loss: 0.1021 - val_acc: 0.9715\n",
      "Epoch 52/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0454 - acc: 0.9852 - val_loss: 0.0645 - val_acc: 0.9802\n",
      "Epoch 53/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0445 - acc: 0.9856 - val_loss: 0.0937 - val_acc: 0.9728\n",
      "Epoch 54/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0447 - acc: 0.9856 - val_loss: 0.1130 - val_acc: 0.9727\n",
      "Epoch 55/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0445 - acc: 0.9856 - val_loss: 0.0916 - val_acc: 0.9747\n",
      "Epoch 56/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0428 - acc: 0.9861 - val_loss: 0.0897 - val_acc: 0.9752\n",
      "Epoch 57/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0431 - acc: 0.9861 - val_loss: 0.0892 - val_acc: 0.9770\n",
      "Epoch 58/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0434 - acc: 0.9860 - val_loss: 0.0977 - val_acc: 0.9706\n",
      "Epoch 59/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0420 - acc: 0.9865 - val_loss: 0.0694 - val_acc: 0.9796\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0419 - acc: 0.9865 - val_loss: 0.0824 - val_acc: 0.9764\n",
      "Epoch 61/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0425 - acc: 0.9863 - val_loss: 0.1124 - val_acc: 0.9655\n",
      "Epoch 62/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0418 - acc: 0.9867 - val_loss: 0.0929 - val_acc: 0.9753\n",
      "Epoch 63/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0417 - acc: 0.9868 - val_loss: 0.1012 - val_acc: 0.9715\n",
      "Epoch 64/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0403 - acc: 0.9873 - val_loss: 0.1055 - val_acc: 0.9717\n",
      "Epoch 65/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0403 - acc: 0.9872 - val_loss: 0.0877 - val_acc: 0.9742\n",
      "Epoch 66/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0403 - acc: 0.9870 - val_loss: 0.1398 - val_acc: 0.9660\n",
      "Epoch 67/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0396 - acc: 0.9874 - val_loss: 0.1206 - val_acc: 0.9716\n",
      "Epoch 68/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0398 - acc: 0.9875 - val_loss: 0.0969 - val_acc: 0.9735\n",
      "Epoch 69/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.0802 - val_acc: 0.9770\n",
      "Epoch 70/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0385 - acc: 0.9876 - val_loss: 0.1177 - val_acc: 0.9709\n",
      "Epoch 71/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0389 - acc: 0.9878 - val_loss: 0.1117 - val_acc: 0.9715\n",
      "Epoch 72/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0386 - acc: 0.9879 - val_loss: 0.1112 - val_acc: 0.9709\n",
      "Epoch 73/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0381 - acc: 0.9879 - val_loss: 0.1160 - val_acc: 0.9688\n",
      "Epoch 74/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0384 - acc: 0.9879 - val_loss: 0.1250 - val_acc: 0.9681\n",
      "Epoch 75/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0378 - acc: 0.9880 - val_loss: 0.0804 - val_acc: 0.9777\n",
      "Epoch 76/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0380 - acc: 0.9882 - val_loss: 0.1005 - val_acc: 0.9700\n",
      "Epoch 77/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0382 - acc: 0.9880 - val_loss: 0.1114 - val_acc: 0.9711\n",
      "Epoch 78/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0373 - acc: 0.9883 - val_loss: 0.1433 - val_acc: 0.9640\n",
      "Epoch 79/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0368 - acc: 0.9885 - val_loss: 0.1067 - val_acc: 0.9721\n",
      "Epoch 80/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0364 - acc: 0.9887 - val_loss: 0.1241 - val_acc: 0.9687\n",
      "Epoch 81/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0366 - acc: 0.9885 - val_loss: 0.0959 - val_acc: 0.9768\n",
      "Epoch 82/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0369 - acc: 0.9885 - val_loss: 0.0888 - val_acc: 0.9738\n",
      "Epoch 83/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0362 - acc: 0.9888 - val_loss: 0.1063 - val_acc: 0.9748\n",
      "Epoch 84/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0359 - acc: 0.9888 - val_loss: 0.1182 - val_acc: 0.9695\n",
      "Epoch 85/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0360 - acc: 0.9888 - val_loss: 0.1287 - val_acc: 0.9688\n",
      "Epoch 86/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0355 - acc: 0.9889 - val_loss: 0.1219 - val_acc: 0.9679\n",
      "Epoch 87/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0351 - acc: 0.9891 - val_loss: 0.1012 - val_acc: 0.9729\n",
      "Epoch 88/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0350 - acc: 0.9893 - val_loss: 0.1368 - val_acc: 0.9677\n",
      "Epoch 89/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0355 - acc: 0.9891 - val_loss: 0.0811 - val_acc: 0.9770\n",
      "Epoch 90/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0340 - acc: 0.9896 - val_loss: 0.1280 - val_acc: 0.9686\n",
      "Epoch 91/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0351 - acc: 0.9892 - val_loss: 0.1068 - val_acc: 0.9743\n",
      "Epoch 92/1000\n",
      "2009/2009 [==============================] - 9s 4ms/step - loss: 0.0346 - acc: 0.9892 - val_loss: 0.1065 - val_acc: 0.9732\n",
      "Epoch 93/1000\n",
      "2009/2009 [==============================] - 9s 4ms/step - loss: 0.0346 - acc: 0.9893 - val_loss: 0.1017 - val_acc: 0.9723\n",
      "Epoch 94/1000\n",
      "2009/2009 [==============================] - 10s 5ms/step - loss: 0.0345 - acc: 0.9895 - val_loss: 0.1553 - val_acc: 0.9620\n",
      "Epoch 95/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0968 - val_acc: 0.9754\n",
      "Epoch 96/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0351 - acc: 0.9892 - val_loss: 0.1159 - val_acc: 0.9701\n",
      "Epoch 97/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.1295 - val_acc: 0.9709\n",
      "Epoch 98/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0345 - acc: 0.9895 - val_loss: 0.1393 - val_acc: 0.9681\n",
      "Epoch 99/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0343 - acc: 0.9897 - val_loss: 0.0819 - val_acc: 0.9776\n",
      "Epoch 100/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0342 - acc: 0.9895 - val_loss: 0.1134 - val_acc: 0.9711\n",
      "Epoch 101/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0331 - acc: 0.9900 - val_loss: 0.1256 - val_acc: 0.9680\n",
      "Epoch 102/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0334 - acc: 0.9897 - val_loss: 0.0892 - val_acc: 0.9772\n",
      "Epoch 103/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0332 - acc: 0.9898 - val_loss: 0.1149 - val_acc: 0.9709\n",
      "Epoch 104/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0330 - acc: 0.9901 - val_loss: 0.1145 - val_acc: 0.9703\n",
      "Epoch 105/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0329 - acc: 0.9899 - val_loss: 0.1024 - val_acc: 0.9765\n",
      "Epoch 106/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0336 - acc: 0.9897 - val_loss: 0.1159 - val_acc: 0.9691\n",
      "Epoch 107/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0341 - acc: 0.9894 - val_loss: 0.1011 - val_acc: 0.9737\n",
      "Epoch 108/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0335 - acc: 0.9895 - val_loss: 0.0962 - val_acc: 0.9746\n",
      "Epoch 109/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0334 - acc: 0.9898 - val_loss: 0.0945 - val_acc: 0.9771\n",
      "Epoch 110/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0326 - acc: 0.9901 - val_loss: 0.1150 - val_acc: 0.9738\n",
      "Epoch 111/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0325 - acc: 0.9900 - val_loss: 0.0994 - val_acc: 0.9749\n",
      "Epoch 112/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0319 - acc: 0.9903 - val_loss: 0.1236 - val_acc: 0.9652\n",
      "Epoch 113/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0326 - acc: 0.9900 - val_loss: 0.1237 - val_acc: 0.9706\n",
      "Epoch 114/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0324 - acc: 0.9903 - val_loss: 0.1514 - val_acc: 0.9677\n",
      "Epoch 115/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0326 - acc: 0.9899 - val_loss: 0.1327 - val_acc: 0.9732\n",
      "Epoch 116/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0315 - acc: 0.9904 - val_loss: 0.1052 - val_acc: 0.9721\n",
      "Epoch 117/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0319 - acc: 0.9904 - val_loss: 0.1166 - val_acc: 0.9732\n",
      "Epoch 118/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0322 - acc: 0.9903 - val_loss: 0.1056 - val_acc: 0.9730\n",
      "Epoch 119/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0321 - acc: 0.9902 - val_loss: 0.1144 - val_acc: 0.9705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0323 - acc: 0.9902 - val_loss: 0.1118 - val_acc: 0.9727\n",
      "Epoch 121/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0319 - acc: 0.9904 - val_loss: 0.0970 - val_acc: 0.9736\n",
      "Epoch 122/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0307 - acc: 0.9907 - val_loss: 0.1257 - val_acc: 0.9700\n",
      "Epoch 123/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0317 - acc: 0.9904 - val_loss: 0.1262 - val_acc: 0.9666\n",
      "Epoch 124/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0317 - acc: 0.9905 - val_loss: 0.0842 - val_acc: 0.9759\n",
      "Epoch 125/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0322 - acc: 0.9903 - val_loss: 0.1080 - val_acc: 0.9727\n",
      "Epoch 126/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0320 - acc: 0.9904 - val_loss: 0.1351 - val_acc: 0.9715\n",
      "Epoch 127/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0311 - acc: 0.9906 - val_loss: 0.1038 - val_acc: 0.9725\n",
      "Epoch 128/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0328 - acc: 0.9900 - val_loss: 0.1367 - val_acc: 0.9678\n",
      "Epoch 129/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0316 - acc: 0.9905 - val_loss: 0.1052 - val_acc: 0.9751\n",
      "Epoch 130/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0312 - acc: 0.9906 - val_loss: 0.0997 - val_acc: 0.9735\n",
      "Epoch 131/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0316 - acc: 0.9905 - val_loss: 0.1081 - val_acc: 0.9724\n",
      "Epoch 132/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0311 - acc: 0.9906 - val_loss: 0.0864 - val_acc: 0.9759\n",
      "Epoch 133/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0315 - acc: 0.9905 - val_loss: 0.0553 - val_acc: 0.9824\n",
      "Epoch 134/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0303 - acc: 0.9909 - val_loss: 0.0915 - val_acc: 0.9732\n",
      "Epoch 135/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0313 - acc: 0.9906 - val_loss: 0.1228 - val_acc: 0.9708\n",
      "Epoch 136/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0312 - acc: 0.9907 - val_loss: 0.1595 - val_acc: 0.9678\n",
      "Epoch 137/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0311 - acc: 0.9908 - val_loss: 0.1015 - val_acc: 0.9757\n",
      "Epoch 138/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0309 - acc: 0.9906 - val_loss: 0.0823 - val_acc: 0.9804\n",
      "Epoch 139/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0303 - acc: 0.9909 - val_loss: 0.1236 - val_acc: 0.9710\n",
      "Epoch 140/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0306 - acc: 0.9908 - val_loss: 0.1159 - val_acc: 0.9725\n",
      "Epoch 141/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0305 - acc: 0.9908 - val_loss: 0.1359 - val_acc: 0.9673\n",
      "Epoch 142/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0320 - acc: 0.9905 - val_loss: 0.0936 - val_acc: 0.9759\n",
      "Epoch 143/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0306 - acc: 0.9909 - val_loss: 0.1104 - val_acc: 0.9682\n",
      "Epoch 144/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0306 - acc: 0.9909 - val_loss: 0.0973 - val_acc: 0.9705\n",
      "Epoch 145/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0306 - acc: 0.9910 - val_loss: 0.0990 - val_acc: 0.9719\n",
      "Epoch 146/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0323 - acc: 0.9903 - val_loss: 0.0971 - val_acc: 0.9734\n",
      "Epoch 147/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0306 - acc: 0.9908 - val_loss: 0.1168 - val_acc: 0.9750\n",
      "Epoch 148/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0309 - acc: 0.9907 - val_loss: 0.1099 - val_acc: 0.9695\n",
      "Epoch 149/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0314 - acc: 0.9908 - val_loss: 0.1157 - val_acc: 0.9727\n",
      "Epoch 150/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0304 - acc: 0.9908 - val_loss: 0.1046 - val_acc: 0.9720\n",
      "Epoch 151/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0292 - acc: 0.9914 - val_loss: 0.1170 - val_acc: 0.9713\n",
      "Epoch 152/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0320 - acc: 0.9902 - val_loss: 0.0838 - val_acc: 0.9765\n",
      "Epoch 153/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0302 - acc: 0.9910 - val_loss: 0.1199 - val_acc: 0.9707\n",
      "Epoch 154/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0311 - acc: 0.9907 - val_loss: 0.1085 - val_acc: 0.9733\n",
      "Epoch 155/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0298 - acc: 0.9912 - val_loss: 0.1176 - val_acc: 0.9720\n",
      "Epoch 156/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0323 - acc: 0.9902 - val_loss: 0.0940 - val_acc: 0.9757\n",
      "Epoch 157/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0293 - acc: 0.9913 - val_loss: 0.0940 - val_acc: 0.9742\n",
      "Epoch 158/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0323 - acc: 0.9905 - val_loss: 0.1008 - val_acc: 0.9738\n",
      "Epoch 159/1000\n",
      "2009/2009 [==============================] - 8s 4ms/step - loss: 0.0312 - acc: 0.9906 - val_loss: 0.0994 - val_acc: 0.9737\n",
      "Epoch 160/1000\n",
      "1740/2009 [========================>.....] - ETA: 1s - loss: 0.0301 - acc: 0.9910"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82a887668357>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_G\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mck\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtfb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                     )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1313\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2228\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2229\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1883\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1885\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m-> 1107\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m   1108\u001b[0m             raise ValueError('Cannot feed value of shape %r for Tensor %r, '\n\u001b[1;32m   1109\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \"\"\"\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "\n",
    "epochs = 100\n",
    "def build_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='selu', input_shape=(256,)))\n",
    "    model.add(Dense(256, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(128, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(64, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(32, activation='selu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='selu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['accuracy'])\n",
    "    return model\n",
    "x,y = next(G)\n",
    "print x.shape, y.shape\n",
    "np.random.seed(1337)\n",
    "model = build_model()\n",
    "model.fit(x,y)\n",
    "ck = keras.callbacks.ModelCheckpoint('./weights.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "tfb = keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "model.fit_generator(G,\n",
    "                    steps_per_epoch=train_X.shape[0]//batch_size,\n",
    "                    epochs=1000, verbose=1,\n",
    "                    validation_data=val_G,\n",
    "                    validation_steps=val_X.shape[0]//batch_size,\n",
    "                    callbacks=[ck,tfb]\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "done\n",
      "(88074, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "model = load_model('./weights.hdf5')\n",
    "\n",
    "z = model.predict(test_X)\n",
    "with open('pred.txt', 'w') as f:\n",
    "    for i in range(z.shape[0]):\n",
    "        p = z[i,0]\n",
    "        ans = 1 if p >= 0.5 else 0\n",
    "        f.write('%d\\n' % ans)\n",
    "pred_file = 'pred.txt'\n",
    "with open(pred_file, 'r') as f, open(pred_file + '.csv', 'w') as g:\n",
    "    g.write('query_id,prediction\\n')\n",
    "    for idx, line in enumerate(f):\n",
    "        g.write('%d,%d\\n' % (1 + idx, int(line)))\n",
    "#         print idx\n",
    "        \n",
    "\n",
    "\n",
    "print 'done'\n",
    "print z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(714296, 256)\n",
      "prob done\n"
     ]
    }
   ],
   "source": [
    "from svm import *\n",
    "from svmutil import *\n",
    "# prepare data for SVM\n",
    "def generate_neg_X(X, adj_mat, node_emb_dict,neg_rate=1.5):\n",
    "    exist_node_list = node_emb_dict.keys()\n",
    "    exist_N = len(exist_node_list)\n",
    "    num_edge = X.shape[0]\n",
    "        \n",
    "    pos_X = X\n",
    "\n",
    "    neg_count = int(num_edge*neg_rate)\n",
    "    neg_idx = np.random.randint(exist_N, size=[neg_count, 2])\n",
    "    neg_X = []\n",
    "    for i in range(neg_count):\n",
    "        src, dst = neg_idx[i]\n",
    "        src = exist_node_list[src]\n",
    "        dst = exist_node_list[dst]\n",
    "        if src != dst and adj_mat[src, dst] == 0:\n",
    "            fea = np.concatenate([node_emb_dict[src], node_emb_dict[dst]], axis=-1)\n",
    "            neg_X.append(fea)\n",
    "    neg_X = np.vstack(neg_X)\n",
    "\n",
    "    ret_X = np.vstack([pos_X, neg_X])\n",
    "    ret_Y = np.ones([ret_X.shape[0], 1])\n",
    "    ret_Y[pos_X.shape[0]:, 0] = -1\n",
    "    return ret_X, ret_Y\n",
    "train_X, train_Y = generate_neg_X(X, adj_mat, node_emb_dict,neg_rate=1.5)\n",
    "print train_X.shape\n",
    "prob = svm_problem(train_Y.flatten(),train_X)\n",
    "print 'prob done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = svm_parameter('-s 0 -t 2 -m 3000')\n",
    "m = svm_train(prob, param)\n",
    "p_labels, p_acc, p_vals = svm_predict([], test_X, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake Link Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 475946/571578 [00:01<00:00, 360325.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 475946\n"
     ]
    }
   ],
   "source": [
    "# randomly sample test link\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# training data\n",
    "train_node_set = get_node_set(join(data_path,'t1-train.txt'))\n",
    "test_node_set = get_node_set(join(data_path,'t1-test-seen.txt'))\n",
    "node_set = set.union(train_node_set, test_node_set)\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t1-merge.txt'), dtype=np.int32)\n",
    "for i in range(links.shape[0]):\n",
    "    \n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "out_degree = np.sum(adj_mat, axis=1).flatten()\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "total_link_num = links.shape[0] + int(np.sum(out_degree))\n",
    "with tqdm(total=total_link_num) as pbar:\n",
    "    with open(join(data_path,'t1-fake.txt'), 'w') as f:\n",
    "        for i in range(links.shape[0]):\n",
    "            src, dst = links[i].tolist()\n",
    "            s = '%d %d\\n' % (src, dst)\n",
    "            f.write(s)\n",
    "            pbar.update(1)\n",
    "        train_node_list = list(train_node_set)\n",
    "        for node_id in list(test_node_set):\n",
    "            i = idx_map[node_id]\n",
    "            d = out_degree[i]\n",
    "            \n",
    "            for j in range(d):\n",
    "                idx = np.random.randint(len(train_node_list))\n",
    "                dst = idx_map[train_node_list[idx]]\n",
    "                while adj_mat[i, dst] == 1 or dst == i:\n",
    "                    idx = np.random.randint(len(train_node_list))\n",
    "                    dst = idx_map[train_node_list[idx]]\n",
    "                \n",
    "                adj_mat[i, dst] = 1\n",
    "                s = '%d %d\\n' % (rev_map[i], rev_map[dst])\n",
    "                f.write(s)\n",
    "            \n",
    "                pbar.update(1)\n",
    "    \n",
    "print 'done', np.sum(adj_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from file ./kaggle/t1-fake.txt ...\n",
      "torch.Size([29402, 128]) 29402\n",
      "('Epoch: 0011', 'loss_train: 546.4982', 'loss_val: 527.1210', 'acc1_val: 0.0000', 'time: 1.9512s')\n",
      "('Epoch: 0021', 'loss_train: 517.0976', 'loss_val: 503.0954', 'acc1_val: 0.0000', 'time: 3.8818s')\n",
      "('Epoch: 0031', 'loss_train: 518.0991', 'loss_val: 505.3470', 'acc1_val: 0.0000', 'time: 5.8127s')\n",
      "('Epoch: 0041', 'loss_train: 516.2365', 'loss_val: 501.8302', 'acc1_val: 0.0000', 'time: 7.7438s')\n",
      "('Epoch: 0051', 'loss_train: 514.1308', 'loss_val: 500.7548', 'acc1_val: 0.0000', 'time: 9.6746s')\n",
      "('Epoch: 0061', 'loss_train: 511.6733', 'loss_val: 499.1846', 'acc1_val: 0.0000', 'time: 11.6054s')\n",
      "('Epoch: 0071', 'loss_train: 507.1001', 'loss_val: 495.6954', 'acc1_val: 0.0000', 'time: 13.5363s')\n",
      "('Epoch: 0081', 'loss_train: 501.9738', 'loss_val: 494.1317', 'acc1_val: 0.0000', 'time: 15.4673s')\n",
      "('Epoch: 0091', 'loss_train: 498.0366', 'loss_val: 494.4380', 'acc1_val: 0.0000', 'time: 17.3982s')\n",
      "('Epoch: 0101', 'loss_train: 494.9583', 'loss_val: 494.1017', 'acc1_val: 0.0000', 'time: 19.3290s')\n",
      "('Epoch: 0111', 'loss_train: 492.4157', 'loss_val: 493.7599', 'acc1_val: 0.0000', 'time: 21.2598s')\n",
      "('Epoch: 0121', 'loss_train: 491.3024', 'loss_val: 492.8986', 'acc1_val: 0.0000', 'time: 23.1906s')\n",
      "('Epoch: 0131', 'loss_train: 489.7607', 'loss_val: 493.6067', 'acc1_val: 0.0000', 'time: 25.1217s')\n",
      "('Epoch: 0141', 'loss_train: 488.7442', 'loss_val: 495.6129', 'acc1_val: 0.0000', 'time: 27.0528s')\n",
      "('Epoch: 0151', 'loss_train: 487.3207', 'loss_val: 494.2771', 'acc1_val: 0.0000', 'time: 28.9837s')\n",
      "('Epoch: 0161', 'loss_train: 485.6447', 'loss_val: 495.2579', 'acc1_val: 0.0000', 'time: 30.9148s')\n",
      "('Epoch: 0171', 'loss_train: 485.1468', 'loss_val: 496.8433', 'acc1_val: 0.0000', 'time: 32.8456s')\n",
      "('Epoch: 0181', 'loss_train: 483.0627', 'loss_val: 494.8748', 'acc1_val: 0.0000', 'time: 34.7764s')\n",
      "('Epoch: 0191', 'loss_train: 482.8929', 'loss_val: 495.1649', 'acc1_val: 0.0000', 'time: 36.7071s')\n",
      "('Epoch: 0201', 'loss_train: 480.1739', 'loss_val: 497.7747', 'acc1_val: 0.0000', 'time: 38.6384s')\n",
      "('Epoch: 0211', 'loss_train: 487.1826', 'loss_val: 497.1972', 'acc1_val: 0.0000', 'time: 40.5694s')\n",
      "('Epoch: 0221', 'loss_train: 480.6018', 'loss_val: 496.5762', 'acc1_val: 0.0000', 'time: 42.4998s')\n",
      "('Epoch: 0231', 'loss_train: 475.5695', 'loss_val: 499.5504', 'acc1_val: 0.0000', 'time: 44.4309s')\n",
      "('Epoch: 0241', 'loss_train: 473.2014', 'loss_val: 499.9448', 'acc1_val: 0.0000', 'time: 46.3621s')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-03a3d5c2b747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msym_adj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;31m#     print output.size(), labels.size(),  'out'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-03a3d5c2b747>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m#         x = self.emb(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw1/task1/layers.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, adj)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GCN version\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers import GraphConvolution\n",
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path, idx_map):\n",
    "    print('Loading from file {} ...'.format(path))\n",
    "\n",
    "\n",
    "    # build graph\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    N = len(idx_map)\n",
    "    \n",
    "#     print edges_unordered.shape\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "#     edges = np.vstack([edges, np.flip(edges, axis=1)])\n",
    "#     print edges.shape\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(N, N),\n",
    "                        dtype=np.float32)\n",
    "    src_adj = adj\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    return src_adj, adj\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "def accuracy_mse(output, labels):\n",
    "    correct = torch.abs(output - labels) < 0.5\n",
    "    return torch.sum(correct).item() / len(labels)\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "# print adj.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, node_num, nhid, nclass, dropout_rate):\n",
    "        super(GCN, self).__init__()\n",
    "#         self.emb = nn.Embedding(node_num, nhid)\n",
    "        self.gc1 = GraphConvolution(nhid, nhid)\n",
    "        self.mid = GraphConvolution(nhid, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "#         x = self.emb(x)\n",
    "        x = F.selu(self.gc1(x, adj))\n",
    "        x = self.dropout(x)\n",
    "        l1 = x = F.selu(self.mid(x, adj))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.gc2(x, adj)\n",
    "#         return F.log_softmax(x, dim=1)\n",
    "        return x, l1\n",
    "\n",
    "\n",
    "def count_degree(adj):\n",
    "    node_num = adj.shape[0]\n",
    "    return \n",
    "def count_neighbor(adj):\n",
    "    N = adj.shape[0]\n",
    "    m = {i:[] for i in range(N)}\n",
    "    rows, cols, values = sp.find(adj)\n",
    "    for src,dst,v in zip(rows, cols, values):\n",
    "        if v == 1:\n",
    "            m[src].append(dst)\n",
    "            m[dst].append(src)\n",
    "    ret = []\n",
    "    for i in range(N):\n",
    "        neighbor_set = set(m[i])\n",
    "        ret.append(len(neighbor_set))\n",
    "    return np.array(ret).reshape([-1, 1])\n",
    "def sparse_diag(degree):\n",
    "    v = degree.flatten()\n",
    "    N = v.size()[0]\n",
    "    i = torch.cat([torch.arange(N).view([1,-1]), torch.arange(N).view([1,-1])], dim=0)\n",
    "    sp = torch.sparse.FloatTensor(i, v, torch.Size([N,N]))\n",
    "    return sp\n",
    "\n",
    "# Load data\n",
    "adj, sym_adj = load_data('./kaggle/t1-fake.txt', idx_map)\n",
    "node_num = adj.shape[0]\n",
    "\n",
    "\n",
    "in_degree = np.sum(adj, axis=0).flatten()\n",
    "out_degree = np.sum(adj, axis=1).flatten()\n",
    "neighbor_count = count_neighbor(adj)\n",
    "degree = in_degree + out_degree\n",
    "second_order_adj = np.dot(adj, adj)\n",
    "second_order_degree = np.sum(second_order_adj, axis=0).flatten() + np.sum(second_order_adj, axis=1).flatten()\n",
    "\n",
    "\n",
    "degree = degree.reshape([-1, 1])\n",
    "\n",
    "second_order_degree = second_order_degree.reshape([-1, 1])\n",
    "idx = np.random.permutation(node_num)\n",
    "train_idx, val_idx = idx[:node_num//10], idx[node_num//10:]\n",
    "\n",
    "# \n",
    "# features = torch.LongTensor(features)\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "features = torch.FloatTensor(np.vstack([node_emb_dict[rev_map[i]] for i in range(node_num)]))\n",
    "# features = torch.arange(node_num)\n",
    "\n",
    "degree = torch.FloatTensor(degree)\n",
    "out_degree = torch.FloatTensor(out_degree)\n",
    "label_neighbor_count = torch.FloatTensor(neighbor_count)\n",
    "\n",
    "# labels = label_neighbor_count\n",
    "labels = out_degree\n",
    "# labels = torch.FloatTensor(in_degree.reshape([-1,1]))\n",
    "# Model and optimizer\n",
    "model = GCN(\n",
    "            node_num=node_num,\n",
    "            nhid=128,\n",
    "            nclass=1,\n",
    "            dropout_rate=0.15)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.0087)\n",
    "criterion = nn.MSELoss()\n",
    "l1_criterion = nn.L1Loss()\n",
    "# \n",
    "model.cuda()\n",
    "features = features.cuda()\n",
    "sym_adj = sym_adj.cuda()\n",
    "labels = labels.view([-1,1]).cuda()\n",
    "\n",
    "reg_adj = sparse_diag(degree).cuda()\n",
    "print features.shape, node_num\n",
    "# \n",
    "t = time.time()\n",
    "    \n",
    "for i in range(1,1000):\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    model.train()\n",
    "    output,_ = model(features, sym_adj)\n",
    "#     print output.size(), labels.size(),  'out'\n",
    "    loss_train = criterion(output[train_idx,:], labels[train_idx,:] )\n",
    "#     gcn_reg_loss = torch.sum( torch.mm(output.t(), torch.spmm(reg_adj, output)) )\n",
    "#     l1_loss = l1_criterion(l1, torch.zeros_like(l1).cuda())\n",
    "#     l2_loss = criterion(l1, torch.zeros_like(l1).cuda())\n",
    "#     loss_train = criterion(output[train_idx,0], labels_1[train_idx,0])\n",
    "#     a = 0.000\n",
    "#     (loss_train + a*gcn_reg_loss).backward()\n",
    "    (loss_train).backward()\n",
    "    optimizer.step()\n",
    "#     break\n",
    "#     val\n",
    "    model.eval()\n",
    "    loss_val = criterion(output[val_idx,:], labels[val_idx,:]) \n",
    "\n",
    "    acc1 = accuracy_mse(output[val_idx,:], labels[val_idx,:])\n",
    "    if i % 10 ==0:\n",
    "#         print( a*gcn_reg_loss.item())\n",
    "        print('Epoch: {:04d}'.format(i+1),\n",
    "              'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "              'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "              'acc1_val: {:.4f}'.format(acc1),\n",
    "              'time: {:.4f}s'.format((time.time() - t)))\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# output GCN emb\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, l1 = model(features, sym_adj)\n",
    "l1 = l1.cpu().numpy()\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "with open('GCN.emb','w' ) as f:\n",
    "    f.write('%d %d\\n' % (node_num, l1.shape[1]) )\n",
    "    for i in range(l1.shape[0]):\n",
    "        f.write('%d' % rev_map[i])\n",
    "        for x in l1[i,:].flatten().tolist():\n",
    "            f.write(' %f' % x)\n",
    "        f.write('\\n')\n",
    "print 'done'\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [0, 2, 0],\n",
       "        [0, 0, 3]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
