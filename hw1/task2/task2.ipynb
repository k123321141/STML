{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate fake link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 161619/173364 [00:00<00:00, 262344.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 161619\n"
     ]
    }
   ],
   "source": [
    "# randomly sample test link\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# training data\n",
    "train_node_set = get_node_set(join(data_path,'t2-train.txt'))\n",
    "test_node_set = get_node_set(join(data_path,'t2-test.txt')).difference(train_node_set)\n",
    "node_set = set.union(train_node_set, test_node_set)\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t2-train.txt'), dtype=np.int32)\n",
    "for i in range(links.shape[0]):\n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "out_degree = np.sum(adj_mat, axis=1).flatten()\n",
    "\n",
    "means, std = np.mean(out_degree), np.std(out_degree)\n",
    "\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "total_link_num = links.shape[0] + int(np.sum(out_degree))\n",
    "with tqdm(total=total_link_num) as pbar:\n",
    "    with open(join(data_path,'t2-fake.txt'), 'w') as f:\n",
    "        for i in range(links.shape[0]):\n",
    "            src, dst = links[i].tolist()\n",
    "            s = '%d %d\\n' % (src, dst)\n",
    "            f.write(s)\n",
    "            pbar.update(1)\n",
    "        train_node_list = list(train_node_set)\n",
    "        for node_id in list(test_node_set):\n",
    "            i = idx_map[node_id]\n",
    "            d = int(np.round(np.random.normal(means, std)))\n",
    "            d = max(1, d)\n",
    "            \n",
    "            for j in range(d):\n",
    "                idx = np.random.randint(len(train_node_list))\n",
    "                dst = idx_map[train_node_list[idx]]\n",
    "                while adj_mat[i, dst] == 1 or dst == i:\n",
    "                    idx = np.random.randint(len(train_node_list))\n",
    "                    dst = idx_map[train_node_list[idx]]\n",
    "                \n",
    "                adj_mat[i, dst] = 1\n",
    "                s = '%d %d\\n' % (rev_map[i], rev_map[dst])\n",
    "                f.write(s)\n",
    "            \n",
    "                pbar.update(1)\n",
    "    \n",
    "print 'done', np.sum(adj_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read all 17500 xml files.\n",
      "Found 82709 unique tokens.\n",
      "Preparing embedding matrix.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from os.path import join\n",
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from constants import MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "def quote_title_abstract(xml_path):\n",
    "    with open(xml_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    soup = BS(data)\n",
    "    title, abstract = soup.find('title').text, soup.find('abstract').text\n",
    "    return title.strip(), abstract.strip()\n",
    "\n",
    "# text preprocessing\n",
    "data_path = join('./','kaggle/')\n",
    "xml_dir = join(data_path, 't2-doc')\n",
    "xml_list = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "# print(len(xml_list))\n",
    "\n",
    "\n",
    "texts = []\n",
    "\n",
    "for xml in xml_list:\n",
    "    path = join(xml_dir,xml)\n",
    "    title, abstract = quote_title_abstract(path)\n",
    "    text = title + '' + abstract\n",
    "    texts.append(text)\n",
    "#     texts.append(title)\n",
    "#     texts.append(abstract)\n",
    "print('read all %d xml files.' % len(xml_list))\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "xml_id_map = {}\n",
    "for i,xml in enumerate(xml_list):\n",
    "    node_id = int(xml.replace('.xml',''))\n",
    "    xml_id_map[node_id] = data[i,:]\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "embeddings_index = {}\n",
    "# with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r', encoding='utf8') as f:\n",
    "with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb\n",
    "buf = np.genfromtxt('./t2.emb', skip_header=1, dtype=np.float32)\n",
    "nodes = buf[:,0].astype(np.int32)\n",
    "emb = buf[:,1:]\n",
    "\n",
    "node_emb_dict = {}\n",
    "for i in range(emb.shape[0]):\n",
    "    node_id = nodes[i]\n",
    "    x = emb[i,:]\n",
    "    node_emb_dict[node_id] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([1, 1])\n",
      "5583617\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "from constants import D_MODEL, STACKED_NUM,DK, DV, H, P_DROP, D_FF, MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "# environment\n",
    "with_gpu = torch.cuda.is_available()\n",
    "# with_gpu = False\n",
    "device = torch.device(\"cuda:0\" if with_gpu else \"cpu\")\n",
    "\n",
    "def positional_encoding(pos):\n",
    "    assert D_MODEL % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,D_MODEL], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "def get_pos_mat(length):\n",
    "    if length > MAX_SEQUENCE_LENGTH:\n",
    "        print('sequence length reach PE_MAT_CACHE. %d ' % length)\n",
    "        ret = torch.cat([positional_encoding(i) for i in range(length)], dim=0).to(device)\n",
    "        ret.requires_grad = False\n",
    "        global PE_CACHE_MATRIX\n",
    "        PE_CACHE_MATRIX = ret\n",
    "        return ret\n",
    "    else:\n",
    "        return PE_CACHE_MATRIX[:length]\n",
    "    \n",
    "PE_CACHE_MATRIX = torch.cat([positional_encoding(i) for i in range(0,MAX_SEQUENCE_LENGTH)], dim=0).to(device)\n",
    "PE_CACHE_MATRIX.requires_grad = False\n",
    "\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask=None):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).to(device)\n",
    "    out = torch.matmul(Q,K.t()) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out = out.masked_fill_(mask, -float('inf'))\n",
    "        \n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "                            \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, emb_matrix):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.emb = Word_Embedding(emb_matrix)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "        self.encoder = Stack_Encoder(layer_num, dk, dv, dm, h)\n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h)\n",
    "        self.summary_decoder = Stack_Decoder(2, dk, dv, dm, h)\n",
    "        \n",
    "        self.summary_weight = nn.Parameter(torch.FloatTensor(1, dm))\n",
    "        torch.nn.init.xavier_uniform_(self.summary_weight)\n",
    "        \n",
    "        self.output_linear = nn.Linear(3*dm, 1)\n",
    "\n",
    "    def forward(self, Q, K, Q_fea, K_fea):\n",
    "        \n",
    "#         encoder\n",
    "        K = self.emb(K)\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        K = K + get_pos_mat(MAX_SEQUENCE_LENGTH)\n",
    "        K = self.emb_drop(K)\n",
    "        \n",
    "        en_out = self.encoder(K)\n",
    "        \n",
    "#         decoder\n",
    "        Q = self.emb(Q)\n",
    "        seq_len, d = Q.size()\n",
    "        \n",
    "        Q = Q + get_pos_mat(MAX_SEQUENCE_LENGTH)\n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q, en_out)\n",
    "        \n",
    "        \n",
    "        summary = self.summary_decoder(self.summary_weight, de_out)\n",
    "        x = torch.cat([summary, Q_fea.view([1,-1]), K_fea.view([1,-1])], dim=-1)\n",
    "        out = self.output_linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "class Word_Embedding(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super(Word_Embedding, self).__init__()\n",
    "        self.emb = nn.Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, padding_idx=0)\n",
    "        self.emb.weight = nn.parameter.Parameter(torch.FloatTensor(emb_matrix))\n",
    "        self.emb.weight.requires_grad_(False)\n",
    "        \n",
    "        self.linear = nn.Linear(EMBEDDING_DIM, D_MODEL, bias=False)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class Stack_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h):\n",
    "        super(Stack_Encoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([Encoder(dk, dv, dm, h) for i in range(layer_num)])\n",
    "\n",
    "    def forward(self, K):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.encoders:\n",
    "            K = lay(K)\n",
    "        return K               \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Encoder, self).__init__()\n",
    "#         attention residual block\n",
    "        self.multi_head_attention_layer = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.attention_norm_lay = nn.LayerNorm([dm,])\n",
    "        self.att_drop = nn.Dropout(P_DROP)\n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(D_MODEL, D_FF)\n",
    "        self.linear_drop = nn.Dropout(P_DROP)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        \n",
    "\n",
    "    def forward(self, K):\n",
    "#         attention\n",
    "        attention_out = self.multi_head_attention_layer(K, K, K)\n",
    "        attention_out = self.att_drop(attention_out)\n",
    "        att_out = self.attention_norm_lay(K + attention_out)\n",
    "#         feed forward\n",
    "        linear_out = self.fcn(att_out)\n",
    "        linear_out = self.linear_drop(linear_out)\n",
    "        out = self.ff_norm_lay(att_out + linear_out)\n",
    "        out = att_out + linear_out\n",
    "    \n",
    "        return out\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q, encoder_out):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        Q_len, d = Q.size()\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q, encoder_out, mask=None)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Decoder, self).__init__()\n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.Q_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.Q_att_drop = nn.Dropout(P_DROP)\n",
    "    \n",
    "#         query key attention residual block\n",
    "        self.QK_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.QK_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.QK_att_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "    \n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(D_MODEL, D_FF)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.linear_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "\n",
    "    def forward(self, Q, encoder_out, mask):\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(Q, Q, Q, mask)\n",
    "        Q_attention_out = self.Q_att_drop(Q_attention_out)\n",
    "        Q_att_out = self.Q_attention_norm_lay(Q + Q_attention_out)\n",
    "#         query key attention\n",
    "        QK_attention_out = self.QK_attention_lay(Q_att_out, encoder_out, encoder_out)\n",
    "        QK_attention_out = self.QK_att_drop(QK_attention_out)\n",
    "        QK_att_out = self.QK_attention_norm_lay(Q_att_out + QK_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(QK_att_out)\n",
    "        out = self.ff_norm_lay(QK_att_out + linear_out)\n",
    "        return out\n",
    "\n",
    "class Multi_Head_attention_layer(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Multi_Head_attention_layer, self).__init__()\n",
    "        self.Q_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.K_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.V_linears = nn.ModuleList([nn.Linear(dm, dv) for i in range(h)])\n",
    "        self.output_linear = nn.Linear(h*dv, dm)\n",
    "                            \n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask=None):\n",
    "        buf = []\n",
    "        for Q_linear, K_linear, V_linear in zip(self.Q_linears, self.K_linears, self.V_linears):\n",
    "            Q = Q_linear(Q_input)\n",
    "            K = K_linear(K_input)\n",
    "            V = V_linear(V_input)\n",
    "            buf.append(scaled_dot_attention(Q, K, V, mask))\n",
    "            \n",
    "        buf = torch.cat(buf,dim=-1)\n",
    "        out = self.output_linear(buf)\n",
    "        \n",
    "        return out      \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        self.cnn2 = nn.Conv1d(d_ff, d_model, 1)\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len,_ = x.size()\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        return x      \n",
    "    \n",
    "# encoder = Stack_Encoder(6, 64,64,20,8)\n",
    "# # print net\n",
    "Q = torch.randint(10000,[MAX_SEQUENCE_LENGTH,], dtype=torch.long).to(device)\n",
    "V = torch.randint(10000,[MAX_SEQUENCE_LENGTH,], dtype=torch.long).to(device)\n",
    "Q_fea = torch.rand([D_MODEL,]).to(device)\n",
    "K_fea = torch.rand([D_MODEL,]).to(device)\n",
    "net = Transformer(STACKED_NUM, DK, DV, D_MODEL, H, embedding_matrix).to(device)\n",
    "print(Q.dtype)\n",
    "o = net(Q, V, Q_fea, K_fea)\n",
    "# print t\n",
    "print(o.size())\n",
    "# print o\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load weight done\n"
     ]
    }
   ],
   "source": [
    "# tmp_m = torch.load('./bak/tmp.pt')\n",
    "# net.decoder.load_state_dict(tmp_m.decoder.state_dict())\n",
    "# net.encoder.load_state_dict(tmp_m.encoder.state_dict())\n",
    "# torch.nn.init.xavier_uniform_(net.output_linear.weight)\n",
    "# print 'load weight done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t2-fake.txt'), dtype=np.int32)\n",
    "idx_map = {node:idx for idx, node in enumerate(list(set(links.flatten().tolist())))}\n",
    "N = links.shape[0]\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "for i in range(links.shape[0]):\n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((150,), (150,), (128,), (128,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2003515/2003515 [00:06<00:00, 290169.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((150,), (150,), (128,), (128,))\n",
      "((16161, 150), (16161, 150), (16161, 128), (16161, 128))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def positive_bootsrap_generator(edges, xml_id_map, node_emb_dict):\n",
    "    num_edge = len(edges)\n",
    "        \n",
    "    while True:\n",
    "        for idx in np.random.permutation(num_edge):\n",
    "            src, dst = edges[idx, :]\n",
    "            Q = xml_id_map[dst]\n",
    "            K = xml_id_map[src]\n",
    "            Q_fea = node_emb_dict[dst]\n",
    "            K_fea = node_emb_dict[src]\n",
    "            yield Q, K, Q_fea, K_fea\n",
    "def negative_bootsrap_generator(adj_mat, links, idx_map, xml_id_map, training_node_list, node_emb_dict, neighbor_link_rate=0.8):\n",
    "    \n",
    "    \n",
    "    exist_node_list = xml_id_map.keys()\n",
    "    exist_N = len(training_node_list)\n",
    "    N = adj_mat.shape[0]\n",
    "    \n",
    "#     adj mat\n",
    "    links = np.array(list(map(idx_map.get, links.flatten())),\n",
    "                     dtype=np.int32).reshape(links.shape)\n",
    "    \n",
    "    adj_sp = sp.coo_matrix((np.ones(links.shape[0]), (links[:, 0], links[:, 1])),\n",
    "                        shape=(N, N),\n",
    "                        dtype=np.uint8)\n",
    "    adj_sp_2 = (sp.coo_matrix.dot(adj_sp,adj_sp) + adj_sp).tocoo()\n",
    "    \n",
    "    rev_map = {v:k for k,v in idx_map.items()}\n",
    "    adj_map = {i:[] for i in range(N)}\n",
    "    with tqdm(total=len(adj_sp_2.row)) as pbar:\n",
    "        for i,j,v in zip(adj_sp_2.row, adj_sp_2.col, adj_sp_2.data):\n",
    "            if adj_mat[i, j] != 1 and v == 1:\n",
    "                adj_map[i].append(j)\n",
    "            pbar.update(1)\n",
    "#             print i,N\n",
    "                \n",
    "    while True:\n",
    "        src = training_node_list[np.random.randint(exist_N)]\n",
    "        \n",
    "#         choose neighbor link\n",
    "        if np.random.rand(1) <= neighbor_link_rate:\n",
    "        \n",
    "            i = idx_map[src]\n",
    "            high = len(adj_map[i])\n",
    "            while high == 0:\n",
    "                src = training_node_list[np.random.randint(exist_N)]\n",
    "                i = idx_map[src]\n",
    "                high = len(adj_map[i])\n",
    "                \n",
    "            idx = np.random.randint(high)\n",
    "            dst = adj_map[i][idx]\n",
    "            dst = rev_map[dst]\n",
    "        else:\n",
    "            dst = training_node_list[np.random.randint(exist_N)]\n",
    "            while adj_mat[idx_map[src], idx_map[dst]] == 1:\n",
    "                dst = training_node_list[np.random.randint(exist_N)]\n",
    "        Q = xml_id_map[dst]\n",
    "        K = xml_id_map[src]\n",
    "        Q_fea = node_emb_dict[dst]\n",
    "        K_fea = node_emb_dict[src]\n",
    "        yield Q, K, Q_fea, K_fea\n",
    "\n",
    "def val_data(edges, xml_id_map):\n",
    "    Q, K = [],[]\n",
    "    Q_f, K_f = [],[]\n",
    "    \n",
    "    for idx in range(edges.shape[0]):\n",
    "        src, dst = edges[idx, :]\n",
    "        q = xml_id_map[dst]\n",
    "        k = xml_id_map[src]\n",
    "        q_fea = node_emb_dict[dst]\n",
    "        k_fea = node_emb_dict[src]\n",
    "        \n",
    "        Q.append(q)\n",
    "        K.append(k)\n",
    "        Q_f.append(q_fea)\n",
    "        K_f.append(k_fea)\n",
    "        \n",
    "    Q = np.vstack(Q)\n",
    "    K = np.vstack(K)\n",
    "    Q_fea = np.vstack(Q_f)\n",
    "    K_fea = np.vstack(K_f)\n",
    "    \n",
    "    return Q, K, Q_fea, K_fea\n",
    "    \n",
    "N = links.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "train_idx = idx[N//10:]\n",
    "val_idx = idx[:N//10]\n",
    "\n",
    "pos_G = positive_bootsrap_generator(links[train_idx,:], xml_id_map, node_emb_dict)\n",
    "training_node_list = list(set(links[train_idx,:].flatten().tolist()))\n",
    "neg_G = negative_bootsrap_generator(adj_mat, links, idx_map, xml_id_map, training_node_list, node_emb_dict)\n",
    "val_Q, val_K, val_Q_fea, val_K_fea = val_data(links[val_idx,:], xml_id_map)\n",
    "q,k,q_f,k_f = next(pos_G)\n",
    "print(q.shape,k.shape, q_f.shape, k_f.shape)\n",
    "q,k,q_f,k_f = next(neg_G)\n",
    "print(q.shape,k.shape, q_f.shape, k_f.shape)\n",
    "print(val_Q.shape,val_K.shape, val_Q_fea.shape, val_K_fea.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Transformer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Word_Embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Stack_Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Multi_Head_attention_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Stack_Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 0101', 'loss_train: 1.3915', 'acc_train: 0.5200', 'loss_val: 0.6916', 'acc_val: 0.5800', 'time: 24.5437s')\n",
      "('iter: 0201', 'loss_train: 1.3880', 'acc_train: 0.5075', 'loss_val: 0.6937', 'acc_val: 0.5700', 'time: 50.7783s')\n",
      "('iter: 0301', 'loss_train: 1.3839', 'acc_train: 0.5217', 'loss_val: 0.6898', 'acc_val: 0.5600', 'time: 75.2045s')\n",
      "('iter: 0401', 'loss_train: 1.3876', 'acc_train: 0.5125', 'loss_val: 0.6908', 'acc_val: 0.5450', 'time: 99.6794s')\n",
      "('iter: 0501', 'loss_train: 1.3854', 'acc_train: 0.5120', 'loss_val: 0.6915', 'acc_val: 0.5420', 'time: 124.0918s')\n",
      "('iter: 0601', 'loss_train: 1.3852', 'acc_train: 0.5100', 'loss_val: 0.6906', 'acc_val: 0.5533', 'time: 148.8284s')\n",
      "('iter: 0701', 'loss_train: 1.3845', 'acc_train: 0.5190', 'loss_val: 0.6904', 'acc_val: 0.5514', 'time: 173.5008s')\n",
      "('iter: 0801', 'loss_train: 1.3872', 'acc_train: 0.5010', 'loss_val: 0.6904', 'acc_val: 0.5450', 'time: 198.0280s')\n",
      "('iter: 0901', 'loss_train: 1.3891', 'acc_train: 0.4970', 'loss_val: 0.6901', 'acc_val: 0.5411', 'time: 224.1066s')\n",
      "('iter: 1001', 'loss_train: 1.3913', 'acc_train: 0.4860', 'loss_val: 0.6903', 'acc_val: 0.5410', 'time: 249.6046s')\n",
      "('iter: 1101', 'loss_train: 1.3905', 'acc_train: 0.4830', 'loss_val: 0.6903', 'acc_val: 0.5380', 'time: 275.7231s')\n",
      "('iter: 1201', 'loss_train: 1.3930', 'acc_train: 0.4580', 'loss_val: 0.6899', 'acc_val: 0.5310', 'time: 300.8661s')\n",
      "('iter: 1301', 'loss_train: 1.3935', 'acc_train: 0.4760', 'loss_val: 0.6894', 'acc_val: 0.5380', 'time: 325.9063s')\n",
      "('iter: 1401', 'loss_train: 1.3918', 'acc_train: 0.4810', 'loss_val: 0.6907', 'acc_val: 0.5330', 'time: 357.4268s')\n",
      "('iter: 1501', 'loss_train: 1.3921', 'acc_train: 0.4960', 'loss_val: 0.6904', 'acc_val: 0.5340', 'time: 397.7313s')\n",
      "('iter: 1601', 'loss_train: 1.3947', 'acc_train: 0.4880', 'loss_val: 0.6911', 'acc_val: 0.5240', 'time: 437.0124s')\n",
      "('iter: 1701', 'loss_train: 1.3974', 'acc_train: 0.5040', 'loss_val: 0.6914', 'acc_val: 0.5230', 'time: 477.0615s')\n",
      "('iter: 1801', 'loss_train: 1.3975', 'acc_train: 0.4880', 'loss_val: 0.6911', 'acc_val: 0.5320', 'time: 516.7604s')\n",
      "('iter: 1901', 'loss_train: 1.3961', 'acc_train: 0.4940', 'loss_val: 0.6918', 'acc_val: 0.5300', 'time: 556.6834s')\n",
      "('iter: 2001', 'loss_train: 1.3942', 'acc_train: 0.4820', 'loss_val: 0.6921', 'acc_val: 0.5300', 'time: 581.3106s')\n",
      "('iter: 2101', 'loss_train: 1.3960', 'acc_train: 0.4800', 'loss_val: 0.6924', 'acc_val: 0.5230', 'time: 605.6739s')\n",
      "('iter: 2201', 'loss_train: 1.3950', 'acc_train: 0.4740', 'loss_val: 0.6931', 'acc_val: 0.5210', 'time: 630.0654s')\n",
      "('iter: 2301', 'loss_train: 1.3960', 'acc_train: 0.4820', 'loss_val: 0.6952', 'acc_val: 0.5100', 'time: 654.4677s')\n",
      "('iter: 2401', 'loss_train: 1.3961', 'acc_train: 0.4760', 'loss_val: 0.6951', 'acc_val: 0.5100', 'time: 678.9714s')\n",
      "('iter: 2501', 'loss_train: 1.3965', 'acc_train: 0.4870', 'loss_val: 0.6962', 'acc_val: 0.5120', 'time: 703.7149s')\n",
      "('iter: 2601', 'loss_train: 1.3937', 'acc_train: 0.5080', 'loss_val: 0.6980', 'acc_val: 0.5090', 'time: 728.1302s')\n",
      "('iter: 2701', 'loss_train: 1.3922', 'acc_train: 0.5190', 'loss_val: 0.6972', 'acc_val: 0.5130', 'time: 752.6114s')\n",
      "('iter: 2801', 'loss_train: 1.3893', 'acc_train: 0.5260', 'loss_val: 0.6982', 'acc_val: 0.5070', 'time: 777.0955s')\n",
      "('iter: 2901', 'loss_train: 1.3885', 'acc_train: 0.5350', 'loss_val: 0.6970', 'acc_val: 0.5150', 'time: 801.5660s')\n",
      "('iter: 3001', 'loss_train: 1.3889', 'acc_train: 0.5280', 'loss_val: 0.6960', 'acc_val: 0.5180', 'time: 826.0197s')\n",
      "('iter: 3101', 'loss_train: 1.3874', 'acc_train: 0.5200', 'loss_val: 0.6961', 'acc_val: 0.5190', 'time: 850.4981s')\n",
      "('iter: 3201', 'loss_train: 1.3856', 'acc_train: 0.5200', 'loss_val: 0.6953', 'acc_val: 0.5310', 'time: 874.9447s')\n",
      "('iter: 3301', 'loss_train: 1.3848', 'acc_train: 0.5140', 'loss_val: 0.6940', 'acc_val: 0.5380', 'time: 899.2895s')\n",
      "('iter: 3401', 'loss_train: 1.3827', 'acc_train: 0.5240', 'loss_val: 0.6914', 'acc_val: 0.5530', 'time: 923.7932s')\n",
      "('iter: 3501', 'loss_train: 1.3834', 'acc_train: 0.5280', 'loss_val: 0.6909', 'acc_val: 0.5540', 'time: 948.4729s')\n",
      "('iter: 3601', 'loss_train: 1.3848', 'acc_train: 0.5140', 'loss_val: 0.6888', 'acc_val: 0.5620', 'time: 972.9770s')\n",
      "('iter: 3701', 'loss_train: 1.3842', 'acc_train: 0.5140', 'loss_val: 0.6904', 'acc_val: 0.5540', 'time: 997.4595s')\n",
      "('iter: 3801', 'loss_train: 1.3852', 'acc_train: 0.5180', 'loss_val: 0.6904', 'acc_val: 0.5510', 'time: 1021.9061s')\n",
      "('iter: 3901', 'loss_train: 1.3860', 'acc_train: 0.5020', 'loss_val: 0.6905', 'acc_val: 0.5520', 'time: 1046.2654s')\n",
      "('iter: 4001', 'loss_train: 1.3843', 'acc_train: 0.5050', 'loss_val: 0.6926', 'acc_val: 0.5420', 'time: 1070.7368s')\n",
      "('iter: 4101', 'loss_train: 1.3843', 'acc_train: 0.5110', 'loss_val: 0.6922', 'acc_val: 0.5440', 'time: 1095.2998s')\n",
      "('iter: 4201', 'loss_train: 1.3848', 'acc_train: 0.5070', 'loss_val: 0.6923', 'acc_val: 0.5370', 'time: 1119.7805s')\n",
      "('iter: 4301', 'loss_train: 1.3851', 'acc_train: 0.5000', 'loss_val: 0.6932', 'acc_val: 0.5280', 'time: 1144.2142s')\n",
      "('iter: 4401', 'loss_train: 1.3892', 'acc_train: 0.5040', 'loss_val: 0.6950', 'acc_val: 0.5160', 'time: 1168.9616s')\n",
      "('iter: 4501', 'loss_train: 1.3885', 'acc_train: 0.4980', 'loss_val: 0.6941', 'acc_val: 0.5170', 'time: 1193.3974s')\n",
      "('iter: 4601', 'loss_train: 1.3866', 'acc_train: 0.5050', 'loss_val: 0.6949', 'acc_val: 0.5090', 'time: 1217.7683s')\n",
      "('iter: 4701', 'loss_train: 1.3858', 'acc_train: 0.5120', 'loss_val: 0.6935', 'acc_val: 0.5220', 'time: 1242.0508s')\n",
      "('iter: 4801', 'loss_train: 1.3853', 'acc_train: 0.5180', 'loss_val: 0.6928', 'acc_val: 0.5230', 'time: 1266.5888s')\n",
      "('iter: 4901', 'loss_train: 1.3850', 'acc_train: 0.5100', 'loss_val: 0.6924', 'acc_val: 0.5270', 'time: 1291.2660s')\n",
      "('iter: 5001', 'loss_train: 1.3850', 'acc_train: 0.5200', 'loss_val: 0.6916', 'acc_val: 0.5360', 'time: 1315.9189s')\n",
      "('iter: 5101', 'loss_train: 1.3856', 'acc_train: 0.5200', 'loss_val: 0.6907', 'acc_val: 0.5420', 'time: 1340.4786s')\n",
      "('iter: 5201', 'loss_train: 1.3843', 'acc_train: 0.5260', 'loss_val: 0.6902', 'acc_val: 0.5500', 'time: 1365.0172s')\n",
      "('iter: 5301', 'loss_train: 1.3833', 'acc_train: 0.5340', 'loss_val: 0.6909', 'acc_val: 0.5510', 'time: 1389.3408s')\n",
      "('iter: 5401', 'loss_train: 1.3809', 'acc_train: 0.5500', 'loss_val: 0.6894', 'acc_val: 0.5620', 'time: 1413.6331s')\n",
      "('iter: 5501', 'loss_train: 1.3793', 'acc_train: 0.5610', 'loss_val: 0.6897', 'acc_val: 0.5550', 'time: 1437.8784s')\n",
      "('iter: 5601', 'loss_train: 1.3797', 'acc_train: 0.5700', 'loss_val: 0.6889', 'acc_val: 0.5710', 'time: 1462.4235s')\n",
      "('iter: 5701', 'loss_train: 1.3812', 'acc_train: 0.5490', 'loss_val: 0.6899', 'acc_val: 0.5600', 'time: 1489.8205s')\n",
      "('iter: 5801', 'loss_train: 1.3814', 'acc_train: 0.5450', 'loss_val: 0.6899', 'acc_val: 0.5630', 'time: 1521.6635s')\n",
      "('iter: 5901', 'loss_train: 1.3829', 'acc_train: 0.5190', 'loss_val: 0.6917', 'acc_val: 0.5530', 'time: 1553.7915s')\n",
      "('iter: 6001', 'loss_train: 1.3807', 'acc_train: 0.5200', 'loss_val: 0.6903', 'acc_val: 0.5590', 'time: 1586.2787s')\n",
      "('iter: 6101', 'loss_train: 1.3788', 'acc_train: 0.5110', 'loss_val: 0.6899', 'acc_val: 0.5670', 'time: 1618.8345s')\n",
      "('iter: 6201', 'loss_train: 1.3807', 'acc_train: 0.5150', 'loss_val: 0.6914', 'acc_val: 0.5530', 'time: 1651.0650s')\n",
      "('iter: 6301', 'loss_train: 1.3811', 'acc_train: 0.5030', 'loss_val: 0.6909', 'acc_val: 0.5560', 'time: 1683.1960s')\n",
      "('iter: 6401', 'loss_train: 1.3813', 'acc_train: 0.5220', 'loss_val: 0.6924', 'acc_val: 0.5480', 'time: 1715.9348s')\n",
      "('iter: 6501', 'loss_train: 1.3842', 'acc_train: 0.4960', 'loss_val: 0.6943', 'acc_val: 0.5390', 'time: 1748.5553s')\n",
      "('iter: 6601', 'loss_train: 1.3852', 'acc_train: 0.4910', 'loss_val: 0.6930', 'acc_val: 0.5410', 'time: 1781.5494s')\n",
      "('iter: 6701', 'loss_train: 1.3850', 'acc_train: 0.4990', 'loss_val: 0.6922', 'acc_val: 0.5480', 'time: 1814.0778s')\n",
      "('iter: 6801', 'loss_train: 1.3838', 'acc_train: 0.5180', 'loss_val: 0.6924', 'acc_val: 0.5480', 'time: 1846.2829s')\n",
      "('iter: 6901', 'loss_train: 1.3806', 'acc_train: 0.5240', 'loss_val: 0.6914', 'acc_val: 0.5540', 'time: 1878.6751s')\n",
      "('iter: 7001', 'loss_train: 1.3829', 'acc_train: 0.5330', 'loss_val: 0.6909', 'acc_val: 0.5540', 'time: 1906.5175s')\n",
      "('iter: 7101', 'loss_train: 1.3841', 'acc_train: 0.5450', 'loss_val: 0.6930', 'acc_val: 0.5320', 'time: 1932.3325s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 7201', 'loss_train: 1.3826', 'acc_train: 0.5530', 'loss_val: 0.6898', 'acc_val: 0.5560', 'time: 1958.0579s')\n",
      "('iter: 7301', 'loss_train: 1.3826', 'acc_train: 0.5430', 'loss_val: 0.6898', 'acc_val: 0.5560', 'time: 1983.9514s')\n",
      "('iter: 7401', 'loss_train: 1.3819', 'acc_train: 0.5320', 'loss_val: 0.6885', 'acc_val: 0.5650', 'time: 2009.9259s')\n",
      "('iter: 7501', 'loss_train: 1.3792', 'acc_train: 0.5420', 'loss_val: 0.6879', 'acc_val: 0.5720', 'time: 2036.0406s')\n",
      "('iter: 7601', 'loss_train: 1.3778', 'acc_train: 0.5370', 'loss_val: 0.6884', 'acc_val: 0.5640', 'time: 2062.0618s')\n",
      "('iter: 7701', 'loss_train: 1.3773', 'acc_train: 0.5310', 'loss_val: 0.6886', 'acc_val: 0.5660', 'time: 2087.9670s')\n",
      "('iter: 7801', 'loss_train: 1.3775', 'acc_train: 0.5400', 'loss_val: 0.6896', 'acc_val: 0.5660', 'time: 2113.8971s')\n",
      "('iter: 7901', 'loss_train: 1.3780', 'acc_train: 0.5550', 'loss_val: 0.6899', 'acc_val: 0.5640', 'time: 2139.9400s')\n",
      "('iter: 8001', 'loss_train: 1.3770', 'acc_train: 0.5500', 'loss_val: 0.6907', 'acc_val: 0.5590', 'time: 2165.8233s')\n",
      "('iter: 8101', 'loss_train: 1.3753', 'acc_train: 0.5600', 'loss_val: 0.6887', 'acc_val: 0.5790', 'time: 2191.8259s')\n",
      "('iter: 8201', 'loss_train: 1.3751', 'acc_train: 0.5630', 'loss_val: 0.6909', 'acc_val: 0.5650', 'time: 2217.7151s')\n",
      "('iter: 8301', 'loss_train: 1.3736', 'acc_train: 0.5580', 'loss_val: 0.6893', 'acc_val: 0.5760', 'time: 2243.6419s')\n",
      "('iter: 8401', 'loss_train: 1.3731', 'acc_train: 0.5580', 'loss_val: 0.6905', 'acc_val: 0.5700', 'time: 2269.9190s')\n",
      "('iter: 8501', 'loss_train: 1.3741', 'acc_train: 0.5600', 'loss_val: 0.6886', 'acc_val: 0.5810', 'time: 2295.6368s')\n",
      "('iter: 8601', 'loss_train: 1.3750', 'acc_train: 0.5550', 'loss_val: 0.6898', 'acc_val: 0.5820', 'time: 2321.3297s')\n",
      "('iter: 8701', 'loss_train: 1.3736', 'acc_train: 0.5640', 'loss_val: 0.6901', 'acc_val: 0.5820', 'time: 2347.3315s')\n",
      "('iter: 8801', 'loss_train: 1.3729', 'acc_train: 0.5690', 'loss_val: 0.6887', 'acc_val: 0.5870', 'time: 2373.1674s')\n",
      "('iter: 8901', 'loss_train: 1.3735', 'acc_train: 0.5530', 'loss_val: 0.6879', 'acc_val: 0.5890', 'time: 2399.0048s')\n",
      "('iter: 9001', 'loss_train: 1.3743', 'acc_train: 0.5470', 'loss_val: 0.6887', 'acc_val: 0.5890', 'time: 2424.7646s')\n",
      "('iter: 9101', 'loss_train: 1.3735', 'acc_train: 0.5550', 'loss_val: 0.6893', 'acc_val: 0.5810', 'time: 2450.9442s')\n",
      "('iter: 9201', 'loss_train: 1.3747', 'acc_train: 0.5450', 'loss_val: 0.6911', 'acc_val: 0.5670', 'time: 2476.9886s')\n",
      "('iter: 9301', 'loss_train: 1.3759', 'acc_train: 0.5400', 'loss_val: 0.6915', 'acc_val: 0.5620', 'time: 2502.7334s')\n",
      "('iter: 9401', 'loss_train: 1.3763', 'acc_train: 0.5410', 'loss_val: 0.6900', 'acc_val: 0.5670', 'time: 2528.5791s')\n",
      "('iter: 9501', 'loss_train: 1.3775', 'acc_train: 0.5390', 'loss_val: 0.6898', 'acc_val: 0.5650', 'time: 2554.6081s')\n",
      "('iter: 9601', 'loss_train: 1.3749', 'acc_train: 0.5430', 'loss_val: 0.6881', 'acc_val: 0.5670', 'time: 2580.4626s')\n",
      "('iter: 9701', 'loss_train: 1.3747', 'acc_train: 0.5400', 'loss_val: 0.6878', 'acc_val: 0.5620', 'time: 2606.2919s')\n",
      "('iter: 9801', 'loss_train: 1.3760', 'acc_train: 0.5340', 'loss_val: 0.6892', 'acc_val: 0.5520', 'time: 2632.1474s')\n",
      "('iter: 9901', 'loss_train: 1.3742', 'acc_train: 0.5510', 'loss_val: 0.6889', 'acc_val: 0.5540', 'time: 2658.1764s')\n",
      "('iter: 10001', 'loss_train: 1.3747', 'acc_train: 0.5580', 'loss_val: 0.6887', 'acc_val: 0.5510', 'time: 2684.1398s')\n",
      "('iter: 10101', 'loss_train: 1.3760', 'acc_train: 0.5580', 'loss_val: 0.6894', 'acc_val: 0.5520', 'time: 2709.8514s')\n",
      "('iter: 10201', 'loss_train: 1.3730', 'acc_train: 0.5720', 'loss_val: 0.6874', 'acc_val: 0.5790', 'time: 2735.8569s')\n",
      "('iter: 10301', 'loss_train: 1.3732', 'acc_train: 0.5800', 'loss_val: 0.6861', 'acc_val: 0.5820', 'time: 2764.1090s')\n",
      "('iter: 10401', 'loss_train: 1.3730', 'acc_train: 0.5730', 'loss_val: 0.6859', 'acc_val: 0.5820', 'time: 2790.8478s')\n",
      "('iter: 10501', 'loss_train: 1.3706', 'acc_train: 0.5710', 'loss_val: 0.6860', 'acc_val: 0.5860', 'time: 2818.3012s')\n",
      "('iter: 10601', 'loss_train: 1.3715', 'acc_train: 0.5650', 'loss_val: 0.6866', 'acc_val: 0.5840', 'time: 2844.1163s')\n",
      "('iter: 10701', 'loss_train: 1.3720', 'acc_train: 0.5610', 'loss_val: 0.6869', 'acc_val: 0.5820', 'time: 2870.0112s')\n",
      "('iter: 10801', 'loss_train: 1.3707', 'acc_train: 0.5530', 'loss_val: 0.6849', 'acc_val: 0.5950', 'time: 2896.4599s')\n",
      "('iter: 10901', 'loss_train: 1.3726', 'acc_train: 0.5480', 'loss_val: 0.6865', 'acc_val: 0.5850', 'time: 2922.3563s')\n",
      "('iter: 11001', 'loss_train: 1.3708', 'acc_train: 0.5580', 'loss_val: 0.6864', 'acc_val: 0.5860', 'time: 2948.2241s')\n",
      "('iter: 11101', 'loss_train: 1.3697', 'acc_train: 0.5550', 'loss_val: 0.6869', 'acc_val: 0.5860', 'time: 2974.2158s')\n",
      "('iter: 11201', 'loss_train: 1.3705', 'acc_train: 0.5560', 'loss_val: 0.6872', 'acc_val: 0.5730', 'time: 2999.9423s')\n",
      "('iter: 11301', 'loss_train: 1.3692', 'acc_train: 0.5680', 'loss_val: 0.6896', 'acc_val: 0.5670', 'time: 3025.7973s')\n",
      "('iter: 11401', 'loss_train: 1.3703', 'acc_train: 0.5680', 'loss_val: 0.6900', 'acc_val: 0.5610', 'time: 3051.7423s')\n",
      "('iter: 11501', 'loss_train: 1.3713', 'acc_train: 0.5600', 'loss_val: 0.6912', 'acc_val: 0.5530', 'time: 3077.6847s')\n",
      "('iter: 11601', 'loss_train: 1.3725', 'acc_train: 0.5580', 'loss_val: 0.6919', 'acc_val: 0.5470', 'time: 3103.6894s')\n",
      "('iter: 11701', 'loss_train: 1.3721', 'acc_train: 0.5540', 'loss_val: 0.6906', 'acc_val: 0.5550', 'time: 3129.5189s')\n",
      "('iter: 11801', 'loss_train: 1.3727', 'acc_train: 0.5530', 'loss_val: 0.6914', 'acc_val: 0.5510', 'time: 3155.3334s')\n",
      "('iter: 11901', 'loss_train: 1.3705', 'acc_train: 0.5620', 'loss_val: 0.6905', 'acc_val: 0.5570', 'time: 3181.3916s')\n",
      "('iter: 12001', 'loss_train: 1.3713', 'acc_train: 0.5650', 'loss_val: 0.6901', 'acc_val: 0.5590', 'time: 3207.3685s')\n",
      "('iter: 12101', 'loss_train: 1.3721', 'acc_train: 0.5700', 'loss_val: 0.6878', 'acc_val: 0.5640', 'time: 3233.4233s')\n",
      "('iter: 12201', 'loss_train: 1.3709', 'acc_train: 0.5680', 'loss_val: 0.6869', 'acc_val: 0.5690', 'time: 3259.3432s')\n",
      "('iter: 12301', 'loss_train: 1.3705', 'acc_train: 0.5650', 'loss_val: 0.6860', 'acc_val: 0.5720', 'time: 3285.3295s')\n",
      "('iter: 12401', 'loss_train: 1.3690', 'acc_train: 0.5590', 'loss_val: 0.6858', 'acc_val: 0.5790', 'time: 3311.2259s')\n",
      "('iter: 12501', 'loss_train: 1.3703', 'acc_train: 0.5510', 'loss_val: 0.6852', 'acc_val: 0.5830', 'time: 3337.2027s')\n",
      "('iter: 12601', 'loss_train: 1.3693', 'acc_train: 0.5490', 'loss_val: 0.6854', 'acc_val: 0.5870', 'time: 3363.1605s')\n",
      "('iter: 12701', 'loss_train: 1.3692', 'acc_train: 0.5470', 'loss_val: 0.6859', 'acc_val: 0.5810', 'time: 3389.0806s')\n",
      "('iter: 12801', 'loss_train: 1.3691', 'acc_train: 0.5430', 'loss_val: 0.6866', 'acc_val: 0.5770', 'time: 3415.0151s')\n",
      "('iter: 12901', 'loss_train: 1.3704', 'acc_train: 0.5400', 'loss_val: 0.6868', 'acc_val: 0.5800', 'time: 3440.8477s')\n",
      "('iter: 13001', 'loss_train: 1.3697', 'acc_train: 0.5520', 'loss_val: 0.6875', 'acc_val: 0.5730', 'time: 3467.3785s')\n",
      "('iter: 13101', 'loss_train: 1.3697', 'acc_train: 0.5520', 'loss_val: 0.6902', 'acc_val: 0.5610', 'time: 3493.3410s')\n",
      "('iter: 13201', 'loss_train: 1.3718', 'acc_train: 0.5470', 'loss_val: 0.6904', 'acc_val: 0.5610', 'time: 3519.1471s')\n",
      "('iter: 13301', 'loss_train: 1.3720', 'acc_train: 0.5530', 'loss_val: 0.6904', 'acc_val: 0.5640', 'time: 3544.8741s')\n",
      "('iter: 13401', 'loss_train: 1.3728', 'acc_train: 0.5620', 'loss_val: 0.6904', 'acc_val: 0.5640', 'time: 3570.6107s')\n",
      "('iter: 13501', 'loss_train: 1.3712', 'acc_train: 0.5540', 'loss_val: 0.6897', 'acc_val: 0.5660', 'time: 3596.2237s')\n",
      "('iter: 13601', 'loss_train: 1.3735', 'acc_train: 0.5460', 'loss_val: 0.6885', 'acc_val: 0.5680', 'time: 3622.1564s')\n",
      "('iter: 13701', 'loss_train: 1.3760', 'acc_train: 0.5350', 'loss_val: 0.6874', 'acc_val: 0.5730', 'time: 3648.3487s')\n",
      "('iter: 13801', 'loss_train: 1.3755', 'acc_train: 0.5360', 'loss_val: 0.6871', 'acc_val: 0.5740', 'time: 3674.1670s')\n",
      "('iter: 13901', 'loss_train: 1.3745', 'acc_train: 0.5400', 'loss_val: 0.6871', 'acc_val: 0.5680', 'time: 3700.1967s')\n",
      "('iter: 14001', 'loss_train: 1.3757', 'acc_train: 0.5440', 'loss_val: 0.6861', 'acc_val: 0.5740', 'time: 3726.1913s')\n",
      "('iter: 14101', 'loss_train: 1.3745', 'acc_train: 0.5610', 'loss_val: 0.6858', 'acc_val: 0.5730', 'time: 3752.1949s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 14201', 'loss_train: 1.3744', 'acc_train: 0.5780', 'loss_val: 0.6864', 'acc_val: 0.5700', 'time: 3778.1641s')\n",
      "('iter: 14301', 'loss_train: 1.3762', 'acc_train: 0.5610', 'loss_val: 0.6868', 'acc_val: 0.5650', 'time: 3803.9361s')\n",
      "('iter: 14401', 'loss_train: 1.3732', 'acc_train: 0.5740', 'loss_val: 0.6863', 'acc_val: 0.5670', 'time: 3829.7966s')\n",
      "('iter: 14501', 'loss_train: 1.3743', 'acc_train: 0.5670', 'loss_val: 0.6867', 'acc_val: 0.5630', 'time: 3855.7241s')\n",
      "('iter: 14601', 'loss_train: 1.3709', 'acc_train: 0.5650', 'loss_val: 0.6877', 'acc_val: 0.5640', 'time: 3881.4838s')\n",
      "('iter: 14701', 'loss_train: 1.3676', 'acc_train: 0.5750', 'loss_val: 0.6874', 'acc_val: 0.5700', 'time: 3907.3564s')\n",
      "('iter: 14801', 'loss_train: 1.3676', 'acc_train: 0.5870', 'loss_val: 0.6870', 'acc_val: 0.5730', 'time: 3933.4269s')\n",
      "('iter: 14901', 'loss_train: 1.3678', 'acc_train: 0.5720', 'loss_val: 0.6864', 'acc_val: 0.5790', 'time: 3959.4283s')\n",
      "('iter: 15001', 'loss_train: 1.3658', 'acc_train: 0.5780', 'loss_val: 0.6849', 'acc_val: 0.5890', 'time: 3985.5534s')\n",
      "('iter: 15101', 'loss_train: 1.3678', 'acc_train: 0.5700', 'loss_val: 0.6814', 'acc_val: 0.6120', 'time: 4018.0303s')\n",
      "('iter: 15201', 'loss_train: 1.3667', 'acc_train: 0.5600', 'loss_val: 0.6805', 'acc_val: 0.6100', 'time: 4046.7091s')\n",
      "('iter: 15301', 'loss_train: 1.3647', 'acc_train: 0.5590', 'loss_val: 0.6789', 'acc_val: 0.6190', 'time: 4074.9719s')\n",
      "('iter: 15401', 'loss_train: 1.3675', 'acc_train: 0.5450', 'loss_val: 0.6808', 'acc_val: 0.6140', 'time: 4101.5317s')\n",
      "('iter: 15501', 'loss_train: 1.3644', 'acc_train: 0.5550', 'loss_val: 0.6797', 'acc_val: 0.6260', 'time: 4128.3454s')\n",
      "('iter: 15601', 'loss_train: 1.3674', 'acc_train: 0.5460', 'loss_val: 0.6765', 'acc_val: 0.6370', 'time: 4156.8892s')\n",
      "('iter: 15701', 'loss_train: 1.3678', 'acc_train: 0.5440', 'loss_val: 0.6770', 'acc_val: 0.6300', 'time: 4184.1269s')\n",
      "('iter: 15801', 'loss_train: 1.3679', 'acc_train: 0.5470', 'loss_val: 0.6775', 'acc_val: 0.6300', 'time: 4210.8671s')\n",
      "('iter: 15901', 'loss_train: 1.3655', 'acc_train: 0.5660', 'loss_val: 0.6769', 'acc_val: 0.6340', 'time: 4236.8056s')\n",
      "('iter: 16001', 'loss_train: 1.3668', 'acc_train: 0.5590', 'loss_val: 0.6789', 'acc_val: 0.6210', 'time: 4262.8424s')\n",
      "('iter: 16101', 'loss_train: 1.3652', 'acc_train: 0.5720', 'loss_val: 0.6819', 'acc_val: 0.6060', 'time: 4288.7727s')\n",
      "('iter: 16201', 'loss_train: 1.3666', 'acc_train: 0.5760', 'loss_val: 0.6809', 'acc_val: 0.6120', 'time: 4314.6079s')\n",
      "('iter: 16301', 'loss_train: 1.3660', 'acc_train: 0.5810', 'loss_val: 0.6837', 'acc_val: 0.5970', 'time: 4340.5553s')\n",
      "('iter: 16401', 'loss_train: 1.3660', 'acc_train: 0.5700', 'loss_val: 0.6826', 'acc_val: 0.6010', 'time: 4366.5124s')\n",
      "('iter: 16501', 'loss_train: 1.3677', 'acc_train: 0.5720', 'loss_val: 0.6819', 'acc_val: 0.5980', 'time: 4392.4703s')\n",
      "('iter: 16601', 'loss_train: 1.3659', 'acc_train: 0.5720', 'loss_val: 0.6860', 'acc_val: 0.5830', 'time: 4418.4338s')\n",
      "('iter: 16701', 'loss_train: 1.3660', 'acc_train: 0.5690', 'loss_val: 0.6864', 'acc_val: 0.5810', 'time: 4444.4290s')\n",
      "('iter: 16801', 'loss_train: 1.3656', 'acc_train: 0.5630', 'loss_val: 0.6845', 'acc_val: 0.5860', 'time: 4470.3427s')\n",
      "('iter: 16901', 'loss_train: 1.3672', 'acc_train: 0.5640', 'loss_val: 0.6880', 'acc_val: 0.5670', 'time: 4496.1085s')\n",
      "('iter: 17001', 'loss_train: 1.3668', 'acc_train: 0.5530', 'loss_val: 0.6856', 'acc_val: 0.5810', 'time: 4522.2657s')\n",
      "('iter: 17101', 'loss_train: 1.3676', 'acc_train: 0.5450', 'loss_val: 0.6850', 'acc_val: 0.5790', 'time: 4548.2552s')\n",
      "('iter: 17201', 'loss_train: 1.3659', 'acc_train: 0.5540', 'loss_val: 0.6868', 'acc_val: 0.5780', 'time: 4574.2246s')\n",
      "('iter: 17301', 'loss_train: 1.3671', 'acc_train: 0.5530', 'loss_val: 0.6851', 'acc_val: 0.5850', 'time: 4600.1338s')\n",
      "('iter: 17401', 'loss_train: 1.3672', 'acc_train: 0.5580', 'loss_val: 0.6836', 'acc_val: 0.5890', 'time: 4626.0384s')\n",
      "('iter: 17501', 'loss_train: 1.3664', 'acc_train: 0.5710', 'loss_val: 0.6857', 'acc_val: 0.5840', 'time: 4651.9598s')\n",
      "('iter: 17601', 'loss_train: 1.3650', 'acc_train: 0.5850', 'loss_val: 0.6836', 'acc_val: 0.5870', 'time: 4677.8771s')\n",
      "('iter: 17701', 'loss_train: 1.3628', 'acc_train: 0.5920', 'loss_val: 0.6839', 'acc_val: 0.5880', 'time: 4703.6909s')\n",
      "('iter: 17801', 'loss_train: 1.3645', 'acc_train: 0.5930', 'loss_val: 0.6850', 'acc_val: 0.5830', 'time: 4729.5392s')\n",
      "('iter: 17901', 'loss_train: 1.3650', 'acc_train: 0.6050', 'loss_val: 0.6844', 'acc_val: 0.5850', 'time: 4755.4325s')\n",
      "('iter: 18001', 'loss_train: 1.3644', 'acc_train: 0.5980', 'loss_val: 0.6847', 'acc_val: 0.5840', 'time: 4781.1368s')\n",
      "('iter: 18101', 'loss_train: 1.3653', 'acc_train: 0.5790', 'loss_val: 0.6843', 'acc_val: 0.5870', 'time: 4806.8730s')\n",
      "('iter: 18201', 'loss_train: 1.3663', 'acc_train: 0.5660', 'loss_val: 0.6844', 'acc_val: 0.5810', 'time: 4832.6782s')\n",
      "('iter: 18301', 'loss_train: 1.3688', 'acc_train: 0.5580', 'loss_val: 0.6852', 'acc_val: 0.5810', 'time: 4858.2367s')\n",
      "('iter: 18401', 'loss_train: 1.3681', 'acc_train: 0.5440', 'loss_val: 0.6883', 'acc_val: 0.5650', 'time: 4883.6685s')\n",
      "('iter: 18501', 'loss_train: 1.3695', 'acc_train: 0.5410', 'loss_val: 0.6880', 'acc_val: 0.5640', 'time: 4909.5576s')\n",
      "('iter: 18601', 'loss_train: 1.3697', 'acc_train: 0.5540', 'loss_val: 0.6898', 'acc_val: 0.5630', 'time: 4935.6439s')\n",
      "('iter: 18701', 'loss_train: 1.3723', 'acc_train: 0.5570', 'loss_val: 0.6907', 'acc_val: 0.5620', 'time: 4961.5359s')\n",
      "('iter: 18801', 'loss_train: 1.3702', 'acc_train: 0.5720', 'loss_val: 0.6914', 'acc_val: 0.5610', 'time: 4987.5110s')\n",
      "('iter: 18901', 'loss_train: 1.3692', 'acc_train: 0.5760', 'loss_val: 0.6873', 'acc_val: 0.5850', 'time: 5013.5140s')\n",
      "('iter: 19001', 'loss_train: 1.3721', 'acc_train: 0.5710', 'loss_val: 0.6885', 'acc_val: 0.5800', 'time: 5039.1479s')\n",
      "('iter: 19101', 'loss_train: 1.3707', 'acc_train: 0.5600', 'loss_val: 0.6885', 'acc_val: 0.5800', 'time: 5065.1009s')\n",
      "('iter: 19201', 'loss_train: 1.3694', 'acc_train: 0.5630', 'loss_val: 0.6871', 'acc_val: 0.5850', 'time: 5090.9217s')\n",
      "('iter: 19301', 'loss_train: 1.3636', 'acc_train: 0.5710', 'loss_val: 0.6858', 'acc_val: 0.5880', 'time: 5116.5232s')\n",
      "('iter: 19401', 'loss_train: 1.3594', 'acc_train: 0.5800', 'loss_val: 0.6836', 'acc_val: 0.5930', 'time: 5142.2244s')\n",
      "('iter: 19501', 'loss_train: 1.3578', 'acc_train: 0.5850', 'loss_val: 0.6816', 'acc_val: 0.5990', 'time: 5168.1308s')\n",
      "('iter: 19601', 'loss_train: 1.3602', 'acc_train: 0.5910', 'loss_val: 0.6781', 'acc_val: 0.6110', 'time: 5193.9441s')\n",
      "('iter: 19701', 'loss_train: 1.3598', 'acc_train: 0.5820', 'loss_val: 0.6776', 'acc_val: 0.6100', 'time: 5219.5884s')\n",
      "('iter: 19801', 'loss_train: 1.3603', 'acc_train: 0.5720', 'loss_val: 0.6771', 'acc_val: 0.6110', 'time: 5244.4971s')\n",
      "('iter: 19901', 'loss_train: 1.3630', 'acc_train: 0.5560', 'loss_val: 0.6799', 'acc_val: 0.5940', 'time: 5270.3923s')\n",
      "('iter: 20001', 'loss_train: 1.3610', 'acc_train: 0.5490', 'loss_val: 0.6810', 'acc_val: 0.5860', 'time: 5296.0086s')\n",
      "('iter: 20101', 'loss_train: 1.3628', 'acc_train: 0.5500', 'loss_val: 0.6823', 'acc_val: 0.5810', 'time: 5321.7644s')\n",
      "('iter: 20201', 'loss_train: 1.3645', 'acc_train: 0.5530', 'loss_val: 0.6838', 'acc_val: 0.5740', 'time: 5347.6084s')\n",
      "('iter: 20301', 'loss_train: 1.3666', 'acc_train: 0.5570', 'loss_val: 0.6839', 'acc_val: 0.5800', 'time: 5373.3729s')\n",
      "('iter: 20401', 'loss_train: 1.3683', 'acc_train: 0.5660', 'loss_val: 0.6830', 'acc_val: 0.5890', 'time: 5399.1190s')\n",
      "('iter: 20501', 'loss_train: 1.3680', 'acc_train: 0.5820', 'loss_val: 0.6841', 'acc_val: 0.5840', 'time: 5429.1948s')\n",
      "('iter: 20601', 'loss_train: 1.3686', 'acc_train: 0.5740', 'loss_val: 0.6858', 'acc_val: 0.5720', 'time: 5461.2086s')\n",
      "('iter: 20701', 'loss_train: 1.3688', 'acc_train: 0.5750', 'loss_val: 0.6846', 'acc_val: 0.5810', 'time: 5492.9692s')\n",
      "('iter: 20801', 'loss_train: 1.3672', 'acc_train: 0.5720', 'loss_val: 0.6828', 'acc_val: 0.5910', 'time: 5522.2900s')\n",
      "('iter: 20901', 'loss_train: 1.3645', 'acc_train: 0.5720', 'loss_val: 0.6803', 'acc_val: 0.6030', 'time: 5548.2141s')\n",
      "('iter: 21001', 'loss_train: 1.3617', 'acc_train: 0.5700', 'loss_val: 0.6793', 'acc_val: 0.6080', 'time: 5574.1590s')\n",
      "('iter: 21101', 'loss_train: 1.3591', 'acc_train: 0.5860', 'loss_val: 0.6774', 'acc_val: 0.6160', 'time: 5599.8075s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 21201', 'loss_train: 1.3578', 'acc_train: 0.5940', 'loss_val: 0.6766', 'acc_val: 0.6250', 'time: 5625.7267s')\n",
      "('iter: 21301', 'loss_train: 1.3640', 'acc_train: 0.5930', 'loss_val: 0.6758', 'acc_val: 0.6240', 'time: 5652.0665s')\n",
      "('iter: 21401', 'loss_train: 1.3690', 'acc_train: 0.5720', 'loss_val: 0.6771', 'acc_val: 0.6230', 'time: 5679.7374s')\n",
      "('iter: 21501', 'loss_train: 1.3690', 'acc_train: 0.5730', 'loss_val: 0.6776', 'acc_val: 0.6260', 'time: 5705.5899s')\n",
      "('iter: 21601', 'loss_train: 1.3651', 'acc_train: 0.5730', 'loss_val: 0.6782', 'acc_val: 0.6290', 'time: 5731.4917s')\n",
      "('iter: 21701', 'loss_train: 1.3649', 'acc_train: 0.5600', 'loss_val: 0.6802', 'acc_val: 0.6190', 'time: 5757.4340s')\n",
      "('iter: 21801', 'loss_train: 1.3673', 'acc_train: 0.5500', 'loss_val: 0.6815', 'acc_val: 0.6140', 'time: 5783.5021s')\n",
      "('iter: 21901', 'loss_train: 1.3690', 'acc_train: 0.5560', 'loss_val: 0.6828', 'acc_val: 0.6070', 'time: 5809.1639s')\n",
      "('iter: 22001', 'loss_train: 1.3721', 'acc_train: 0.5530', 'loss_val: 0.6827', 'acc_val: 0.6100', 'time: 5835.0202s')\n",
      "('iter: 22101', 'loss_train: 1.3741', 'acc_train: 0.5430', 'loss_val: 0.6836', 'acc_val: 0.6040', 'time: 5862.8658s')\n",
      "('iter: 22201', 'loss_train: 1.3723', 'acc_train: 0.5600', 'loss_val: 0.6841', 'acc_val: 0.6000', 'time: 5888.7715s')\n",
      "('iter: 22301', 'loss_train: 1.3645', 'acc_train: 0.5800', 'loss_val: 0.6840', 'acc_val: 0.6000', 'time: 5914.6620s')\n",
      "('iter: 22401', 'loss_train: 1.3612', 'acc_train: 0.5860', 'loss_val: 0.6831', 'acc_val: 0.6010', 'time: 5940.5532s')\n",
      "('iter: 22501', 'loss_train: 1.3590', 'acc_train: 0.5970', 'loss_val: 0.6831', 'acc_val: 0.5960', 'time: 5966.2869s')\n",
      "('iter: 22601', 'loss_train: 1.3614', 'acc_train: 0.5960', 'loss_val: 0.6841', 'acc_val: 0.5910', 'time: 5991.8850s')\n",
      "('iter: 22701', 'loss_train: 1.3584', 'acc_train: 0.5880', 'loss_val: 0.6822', 'acc_val: 0.6010', 'time: 6017.6947s')\n",
      "('iter: 22801', 'loss_train: 1.3580', 'acc_train: 0.5750', 'loss_val: 0.6832', 'acc_val: 0.5950', 'time: 6043.7002s')\n",
      "('iter: 22901', 'loss_train: 1.3564', 'acc_train: 0.5790', 'loss_val: 0.6804', 'acc_val: 0.6060', 'time: 6069.6073s')\n",
      "('iter: 23001', 'loss_train: 1.3554', 'acc_train: 0.5670', 'loss_val: 0.6789', 'acc_val: 0.6130', 'time: 6095.6216s')\n",
      "('iter: 23101', 'loss_train: 1.3544', 'acc_train: 0.5680', 'loss_val: 0.6783', 'acc_val: 0.6170', 'time: 6121.6141s')\n",
      "('iter: 23201', 'loss_train: 1.3566', 'acc_train: 0.5670', 'loss_val: 0.6760', 'acc_val: 0.6210', 'time: 6147.6300s')\n",
      "('iter: 23301', 'loss_train: 1.3608', 'acc_train: 0.5590', 'loss_val: 0.6775', 'acc_val: 0.6170', 'time: 6173.5817s')\n",
      "('iter: 23401', 'loss_train: 1.3615', 'acc_train: 0.5540', 'loss_val: 0.6775', 'acc_val: 0.6200', 'time: 6200.4009s')\n",
      "('iter: 23501', 'loss_train: 1.3653', 'acc_train: 0.5460', 'loss_val: 0.6752', 'acc_val: 0.6330', 'time: 6226.4907s')\n",
      "('iter: 23601', 'loss_train: 1.3628', 'acc_train: 0.5570', 'loss_val: 0.6750', 'acc_val: 0.6340', 'time: 6254.4650s')\n",
      "('iter: 23701', 'loss_train: 1.3653', 'acc_train: 0.5540', 'loss_val: 0.6753', 'acc_val: 0.6290', 'time: 6283.5581s')\n",
      "('iter: 23801', 'loss_train: 1.3639', 'acc_train: 0.5680', 'loss_val: 0.6725', 'acc_val: 0.6370', 'time: 6310.5063s')\n",
      "('iter: 23901', 'loss_train: 1.3642', 'acc_train: 0.5730', 'loss_val: 0.6765', 'acc_val: 0.6220', 'time: 6338.3088s')\n",
      "('iter: 24001', 'loss_train: 1.3628', 'acc_train: 0.5920', 'loss_val: 0.6799', 'acc_val: 0.6100', 'time: 6365.6555s')\n",
      "('iter: 24101', 'loss_train: 1.3615', 'acc_train: 0.5920', 'loss_val: 0.6798', 'acc_val: 0.6110', 'time: 6393.2085s')\n",
      "('iter: 24201', 'loss_train: 1.3639', 'acc_train: 0.5780', 'loss_val: 0.6787', 'acc_val: 0.6180', 'time: 6420.9473s')\n",
      "('iter: 24301', 'loss_train: 1.3573', 'acc_train: 0.5860', 'loss_val: 0.6777', 'acc_val: 0.6160', 'time: 6448.4834s')\n",
      "('iter: 24401', 'loss_train: 1.3554', 'acc_train: 0.5860', 'loss_val: 0.6773', 'acc_val: 0.6150', 'time: 6474.4952s')\n",
      "('iter: 24501', 'loss_train: 1.3506', 'acc_train: 0.5880', 'loss_val: 0.6824', 'acc_val: 0.5940', 'time: 6500.5629s')\n",
      "('iter: 24601', 'loss_train: 1.3518', 'acc_train: 0.5720', 'loss_val: 0.6825', 'acc_val: 0.5960', 'time: 6526.5326s')\n",
      "('iter: 24701', 'loss_train: 1.3524', 'acc_train: 0.5820', 'loss_val: 0.6819', 'acc_val: 0.5940', 'time: 6553.2674s')\n",
      "('iter: 24801', 'loss_train: 1.3500', 'acc_train: 0.5820', 'loss_val: 0.6847', 'acc_val: 0.5900', 'time: 6585.7544s')\n",
      "('iter: 24901', 'loss_train: 1.3457', 'acc_train: 0.5970', 'loss_val: 0.6839', 'acc_val: 0.5970', 'time: 6616.3454s')\n",
      "('iter: 25001', 'loss_train: 1.3448', 'acc_train: 0.5910', 'loss_val: 0.6828', 'acc_val: 0.6010', 'time: 6648.2012s')\n",
      "('iter: 25101', 'loss_train: 1.3447', 'acc_train: 0.5960', 'loss_val: 0.6820', 'acc_val: 0.6030', 'time: 6679.9970s')\n",
      "('iter: 25201', 'loss_train: 1.3435', 'acc_train: 0.5940', 'loss_val: 0.6835', 'acc_val: 0.5990', 'time: 6711.2063s')\n",
      "('iter: 25301', 'loss_train: 1.3460', 'acc_train: 0.5800', 'loss_val: 0.6837', 'acc_val: 0.6010', 'time: 6743.6101s')\n",
      "('iter: 25401', 'loss_train: 1.3477', 'acc_train: 0.5660', 'loss_val: 0.6846', 'acc_val: 0.5900', 'time: 6775.9509s')\n",
      "('iter: 25501', 'loss_train: 1.3518', 'acc_train: 0.5600', 'loss_val: 0.6821', 'acc_val: 0.5940', 'time: 6808.2336s')\n",
      "('iter: 25601', 'loss_train: 1.3577', 'acc_train: 0.5700', 'loss_val: 0.6823', 'acc_val: 0.5880', 'time: 6840.7292s')\n",
      "('iter: 25701', 'loss_train: 1.3551', 'acc_train: 0.5830', 'loss_val: 0.6819', 'acc_val: 0.5950', 'time: 6872.6429s')\n",
      "('iter: 25801', 'loss_train: 1.3579', 'acc_train: 0.5910', 'loss_val: 0.6796', 'acc_val: 0.5990', 'time: 6903.5187s')\n",
      "('iter: 25901', 'loss_train: 1.3634', 'acc_train: 0.5890', 'loss_val: 0.6800', 'acc_val: 0.5920', 'time: 6934.0685s')\n",
      "('iter: 26001', 'loss_train: 1.3642', 'acc_train: 0.5910', 'loss_val: 0.6794', 'acc_val: 0.5890', 'time: 6966.3479s')\n",
      "('iter: 26101', 'loss_train: 1.3622', 'acc_train: 0.5970', 'loss_val: 0.6812', 'acc_val: 0.5810', 'time: 6998.1679s')\n",
      "('iter: 26201', 'loss_train: 1.3589', 'acc_train: 0.5920', 'loss_val: 0.6809', 'acc_val: 0.5820', 'time: 7030.6610s')\n",
      "('iter: 26301', 'loss_train: 1.3610', 'acc_train: 0.5860', 'loss_val: 0.6813', 'acc_val: 0.5890', 'time: 7062.4415s')\n",
      "('iter: 26401', 'loss_train: 1.3625', 'acc_train: 0.5780', 'loss_val: 0.6801', 'acc_val: 0.5980', 'time: 7094.7034s')\n",
      "('iter: 26501', 'loss_train: 1.3631', 'acc_train: 0.5770', 'loss_val: 0.6769', 'acc_val: 0.6110', 'time: 7126.9380s')\n",
      "('iter: 26601', 'loss_train: 1.3552', 'acc_train: 0.5670', 'loss_val: 0.6746', 'acc_val: 0.6240', 'time: 7158.1631s')\n",
      "('iter: 26701', 'loss_train: 1.3557', 'acc_train: 0.5650', 'loss_val: 0.6747', 'acc_val: 0.6180', 'time: 7189.5260s')\n",
      "('iter: 26801', 'loss_train: 1.3581', 'acc_train: 0.5530', 'loss_val: 0.6753', 'acc_val: 0.6110', 'time: 7222.0688s')\n",
      "('iter: 26901', 'loss_train: 1.3568', 'acc_train: 0.5660', 'loss_val: 0.6763', 'acc_val: 0.6060', 'time: 7254.5730s')\n",
      "('iter: 27001', 'loss_train: 1.3588', 'acc_train: 0.5650', 'loss_val: 0.6759', 'acc_val: 0.6100', 'time: 7287.2858s')\n",
      "('iter: 27101', 'loss_train: 1.3611', 'acc_train: 0.5610', 'loss_val: 0.6752', 'acc_val: 0.6160', 'time: 7318.4305s')\n",
      "('iter: 27201', 'loss_train: 1.3644', 'acc_train: 0.5610', 'loss_val: 0.6780', 'acc_val: 0.6040', 'time: 7349.4586s')\n",
      "('iter: 27301', 'loss_train: 1.3638', 'acc_train: 0.5780', 'loss_val: 0.6782', 'acc_val: 0.5960', 'time: 7381.8575s')\n",
      "('iter: 27401', 'loss_train: 1.3620', 'acc_train: 0.5680', 'loss_val: 0.6791', 'acc_val: 0.5920', 'time: 7414.3896s')\n",
      "('iter: 27501', 'loss_train: 1.3597', 'acc_train: 0.5650', 'loss_val: 0.6826', 'acc_val: 0.5830', 'time: 7446.5709s')\n",
      "('iter: 27601', 'loss_train: 1.3639', 'acc_train: 0.5600', 'loss_val: 0.6843', 'acc_val: 0.5760', 'time: 7479.0304s')\n",
      "('iter: 27701', 'loss_train: 1.3660', 'acc_train: 0.5680', 'loss_val: 0.6861', 'acc_val: 0.5750', 'time: 7511.2558s')\n",
      "('iter: 27801', 'loss_train: 1.3650', 'acc_train: 0.5570', 'loss_val: 0.6858', 'acc_val: 0.5740', 'time: 7543.6184s')\n",
      "('iter: 27901', 'loss_train: 1.3680', 'acc_train: 0.5550', 'loss_val: 0.6856', 'acc_val: 0.5740', 'time: 7575.3601s')\n",
      "('iter: 28001', 'loss_train: 1.3678', 'acc_train: 0.5510', 'loss_val: 0.6865', 'acc_val: 0.5680', 'time: 7607.8453s')\n",
      "('iter: 28101', 'loss_train: 1.3687', 'acc_train: 0.5570', 'loss_val: 0.6845', 'acc_val: 0.5720', 'time: 7640.2816s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 28201', 'loss_train: 1.3675', 'acc_train: 0.5500', 'loss_val: 0.6807', 'acc_val: 0.5830', 'time: 7673.1187s')\n",
      "('iter: 28301', 'loss_train: 1.3684', 'acc_train: 0.5510', 'loss_val: 0.6809', 'acc_val: 0.5830', 'time: 7705.6148s')\n",
      "('iter: 28401', 'loss_train: 1.3690', 'acc_train: 0.5520', 'loss_val: 0.6820', 'acc_val: 0.5820', 'time: 7736.8345s')\n",
      "('iter: 28501', 'loss_train: 1.3713', 'acc_train: 0.5520', 'loss_val: 0.6827', 'acc_val: 0.5830', 'time: 7769.4069s')\n",
      "('iter: 28601', 'loss_train: 1.3700', 'acc_train: 0.5490', 'loss_val: 0.6826', 'acc_val: 0.5820', 'time: 7800.9895s')\n",
      "('iter: 28701', 'loss_train: 1.3713', 'acc_train: 0.5400', 'loss_val: 0.6827', 'acc_val: 0.5800', 'time: 7833.2624s')\n",
      "('iter: 28801', 'loss_train: 1.3705', 'acc_train: 0.5480', 'loss_val: 0.6852', 'acc_val: 0.5770', 'time: 7865.9851s')\n",
      "('iter: 28901', 'loss_train: 1.3662', 'acc_train: 0.5650', 'loss_val: 0.6832', 'acc_val: 0.5850', 'time: 7896.7423s')\n",
      "('iter: 29001', 'loss_train: 1.3669', 'acc_train: 0.5670', 'loss_val: 0.6843', 'acc_val: 0.5830', 'time: 7928.0479s')\n",
      "('iter: 29101', 'loss_train: 1.3653', 'acc_train: 0.5790', 'loss_val: 0.6862', 'acc_val: 0.5850', 'time: 7959.1056s')\n",
      "('iter: 29201', 'loss_train: 1.3652', 'acc_train: 0.5880', 'loss_val: 0.6908', 'acc_val: 0.5750', 'time: 7990.0658s')\n",
      "('iter: 29301', 'loss_train: 1.3679', 'acc_train: 0.5700', 'loss_val: 0.6934', 'acc_val: 0.5590', 'time: 8021.8420s')\n",
      "('iter: 29401', 'loss_train: 1.3692', 'acc_train: 0.5510', 'loss_val: 0.6915', 'acc_val: 0.5680', 'time: 8052.8882s')\n",
      "('iter: 29501', 'loss_train: 1.3643', 'acc_train: 0.5660', 'loss_val: 0.6894', 'acc_val: 0.5680', 'time: 8084.2174s')\n",
      "('iter: 29601', 'loss_train: 1.3643', 'acc_train: 0.5460', 'loss_val: 0.6888', 'acc_val: 0.5680', 'time: 8115.2405s')\n",
      "('iter: 29701', 'loss_train: 1.3626', 'acc_train: 0.5450', 'loss_val: 0.6889', 'acc_val: 0.5670', 'time: 8146.1404s')\n",
      "('iter: 29801', 'loss_train: 1.3633', 'acc_train: 0.5520', 'loss_val: 0.6861', 'acc_val: 0.5760', 'time: 8177.1991s')\n",
      "('iter: 29901', 'loss_train: 1.3671', 'acc_train: 0.5580', 'loss_val: 0.6856', 'acc_val: 0.5790', 'time: 8208.2494s')\n",
      "('iter: 30001', 'loss_train: 1.3645', 'acc_train: 0.5500', 'loss_val: 0.6848', 'acc_val: 0.5810', 'time: 8239.5876s')\n",
      "('iter: 30101', 'loss_train: 1.3674', 'acc_train: 0.5630', 'loss_val: 0.6858', 'acc_val: 0.5700', 'time: 8271.6602s')\n",
      "('iter: 30201', 'loss_train: 1.3689', 'acc_train: 0.5580', 'loss_val: 0.6846', 'acc_val: 0.5660', 'time: 8304.2765s')\n",
      "('iter: 30301', 'loss_train: 1.3662', 'acc_train: 0.5610', 'loss_val: 0.6826', 'acc_val: 0.5760', 'time: 8336.9095s')\n",
      "('iter: 30401', 'loss_train: 1.3638', 'acc_train: 0.5620', 'loss_val: 0.6837', 'acc_val: 0.5680', 'time: 8368.5740s')\n",
      "('iter: 30501', 'loss_train: 1.3648', 'acc_train: 0.5710', 'loss_val: 0.6838', 'acc_val: 0.5730', 'time: 8400.8058s')\n",
      "('iter: 30601', 'loss_train: 1.3629', 'acc_train: 0.5730', 'loss_val: 0.6803', 'acc_val: 0.5840', 'time: 8431.3021s')\n",
      "('iter: 30701', 'loss_train: 1.3599', 'acc_train: 0.6000', 'loss_val: 0.6799', 'acc_val: 0.5860', 'time: 8462.2376s')\n",
      "('iter: 30801', 'loss_train: 1.3588', 'acc_train: 0.6010', 'loss_val: 0.6812', 'acc_val: 0.5830', 'time: 8493.2078s')\n",
      "('iter: 30901', 'loss_train: 1.3559', 'acc_train: 0.6050', 'loss_val: 0.6811', 'acc_val: 0.5840', 'time: 8524.4876s')\n",
      "('iter: 31001', 'loss_train: 1.3559', 'acc_train: 0.5960', 'loss_val: 0.6798', 'acc_val: 0.5950', 'time: 8556.6887s')\n",
      "('iter: 31101', 'loss_train: 1.3534', 'acc_train: 0.5920', 'loss_val: 0.6808', 'acc_val: 0.5930', 'time: 8588.9815s')\n",
      "('iter: 31201', 'loss_train: 1.3499', 'acc_train: 0.5710', 'loss_val: 0.6768', 'acc_val: 0.6110', 'time: 8620.8204s')\n",
      "('iter: 31301', 'loss_train: 1.3459', 'acc_train: 0.5900', 'loss_val: 0.6731', 'acc_val: 0.6280', 'time: 8652.2023s')\n",
      "('iter: 31401', 'loss_train: 1.3453', 'acc_train: 0.5910', 'loss_val: 0.6732', 'acc_val: 0.6290', 'time: 8685.1640s')\n",
      "('iter: 31501', 'loss_train: 1.3479', 'acc_train: 0.5880', 'loss_val: 0.6737', 'acc_val: 0.6280', 'time: 8717.1353s')\n",
      "('iter: 31601', 'loss_train: 1.3466', 'acc_train: 0.5950', 'loss_val: 0.6781', 'acc_val: 0.6170', 'time: 8749.1355s')\n",
      "('iter: 31701', 'loss_train: 1.3473', 'acc_train: 0.6040', 'loss_val: 0.6741', 'acc_val: 0.6340', 'time: 8780.8587s')\n",
      "('iter: 31801', 'loss_train: 1.3463', 'acc_train: 0.5940', 'loss_val: 0.6723', 'acc_val: 0.6350', 'time: 8813.1931s')\n",
      "('iter: 31901', 'loss_train: 1.3452', 'acc_train: 0.5920', 'loss_val: 0.6726', 'acc_val: 0.6360', 'time: 8845.6224s')\n",
      "('iter: 32001', 'loss_train: 1.3457', 'acc_train: 0.5990', 'loss_val: 0.6740', 'acc_val: 0.6270', 'time: 8877.9621s')\n",
      "('iter: 32101', 'loss_train: 1.3473', 'acc_train: 0.5900', 'loss_val: 0.6704', 'acc_val: 0.6390', 'time: 8909.6285s')\n",
      "('iter: 32201', 'loss_train: 1.3492', 'acc_train: 0.5830', 'loss_val: 0.6762', 'acc_val: 0.6130', 'time: 8942.8252s')\n",
      "('iter: 32301', 'loss_train: 1.3535', 'acc_train: 0.5670', 'loss_val: 0.6788', 'acc_val: 0.6000', 'time: 8974.9374s')\n",
      "('iter: 32401', 'loss_train: 1.3553', 'acc_train: 0.5630', 'loss_val: 0.6803', 'acc_val: 0.5960', 'time: 9005.1949s')\n",
      "('iter: 32501', 'loss_train: 1.3558', 'acc_train: 0.5500', 'loss_val: 0.6809', 'acc_val: 0.5850', 'time: 9036.4378s')\n",
      "('iter: 32601', 'loss_train: 1.3546', 'acc_train: 0.5600', 'loss_val: 0.6805', 'acc_val: 0.5820', 'time: 9069.2744s')\n",
      "('iter: 32701', 'loss_train: 1.3582', 'acc_train: 0.5510', 'loss_val: 0.6820', 'acc_val: 0.5760', 'time: 9101.8037s')\n",
      "('iter: 32801', 'loss_train: 1.3593', 'acc_train: 0.5610', 'loss_val: 0.6855', 'acc_val: 0.5650', 'time: 9133.6091s')\n",
      "('iter: 32901', 'loss_train: 1.3595', 'acc_train: 0.5740', 'loss_val: 0.6834', 'acc_val: 0.5640', 'time: 9165.7814s')\n",
      "('iter: 33001', 'loss_train: 1.3586', 'acc_train: 0.5820', 'loss_val: 0.6854', 'acc_val: 0.5610', 'time: 9196.7093s')\n",
      "('iter: 33101', 'loss_train: 1.3583', 'acc_train: 0.5730', 'loss_val: 0.6878', 'acc_val: 0.5500', 'time: 9227.5570s')\n",
      "('iter: 33201', 'loss_train: 1.3564', 'acc_train: 0.5880', 'loss_val: 0.6853', 'acc_val: 0.5680', 'time: 9257.6277s')\n",
      "('iter: 33301', 'loss_train: 1.3568', 'acc_train: 0.5810', 'loss_val: 0.6843', 'acc_val: 0.5750', 'time: 9289.8862s')\n",
      "('iter: 33401', 'loss_train: 1.3571', 'acc_train: 0.5630', 'loss_val: 0.6833', 'acc_val: 0.5770', 'time: 9321.9517s')\n",
      "('iter: 33501', 'loss_train: 1.3556', 'acc_train: 0.5610', 'loss_val: 0.6810', 'acc_val: 0.5850', 'time: 9354.4237s')\n",
      "('iter: 33601', 'loss_train: 1.3579', 'acc_train: 0.5660', 'loss_val: 0.6789', 'acc_val: 0.5890', 'time: 9387.0698s')\n",
      "('iter: 33701', 'loss_train: 1.3555', 'acc_train: 0.5710', 'loss_val: 0.6811', 'acc_val: 0.5770', 'time: 9419.0054s')\n",
      "('iter: 33801', 'loss_train: 1.3573', 'acc_train: 0.5700', 'loss_val: 0.6789', 'acc_val: 0.5890', 'time: 9451.0038s')\n",
      "('iter: 33901', 'loss_train: 1.3565', 'acc_train: 0.5870', 'loss_val: 0.6828', 'acc_val: 0.5800', 'time: 9483.3045s')\n",
      "('iter: 34001', 'loss_train: 1.3584', 'acc_train: 0.5830', 'loss_val: 0.6808', 'acc_val: 0.5820', 'time: 9515.7713s')\n",
      "('iter: 34101', 'loss_train: 1.3577', 'acc_train: 0.5850', 'loss_val: 0.6805', 'acc_val: 0.5870', 'time: 9548.6275s')\n",
      "('iter: 34201', 'loss_train: 1.3594', 'acc_train: 0.5700', 'loss_val: 0.6788', 'acc_val: 0.5870', 'time: 9577.6384s')\n",
      "('iter: 34301', 'loss_train: 1.3601', 'acc_train: 0.5700', 'loss_val: 0.6821', 'acc_val: 0.5740', 'time: 9608.1551s')\n",
      "('iter: 34401', 'loss_train: 1.3572', 'acc_train: 0.5670', 'loss_val: 0.6822', 'acc_val: 0.5710', 'time: 9640.0261s')\n",
      "('iter: 34501', 'loss_train: 1.3594', 'acc_train: 0.5650', 'loss_val: 0.6848', 'acc_val: 0.5670', 'time: 9672.2604s')\n",
      "('iter: 34601', 'loss_train: 1.3572', 'acc_train: 0.5710', 'loss_val: 0.6876', 'acc_val: 0.5600', 'time: 9702.7781s')\n",
      "('iter: 34701', 'loss_train: 1.3607', 'acc_train: 0.5610', 'loss_val: 0.6883', 'acc_val: 0.5590', 'time: 9734.9017s')\n",
      "('iter: 34801', 'loss_train: 1.3575', 'acc_train: 0.5710', 'loss_val: 0.6887', 'acc_val: 0.5550', 'time: 9767.1154s')\n",
      "('iter: 34901', 'loss_train: 1.3582', 'acc_train: 0.5650', 'loss_val: 0.6891', 'acc_val: 0.5570', 'time: 9799.3143s')\n",
      "('iter: 35001', 'loss_train: 1.3608', 'acc_train: 0.5590', 'loss_val: 0.6859', 'acc_val: 0.5660', 'time: 9831.4802s')\n",
      "('iter: 35101', 'loss_train: 1.3601', 'acc_train: 0.5530', 'loss_val: 0.6840', 'acc_val: 0.5720', 'time: 9863.3577s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 35201', 'loss_train: 1.3598', 'acc_train: 0.5770', 'loss_val: 0.6853', 'acc_val: 0.5680', 'time: 9895.3726s')\n",
      "('iter: 35301', 'loss_train: 1.3570', 'acc_train: 0.5680', 'loss_val: 0.6845', 'acc_val: 0.5730', 'time: 9927.5920s')\n",
      "('iter: 35401', 'loss_train: 1.3581', 'acc_train: 0.5700', 'loss_val: 0.6827', 'acc_val: 0.5780', 'time: 9959.0450s')\n",
      "('iter: 35501', 'loss_train: 1.3604', 'acc_train: 0.5750', 'loss_val: 0.6829', 'acc_val: 0.5770', 'time: 9991.2053s')\n",
      "('iter: 35601', 'loss_train: 1.3661', 'acc_train: 0.5560', 'loss_val: 0.6802', 'acc_val: 0.5820', 'time: 10022.6302s')\n",
      "('iter: 35701', 'loss_train: 1.3627', 'acc_train: 0.5440', 'loss_val: 0.6775', 'acc_val: 0.5950', 'time: 10054.4444s')\n",
      "('iter: 35801', 'loss_train: 1.3667', 'acc_train: 0.5510', 'loss_val: 0.6754', 'acc_val: 0.6010', 'time: 10086.4543s')\n",
      "('iter: 35901', 'loss_train: 1.3707', 'acc_train: 0.5350', 'loss_val: 0.6719', 'acc_val: 0.6110', 'time: 10118.0685s')\n",
      "('iter: 36001', 'loss_train: 1.3686', 'acc_train: 0.5400', 'loss_val: 0.6777', 'acc_val: 0.5900', 'time: 10149.4247s')\n",
      "('iter: 36101', 'loss_train: 1.3682', 'acc_train: 0.5580', 'loss_val: 0.6786', 'acc_val: 0.5870', 'time: 10181.1401s')\n",
      "('iter: 36201', 'loss_train: 1.3684', 'acc_train: 0.5640', 'loss_val: 0.6784', 'acc_val: 0.5860', 'time: 10212.3786s')\n",
      "('iter: 36301', 'loss_train: 1.3670', 'acc_train: 0.5730', 'loss_val: 0.6802', 'acc_val: 0.5750', 'time: 10244.1138s')\n",
      "('iter: 36401', 'loss_train: 1.3661', 'acc_train: 0.5900', 'loss_val: 0.6800', 'acc_val: 0.5830', 'time: 10274.0926s')\n",
      "('iter: 36501', 'loss_train: 1.3655', 'acc_train: 0.5780', 'loss_val: 0.6775', 'acc_val: 0.5910', 'time: 10304.6830s')\n",
      "('iter: 36601', 'loss_train: 1.3617', 'acc_train: 0.5870', 'loss_val: 0.6777', 'acc_val: 0.5940', 'time: 10336.3750s')\n",
      "('iter: 36701', 'loss_train: 1.3637', 'acc_train: 0.5790', 'loss_val: 0.6803', 'acc_val: 0.5850', 'time: 10366.4749s')\n",
      "('iter: 36801', 'loss_train: 1.3606', 'acc_train: 0.5710', 'loss_val: 0.6813', 'acc_val: 0.5810', 'time: 10398.3910s')\n",
      "('iter: 36901', 'loss_train: 1.3584', 'acc_train: 0.5670', 'loss_val: 0.6850', 'acc_val: 0.5690', 'time: 10429.4691s')\n",
      "('iter: 37001', 'loss_train: 1.3587', 'acc_train: 0.5700', 'loss_val: 0.6788', 'acc_val: 0.5950', 'time: 10461.3588s')\n",
      "('iter: 37101', 'loss_train: 1.3623', 'acc_train: 0.5490', 'loss_val: 0.6771', 'acc_val: 0.6000', 'time: 10493.4541s')\n",
      "('iter: 37201', 'loss_train: 1.3608', 'acc_train: 0.5490', 'loss_val: 0.6758', 'acc_val: 0.6080', 'time: 10525.2235s')\n",
      "('iter: 37301', 'loss_train: 1.3593', 'acc_train: 0.5670', 'loss_val: 0.6734', 'acc_val: 0.6170', 'time: 10557.2615s')\n",
      "('iter: 37401', 'loss_train: 1.3612', 'acc_train: 0.5630', 'loss_val: 0.6743', 'acc_val: 0.6090', 'time: 10589.3476s')\n",
      "('iter: 37501', 'loss_train: 1.3578', 'acc_train: 0.5830', 'loss_val: 0.6741', 'acc_val: 0.6080', 'time: 10621.4261s')\n",
      "('iter: 37601', 'loss_train: 1.3586', 'acc_train: 0.5930', 'loss_val: 0.6766', 'acc_val: 0.6000', 'time: 10653.4078s')\n",
      "('iter: 37701', 'loss_train: 1.3577', 'acc_train: 0.5930', 'loss_val: 0.6739', 'acc_val: 0.6060', 'time: 10685.5130s')\n",
      "('iter: 37801', 'loss_train: 1.3589', 'acc_train: 0.5710', 'loss_val: 0.6760', 'acc_val: 0.6010', 'time: 10717.1337s')\n",
      "('iter: 37901', 'loss_train: 1.3611', 'acc_train: 0.5650', 'loss_val: 0.6749', 'acc_val: 0.6040', 'time: 10749.2550s')\n",
      "('iter: 38001', 'loss_train: 1.3621', 'acc_train: 0.5470', 'loss_val: 0.6778', 'acc_val: 0.5890', 'time: 10780.7521s')\n",
      "('iter: 38101', 'loss_train: 1.3570', 'acc_train: 0.5500', 'loss_val: 0.6812', 'acc_val: 0.5750', 'time: 10812.2394s')\n",
      "('iter: 38201', 'loss_train: 1.3592', 'acc_train: 0.5410', 'loss_val: 0.6820', 'acc_val: 0.5710', 'time: 10844.0514s')\n",
      "('iter: 38301', 'loss_train: 1.3623', 'acc_train: 0.5430', 'loss_val: 0.6796', 'acc_val: 0.5800', 'time: 10875.9682s')\n",
      "('iter: 38401', 'loss_train: 1.3594', 'acc_train: 0.5580', 'loss_val: 0.6801', 'acc_val: 0.5790', 'time: 10908.2530s')\n",
      "('iter: 38501', 'loss_train: 1.3609', 'acc_train: 0.5670', 'loss_val: 0.6819', 'acc_val: 0.5750', 'time: 10940.3260s')\n",
      "('iter: 38601', 'loss_train: 1.3616', 'acc_train: 0.5650', 'loss_val: 0.6802', 'acc_val: 0.5820', 'time: 10972.1199s')\n",
      "('iter: 38701', 'loss_train: 1.3617', 'acc_train: 0.5740', 'loss_val: 0.6833', 'acc_val: 0.5750', 'time: 11002.7692s')\n",
      "('iter: 38801', 'loss_train: 1.3608', 'acc_train: 0.5790', 'loss_val: 0.6829', 'acc_val: 0.5780', 'time: 11035.0859s')\n",
      "('iter: 38901', 'loss_train: 1.3608', 'acc_train: 0.5650', 'loss_val: 0.6796', 'acc_val: 0.5890', 'time: 11067.1521s')\n",
      "('iter: 39001', 'loss_train: 1.3601', 'acc_train: 0.5660', 'loss_val: 0.6766', 'acc_val: 0.5990', 'time: 11099.1809s')\n",
      "('iter: 39101', 'loss_train: 1.3630', 'acc_train: 0.5680', 'loss_val: 0.6737', 'acc_val: 0.6120', 'time: 11131.2215s')\n",
      "('iter: 39201', 'loss_train: 1.3621', 'acc_train: 0.5690', 'loss_val: 0.6740', 'acc_val: 0.6120', 'time: 11163.4937s')\n",
      "('iter: 39301', 'loss_train: 1.3621', 'acc_train: 0.5640', 'loss_val: 0.6743', 'acc_val: 0.6100', 'time: 11195.9227s')\n",
      "('iter: 39401', 'loss_train: 1.3663', 'acc_train: 0.5640', 'loss_val: 0.6752', 'acc_val: 0.6090', 'time: 11226.8314s')\n",
      "('iter: 39501', 'loss_train: 1.3660', 'acc_train: 0.5650', 'loss_val: 0.6744', 'acc_val: 0.6150', 'time: 11259.3282s')\n",
      "('iter: 39601', 'loss_train: 1.3638', 'acc_train: 0.5630', 'loss_val: 0.6737', 'acc_val: 0.6240', 'time: 11291.5248s')\n",
      "('iter: 39701', 'loss_train: 1.3628', 'acc_train: 0.5620', 'loss_val: 0.6705', 'acc_val: 0.6340', 'time: 11324.2589s')\n",
      "('iter: 39801', 'loss_train: 1.3611', 'acc_train: 0.5610', 'loss_val: 0.6714', 'acc_val: 0.6320', 'time: 11355.0968s')\n",
      "('iter: 39901', 'loss_train: 1.3582', 'acc_train: 0.5760', 'loss_val: 0.6758', 'acc_val: 0.6180', 'time: 11387.4697s')\n",
      "('iter: 40001', 'loss_train: 1.3591', 'acc_train: 0.5660', 'loss_val: 0.6760', 'acc_val: 0.6200', 'time: 11420.0361s')\n",
      "('iter: 40101', 'loss_train: 1.3575', 'acc_train: 0.5680', 'loss_val: 0.6782', 'acc_val: 0.6140', 'time: 11452.1238s')\n",
      "('iter: 40201', 'loss_train: 1.3581', 'acc_train: 0.5660', 'loss_val: 0.6768', 'acc_val: 0.6180', 'time: 11482.9300s')\n",
      "('iter: 40301', 'loss_train: 1.3595', 'acc_train: 0.5670', 'loss_val: 0.6781', 'acc_val: 0.6140', 'time: 11514.8767s')\n",
      "('iter: 40401', 'loss_train: 1.3573', 'acc_train: 0.5610', 'loss_val: 0.6736', 'acc_val: 0.6280', 'time: 11546.9478s')\n",
      "('iter: 40501', 'loss_train: 1.3584', 'acc_train: 0.5660', 'loss_val: 0.6770', 'acc_val: 0.6130', 'time: 11578.9096s')\n",
      "('iter: 40601', 'loss_train: 1.3623', 'acc_train: 0.5560', 'loss_val: 0.6798', 'acc_val: 0.6030', 'time: 11610.6128s')\n",
      "('iter: 40701', 'loss_train: 1.3619', 'acc_train: 0.5580', 'loss_val: 0.6832', 'acc_val: 0.5900', 'time: 11643.0216s')\n",
      "('iter: 40801', 'loss_train: 1.3616', 'acc_train: 0.5630', 'loss_val: 0.6797', 'acc_val: 0.5960', 'time: 11675.4588s')\n",
      "('iter: 40901', 'loss_train: 1.3615', 'acc_train: 0.5650', 'loss_val: 0.6786', 'acc_val: 0.5990', 'time: 11707.3087s')\n",
      "('iter: 41001', 'loss_train: 1.3601', 'acc_train: 0.5690', 'loss_val: 0.6814', 'acc_val: 0.5930', 'time: 11739.6917s')\n",
      "('iter: 41101', 'loss_train: 1.3581', 'acc_train: 0.5850', 'loss_val: 0.6826', 'acc_val: 0.5860', 'time: 11771.4791s')\n",
      "('iter: 41201', 'loss_train: 1.3586', 'acc_train: 0.5900', 'loss_val: 0.6847', 'acc_val: 0.5800', 'time: 11802.6199s')\n",
      "('iter: 41301', 'loss_train: 1.3583', 'acc_train: 0.5800', 'loss_val: 0.6845', 'acc_val: 0.5780', 'time: 11834.7611s')\n",
      "('iter: 41401', 'loss_train: 1.3586', 'acc_train: 0.5780', 'loss_val: 0.6851', 'acc_val: 0.5740', 'time: 11866.8033s')\n",
      "('iter: 41501', 'loss_train: 1.3531', 'acc_train: 0.5850', 'loss_val: 0.6824', 'acc_val: 0.5780', 'time: 11898.8434s')\n",
      "('iter: 41601', 'loss_train: 1.3507', 'acc_train: 0.5780', 'loss_val: 0.6823', 'acc_val: 0.5700', 'time: 11931.3691s')\n",
      "('iter: 41701', 'loss_train: 1.3486', 'acc_train: 0.5860', 'loss_val: 0.6807', 'acc_val: 0.5740', 'time: 11962.7683s')\n",
      "('iter: 41801', 'loss_train: 1.3500', 'acc_train: 0.5940', 'loss_val: 0.6818', 'acc_val: 0.5750', 'time: 11994.8372s')\n",
      "('iter: 41901', 'loss_train: 1.3491', 'acc_train: 0.6050', 'loss_val: 0.6796', 'acc_val: 0.5830', 'time: 12025.8856s')\n",
      "('iter: 42001', 'loss_train: 1.3502', 'acc_train: 0.5910', 'loss_val: 0.6805', 'acc_val: 0.5770', 'time: 12057.8877s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 42101', 'loss_train: 1.3562', 'acc_train: 0.5840', 'loss_val: 0.6804', 'acc_val: 0.5780', 'time: 12090.1415s')\n",
      "('iter: 42201', 'loss_train: 1.3507', 'acc_train: 0.5830', 'loss_val: 0.6802', 'acc_val: 0.5790', 'time: 12122.4211s')\n",
      "('iter: 42301', 'loss_train: 1.3505', 'acc_train: 0.5750', 'loss_val: 0.6800', 'acc_val: 0.5770', 'time: 12154.5697s')\n",
      "('iter: 42401', 'loss_train: 1.3516', 'acc_train: 0.5620', 'loss_val: 0.6789', 'acc_val: 0.5780', 'time: 12186.7567s')\n",
      "('iter: 42501', 'loss_train: 1.3546', 'acc_train: 0.5760', 'loss_val: 0.6792', 'acc_val: 0.5910', 'time: 12219.1717s')\n",
      "('iter: 42601', 'loss_train: 1.3530', 'acc_train: 0.5850', 'loss_val: 0.6755', 'acc_val: 0.6050', 'time: 12251.1768s')\n",
      "('iter: 42701', 'loss_train: 1.3518', 'acc_train: 0.5890', 'loss_val: 0.6742', 'acc_val: 0.6070', 'time: 12282.9002s')\n",
      "('iter: 42801', 'loss_train: 1.3525', 'acc_train: 0.5990', 'loss_val: 0.6751', 'acc_val: 0.6080', 'time: 12314.8556s')\n",
      "('iter: 42901', 'loss_train: 1.3538', 'acc_train: 0.6010', 'loss_val: 0.6729', 'acc_val: 0.6090', 'time: 12347.0034s')\n",
      "('iter: 43001', 'loss_train: 1.3516', 'acc_train: 0.5840', 'loss_val: 0.6711', 'acc_val: 0.6130', 'time: 12378.9164s')\n",
      "('iter: 43101', 'loss_train: 1.3497', 'acc_train: 0.5770', 'loss_val: 0.6697', 'acc_val: 0.6140', 'time: 12411.4196s')\n",
      "('iter: 43201', 'loss_train: 1.3552', 'acc_train: 0.5630', 'loss_val: 0.6709', 'acc_val: 0.6070', 'time: 12443.4043s')\n",
      "('iter: 43301', 'loss_train: 1.3567', 'acc_train: 0.5520', 'loss_val: 0.6685', 'acc_val: 0.6200', 'time: 12476.0931s')\n",
      "('iter: 43401', 'loss_train: 1.3550', 'acc_train: 0.5520', 'loss_val: 0.6716', 'acc_val: 0.6110', 'time: 12508.0171s')\n",
      "('iter: 43501', 'loss_train: 1.3554', 'acc_train: 0.5610', 'loss_val: 0.6721', 'acc_val: 0.6060', 'time: 12539.9970s')\n",
      "('iter: 43601', 'loss_train: 1.3589', 'acc_train: 0.5610', 'loss_val: 0.6751', 'acc_val: 0.5990', 'time: 12571.9639s')\n",
      "('iter: 43701', 'loss_train: 1.3624', 'acc_train: 0.5640', 'loss_val: 0.6769', 'acc_val: 0.5980', 'time: 12603.9929s')\n",
      "('iter: 43801', 'loss_train: 1.3617', 'acc_train: 0.5690', 'loss_val: 0.6769', 'acc_val: 0.5920', 'time: 12635.8867s')\n",
      "('iter: 43901', 'loss_train: 1.3601', 'acc_train: 0.5730', 'loss_val: 0.6815', 'acc_val: 0.5830', 'time: 12667.6955s')\n",
      "('iter: 44001', 'loss_train: 1.3570', 'acc_train: 0.5730', 'loss_val: 0.6833', 'acc_val: 0.5780', 'time: 12699.9237s')\n",
      "('iter: 44101', 'loss_train: 1.3564', 'acc_train: 0.5800', 'loss_val: 0.6840', 'acc_val: 0.5750', 'time: 12732.1731s')\n",
      "('iter: 44201', 'loss_train: 1.3546', 'acc_train: 0.5780', 'loss_val: 0.6844', 'acc_val: 0.5750', 'time: 12764.2456s')\n",
      "('iter: 44301', 'loss_train: 1.3557', 'acc_train: 0.5620', 'loss_val: 0.6844', 'acc_val: 0.5730', 'time: 12796.5430s')\n",
      "('iter: 44401', 'loss_train: 1.3561', 'acc_train: 0.5590', 'loss_val: 0.6858', 'acc_val: 0.5690', 'time: 12828.4969s')\n",
      "('iter: 44501', 'loss_train: 1.3537', 'acc_train: 0.5680', 'loss_val: 0.6840', 'acc_val: 0.5660', 'time: 12860.6655s')\n",
      "('iter: 44601', 'loss_train: 1.3520', 'acc_train: 0.5680', 'loss_val: 0.6828', 'acc_val: 0.5710', 'time: 12893.0162s')\n",
      "('iter: 44701', 'loss_train: 1.3505', 'acc_train: 0.5770', 'loss_val: 0.6800', 'acc_val: 0.5760', 'time: 12925.1385s')\n",
      "('iter: 44801', 'loss_train: 1.3519', 'acc_train: 0.5950', 'loss_val: 0.6780', 'acc_val: 0.5820', 'time: 12955.1601s')\n",
      "('iter: 44901', 'loss_train: 1.3523', 'acc_train: 0.5940', 'loss_val: 0.6765', 'acc_val: 0.5870', 'time: 12987.6861s')\n",
      "('iter: 45001', 'loss_train: 1.3528', 'acc_train: 0.5870', 'loss_val: 0.6749', 'acc_val: 0.5920', 'time: 13019.9473s')\n",
      "('iter: 45101', 'loss_train: 1.3523', 'acc_train: 0.5780', 'loss_val: 0.6746', 'acc_val: 0.5940', 'time: 13051.8550s')\n",
      "('iter: 45201', 'loss_train: 1.3515', 'acc_train: 0.5690', 'loss_val: 0.6764', 'acc_val: 0.5890', 'time: 13084.1076s')\n",
      "('iter: 45301', 'loss_train: 1.3484', 'acc_train: 0.5610', 'loss_val: 0.6795', 'acc_val: 0.5810', 'time: 13115.5765s')\n",
      "('iter: 45401', 'loss_train: 1.3492', 'acc_train: 0.5620', 'loss_val: 0.6788', 'acc_val: 0.5850', 'time: 13146.6732s')\n",
      "('iter: 45501', 'loss_train: 1.3512', 'acc_train: 0.5660', 'loss_val: 0.6808', 'acc_val: 0.5870', 'time: 13178.6295s')\n",
      "('iter: 45601', 'loss_train: 1.3533', 'acc_train: 0.5650', 'loss_val: 0.6818', 'acc_val: 0.5830', 'time: 13209.2983s')\n",
      "('iter: 45701', 'loss_train: 1.3534', 'acc_train: 0.5620', 'loss_val: 0.6831', 'acc_val: 0.5760', 'time: 13241.1287s')\n",
      "('iter: 45801', 'loss_train: 1.3513', 'acc_train: 0.5770', 'loss_val: 0.6849', 'acc_val: 0.5670', 'time: 13272.7667s')\n",
      "('iter: 45901', 'loss_train: 1.3549', 'acc_train: 0.5720', 'loss_val: 0.6837', 'acc_val: 0.5710', 'time: 13304.7913s')\n",
      "('iter: 46001', 'loss_train: 1.3535', 'acc_train: 0.5860', 'loss_val: 0.6834', 'acc_val: 0.5680', 'time: 13336.6778s')\n",
      "('iter: 46101', 'loss_train: 1.3509', 'acc_train: 0.5970', 'loss_val: 0.6819', 'acc_val: 0.5770', 'time: 13368.6056s')\n",
      "('iter: 46201', 'loss_train: 1.3507', 'acc_train: 0.6050', 'loss_val: 0.6792', 'acc_val: 0.5840', 'time: 13398.7672s')\n",
      "('iter: 46301', 'loss_train: 1.3501', 'acc_train: 0.5980', 'loss_val: 0.6806', 'acc_val: 0.5790', 'time: 13430.1497s')\n",
      "('iter: 46401', 'loss_train: 1.3454', 'acc_train: 0.6100', 'loss_val: 0.6803', 'acc_val: 0.5760', 'time: 13460.3759s')\n",
      "('iter: 46501', 'loss_train: 1.3476', 'acc_train: 0.5870', 'loss_val: 0.6786', 'acc_val: 0.5740', 'time: 13492.4967s')\n",
      "('iter: 46601', 'loss_train: 1.3440', 'acc_train: 0.5860', 'loss_val: 0.6802', 'acc_val: 0.5760', 'time: 13524.4194s')\n",
      "('iter: 46701', 'loss_train: 1.3450', 'acc_train: 0.5850', 'loss_val: 0.6782', 'acc_val: 0.5850', 'time: 13556.4505s')\n",
      "('iter: 46801', 'loss_train: 1.3479', 'acc_train: 0.5820', 'loss_val: 0.6783', 'acc_val: 0.5840', 'time: 13588.4266s')\n",
      "('iter: 46901', 'loss_train: 1.3452', 'acc_train: 0.5800', 'loss_val: 0.6782', 'acc_val: 0.5790', 'time: 13620.5223s')\n",
      "('iter: 47001', 'loss_train: 1.3466', 'acc_train: 0.5950', 'loss_val: 0.6786', 'acc_val: 0.5850', 'time: 13652.2834s')\n",
      "('iter: 47101', 'loss_train: 1.3473', 'acc_train: 0.5970', 'loss_val: 0.6812', 'acc_val: 0.5770', 'time: 13683.6033s')\n",
      "('iter: 47201', 'loss_train: 1.3487', 'acc_train: 0.5980', 'loss_val: 0.6758', 'acc_val: 0.5990', 'time: 13715.7703s')\n",
      "('iter: 47301', 'loss_train: 1.3509', 'acc_train: 0.5990', 'loss_val: 0.6760', 'acc_val: 0.6010', 'time: 13748.2074s')\n",
      "('iter: 47401', 'loss_train: 1.3552', 'acc_train: 0.5930', 'loss_val: 0.6736', 'acc_val: 0.6110', 'time: 13780.7566s')\n",
      "('iter: 47501', 'loss_train: 1.3500', 'acc_train: 0.5940', 'loss_val: 0.6717', 'acc_val: 0.6200', 'time: 13812.4721s')\n",
      "('iter: 47601', 'loss_train: 1.3487', 'acc_train: 0.5890', 'loss_val: 0.6687', 'acc_val: 0.6210', 'time: 13844.9132s')\n",
      "('iter: 47701', 'loss_train: 1.3498', 'acc_train: 0.5810', 'loss_val: 0.6735', 'acc_val: 0.6070', 'time: 13876.9653s')\n",
      "('iter: 47801', 'loss_train: 1.3440', 'acc_train: 0.5890', 'loss_val: 0.6719', 'acc_val: 0.6150', 'time: 13909.0140s')\n",
      "('iter: 47901', 'loss_train: 1.3424', 'acc_train: 0.5970', 'loss_val: 0.6697', 'acc_val: 0.6290', 'time: 13941.0961s')\n",
      "('iter: 48001', 'loss_train: 1.3459', 'acc_train: 0.5770', 'loss_val: 0.6687', 'acc_val: 0.6280', 'time: 13972.8941s')\n",
      "('iter: 48101', 'loss_train: 1.3474', 'acc_train: 0.5740', 'loss_val: 0.6660', 'acc_val: 0.6380', 'time: 14007.7829s')\n",
      "('iter: 48201', 'loss_train: 1.3456', 'acc_train: 0.5850', 'loss_val: 0.6711', 'acc_val: 0.6170', 'time: 14040.1103s')\n",
      "('iter: 48301', 'loss_train: 1.3420', 'acc_train: 0.5820', 'loss_val: 0.6715', 'acc_val: 0.6120', 'time: 14071.8628s')\n",
      "('iter: 48401', 'loss_train: 1.3425', 'acc_train: 0.5740', 'loss_val: 0.6731', 'acc_val: 0.6070', 'time: 14104.0454s')\n",
      "('iter: 48501', 'loss_train: 1.3446', 'acc_train: 0.5770', 'loss_val: 0.6765', 'acc_val: 0.5940', 'time: 14135.6716s')\n",
      "('iter: 48601', 'loss_train: 1.3440', 'acc_train: 0.5840', 'loss_val: 0.6767', 'acc_val: 0.5920', 'time: 14166.8589s')\n",
      "('iter: 48701', 'loss_train: 1.3442', 'acc_train: 0.5690', 'loss_val: 0.6747', 'acc_val: 0.5970', 'time: 14198.4976s')\n",
      "('iter: 48801', 'loss_train: 1.3474', 'acc_train: 0.5660', 'loss_val: 0.6740', 'acc_val: 0.6000', 'time: 14230.0600s')\n",
      "('iter: 48901', 'loss_train: 1.3476', 'acc_train: 0.5690', 'loss_val: 0.6775', 'acc_val: 0.5900', 'time: 14261.2469s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 49001', 'loss_train: 1.3493', 'acc_train: 0.5680', 'loss_val: 0.6795', 'acc_val: 0.5840', 'time: 14293.1269s')\n",
      "('iter: 49101', 'loss_train: 1.3494', 'acc_train: 0.5630', 'loss_val: 0.6797', 'acc_val: 0.5820', 'time: 14324.2734s')\n",
      "('iter: 49201', 'loss_train: 1.3522', 'acc_train: 0.5640', 'loss_val: 0.6835', 'acc_val: 0.5770', 'time: 14356.7768s')\n",
      "('iter: 49301', 'loss_train: 1.3515', 'acc_train: 0.5680', 'loss_val: 0.6800', 'acc_val: 0.5910', 'time: 14388.9803s')\n",
      "('iter: 49401', 'loss_train: 1.3503', 'acc_train: 0.5650', 'loss_val: 0.6800', 'acc_val: 0.5880', 'time: 14421.4339s')\n",
      "('iter: 49501', 'loss_train: 1.3515', 'acc_train: 0.5710', 'loss_val: 0.6794', 'acc_val: 0.5950', 'time: 14453.3818s')\n",
      "('iter: 49601', 'loss_train: 1.3551', 'acc_train: 0.5700', 'loss_val: 0.6783', 'acc_val: 0.5980', 'time: 14485.2432s')\n",
      "('iter: 49701', 'loss_train: 1.3539', 'acc_train: 0.5830', 'loss_val: 0.6763', 'acc_val: 0.6030', 'time: 14516.4423s')\n",
      "('iter: 49801', 'loss_train: 1.3539', 'acc_train: 0.5780', 'loss_val: 0.6777', 'acc_val: 0.5970', 'time: 14547.9067s')\n",
      "('iter: 49901', 'loss_train: 1.3558', 'acc_train: 0.5840', 'loss_val: 0.6769', 'acc_val: 0.5940', 'time: 14579.1792s')\n",
      "('iter: 50001', 'loss_train: 1.3535', 'acc_train: 0.5870', 'loss_val: 0.6772', 'acc_val: 0.5930', 'time: 14610.7347s')\n",
      "('iter: 50101', 'loss_train: 1.3565', 'acc_train: 0.5820', 'loss_val: 0.6764', 'acc_val: 0.5940', 'time: 14643.2438s')\n",
      "('iter: 50201', 'loss_train: 1.3554', 'acc_train: 0.5700', 'loss_val: 0.6733', 'acc_val: 0.5960', 'time: 14675.3673s')\n",
      "('iter: 50301', 'loss_train: 1.3557', 'acc_train: 0.5740', 'loss_val: 0.6722', 'acc_val: 0.5990', 'time: 14707.9133s')\n",
      "('iter: 50401', 'loss_train: 1.3564', 'acc_train: 0.5730', 'loss_val: 0.6764', 'acc_val: 0.5870', 'time: 14739.4438s')\n",
      "('iter: 50501', 'loss_train: 1.3567', 'acc_train: 0.5650', 'loss_val: 0.6761', 'acc_val: 0.5850', 'time: 14771.9682s')\n",
      "('iter: 50601', 'loss_train: 1.3528', 'acc_train: 0.5760', 'loss_val: 0.6803', 'acc_val: 0.5720', 'time: 14803.5647s')\n",
      "('iter: 50701', 'loss_train: 1.3544', 'acc_train: 0.5790', 'loss_val: 0.6849', 'acc_val: 0.5600', 'time: 14835.4318s')\n",
      "('iter: 50801', 'loss_train: 1.3532', 'acc_train: 0.5830', 'loss_val: 0.6873', 'acc_val: 0.5530', 'time: 14866.7632s')\n",
      "('iter: 50901', 'loss_train: 1.3517', 'acc_train: 0.5790', 'loss_val: 0.6902', 'acc_val: 0.5450', 'time: 14898.7461s')\n",
      "('iter: 51001', 'loss_train: 1.3531', 'acc_train: 0.5770', 'loss_val: 0.6903', 'acc_val: 0.5470', 'time: 14929.6893s')\n",
      "('iter: 51101', 'loss_train: 1.3487', 'acc_train: 0.5710', 'loss_val: 0.6938', 'acc_val: 0.5370', 'time: 14961.4831s')\n",
      "('iter: 51201', 'loss_train: 1.3490', 'acc_train: 0.5670', 'loss_val: 0.6899', 'acc_val: 0.5450', 'time: 14992.3296s')\n",
      "('iter: 51301', 'loss_train: 1.3527', 'acc_train: 0.5540', 'loss_val: 0.6920', 'acc_val: 0.5380', 'time: 15018.2176s')\n",
      "('iter: 51401', 'loss_train: 1.3520', 'acc_train: 0.5590', 'loss_val: 0.6887', 'acc_val: 0.5530', 'time: 15044.0381s')\n",
      "('iter: 51501', 'loss_train: 1.3493', 'acc_train: 0.5720', 'loss_val: 0.6865', 'acc_val: 0.5620', 'time: 15069.7197s')\n",
      "('iter: 51601', 'loss_train: 1.3509', 'acc_train: 0.5800', 'loss_val: 0.6860', 'acc_val: 0.5640', 'time: 15095.5699s')\n",
      "('iter: 51701', 'loss_train: 1.3499', 'acc_train: 0.5870', 'loss_val: 0.6835', 'acc_val: 0.5700', 'time: 15121.4402s')\n",
      "('iter: 51801', 'loss_train: 1.3574', 'acc_train: 0.5780', 'loss_val: 0.6794', 'acc_val: 0.5850', 'time: 15147.3319s')\n",
      "('iter: 51901', 'loss_train: 1.3577', 'acc_train: 0.5690', 'loss_val: 0.6770', 'acc_val: 0.5880', 'time: 15173.0753s')\n",
      "('iter: 52001', 'loss_train: 1.3574', 'acc_train: 0.5570', 'loss_val: 0.6749', 'acc_val: 0.5920', 'time: 15198.9540s')\n",
      "('iter: 52101', 'loss_train: 1.3587', 'acc_train: 0.5490', 'loss_val: 0.6746', 'acc_val: 0.5910', 'time: 15224.7144s')\n",
      "('iter: 52201', 'loss_train: 1.3586', 'acc_train: 0.5450', 'loss_val: 0.6805', 'acc_val: 0.5810', 'time: 15250.5223s')\n",
      "('iter: 52301', 'loss_train: 1.3542', 'acc_train: 0.5640', 'loss_val: 0.6803', 'acc_val: 0.5800', 'time: 15276.2613s')\n",
      "('iter: 52401', 'loss_train: 1.3541', 'acc_train: 0.5730', 'loss_val: 0.6805', 'acc_val: 0.5760', 'time: 15301.9525s')\n",
      "('iter: 52501', 'loss_train: 1.3557', 'acc_train: 0.5840', 'loss_val: 0.6860', 'acc_val: 0.5570', 'time: 15327.6922s')\n",
      "('iter: 52601', 'loss_train: 1.3583', 'acc_train: 0.5830', 'loss_val: 0.6812', 'acc_val: 0.5750', 'time: 15353.5778s')\n",
      "('iter: 52701', 'loss_train: 1.3604', 'acc_train: 0.5900', 'loss_val: 0.6812', 'acc_val: 0.5730', 'time: 15379.3547s')\n",
      "('iter: 52801', 'loss_train: 1.3491', 'acc_train: 0.6010', 'loss_val: 0.6823', 'acc_val: 0.5640', 'time: 15405.0665s')\n",
      "('iter: 52901', 'loss_train: 1.3511', 'acc_train: 0.5870', 'loss_val: 0.6826', 'acc_val: 0.5680', 'time: 15430.5582s')\n",
      "('iter: 53001', 'loss_train: 1.3477', 'acc_train: 0.5920', 'loss_val: 0.6828', 'acc_val: 0.5670', 'time: 15456.2280s')\n",
      "('iter: 53101', 'loss_train: 1.3466', 'acc_train: 0.5950', 'loss_val: 0.6799', 'acc_val: 0.5770', 'time: 15481.9473s')\n",
      "('iter: 53201', 'loss_train: 1.3443', 'acc_train: 0.5940', 'loss_val: 0.6745', 'acc_val: 0.5930', 'time: 15507.5663s')\n",
      "('iter: 53301', 'loss_train: 1.3499', 'acc_train: 0.5630', 'loss_val: 0.6751', 'acc_val: 0.5940', 'time: 15533.2599s')\n",
      "('iter: 53401', 'loss_train: 1.3525', 'acc_train: 0.5590', 'loss_val: 0.6725', 'acc_val: 0.6000', 'time: 15558.8593s')\n",
      "('iter: 53501', 'loss_train: 1.3547', 'acc_train: 0.5380', 'loss_val: 0.6705', 'acc_val: 0.6040', 'time: 15584.8078s')\n",
      "('iter: 53601', 'loss_train: 1.3522', 'acc_train: 0.5440', 'loss_val: 0.6711', 'acc_val: 0.5960', 'time: 15610.3600s')\n",
      "('iter: 53701', 'loss_train: 1.3470', 'acc_train: 0.5500', 'loss_val: 0.6692', 'acc_val: 0.6060', 'time: 15636.1714s')\n",
      "('iter: 53801', 'loss_train: 1.3502', 'acc_train: 0.5710', 'loss_val: 0.6689', 'acc_val: 0.6050', 'time: 15661.9592s')\n",
      "('iter: 53901', 'loss_train: 1.3480', 'acc_train: 0.5850', 'loss_val: 0.6670', 'acc_val: 0.6110', 'time: 15687.8195s')\n",
      "('iter: 54001', 'loss_train: 1.3529', 'acc_train: 0.5830', 'loss_val: 0.6701', 'acc_val: 0.6030', 'time: 15713.5967s')\n",
      "('iter: 54101', 'loss_train: 1.3541', 'acc_train: 0.5760', 'loss_val: 0.6693', 'acc_val: 0.6030', 'time: 15739.4404s')\n",
      "('iter: 54201', 'loss_train: 1.3552', 'acc_train: 0.5700', 'loss_val: 0.6732', 'acc_val: 0.5900', 'time: 15765.3266s')\n",
      "('iter: 54301', 'loss_train: 1.3541', 'acc_train: 0.5640', 'loss_val: 0.6732', 'acc_val: 0.5870', 'time: 15791.1929s')\n",
      "('iter: 54401', 'loss_train: 1.3565', 'acc_train: 0.5510', 'loss_val: 0.6769', 'acc_val: 0.5730', 'time: 15816.8828s')\n",
      "('iter: 54501', 'loss_train: 1.3561', 'acc_train: 0.5670', 'loss_val: 0.6753', 'acc_val: 0.5820', 'time: 15842.7539s')\n",
      "('iter: 54601', 'loss_train: 1.3572', 'acc_train: 0.5720', 'loss_val: 0.6768', 'acc_val: 0.5830', 'time: 15868.6218s')\n",
      "('iter: 54701', 'loss_train: 1.3591', 'acc_train: 0.5770', 'loss_val: 0.6799', 'acc_val: 0.5700', 'time: 15894.5781s')\n",
      "('iter: 54801', 'loss_train: 1.3593', 'acc_train: 0.5760', 'loss_val: 0.6806', 'acc_val: 0.5740', 'time: 15920.5912s')\n",
      "('iter: 54901', 'loss_train: 1.3605', 'acc_train: 0.5860', 'loss_val: 0.6824', 'acc_val: 0.5650', 'time: 15946.5626s')\n",
      "('iter: 55001', 'loss_train: 1.3587', 'acc_train: 0.5820', 'loss_val: 0.6798', 'acc_val: 0.5720', 'time: 15972.4644s')\n",
      "('iter: 55101', 'loss_train: 1.3590', 'acc_train: 0.5760', 'loss_val: 0.6810', 'acc_val: 0.5740', 'time: 15998.1379s')\n",
      "('iter: 55201', 'loss_train: 1.3632', 'acc_train: 0.5590', 'loss_val: 0.6801', 'acc_val: 0.5770', 'time: 16024.0829s')\n",
      "('iter: 55301', 'loss_train: 1.3591', 'acc_train: 0.5730', 'loss_val: 0.6787', 'acc_val: 0.5840', 'time: 16050.0122s')\n",
      "('iter: 55401', 'loss_train: 1.3564', 'acc_train: 0.5770', 'loss_val: 0.6765', 'acc_val: 0.5900', 'time: 16075.9147s')\n",
      "('iter: 55501', 'loss_train: 1.3540', 'acc_train: 0.5750', 'loss_val: 0.6734', 'acc_val: 0.5990', 'time: 16101.7966s')\n",
      "('iter: 55601', 'loss_train: 1.3536', 'acc_train: 0.5710', 'loss_val: 0.6762', 'acc_val: 0.5890', 'time: 16127.3954s')\n",
      "('iter: 55701', 'loss_train: 1.3523', 'acc_train: 0.5890', 'loss_val: 0.6720', 'acc_val: 0.6070', 'time: 16152.8903s')\n",
      "('iter: 55801', 'loss_train: 1.3550', 'acc_train: 0.5740', 'loss_val: 0.6723', 'acc_val: 0.6090', 'time: 16178.6385s')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('iter: 55901', 'loss_train: 1.3529', 'acc_train: 0.5780', 'loss_val: 0.6723', 'acc_val: 0.6130', 'time: 16204.2666s')\n",
      "('iter: 56001', 'loss_train: 1.3484', 'acc_train: 0.5880', 'loss_val: 0.6746', 'acc_val: 0.6130', 'time: 16230.0277s')\n",
      "('iter: 56101', 'loss_train: 1.3518', 'acc_train: 0.5820', 'loss_val: 0.6754', 'acc_val: 0.6090', 'time: 16255.5226s')\n",
      "('iter: 56201', 'loss_train: 1.3498', 'acc_train: 0.5750', 'loss_val: 0.6743', 'acc_val: 0.6150', 'time: 16280.8745s')\n",
      "('iter: 56301', 'loss_train: 1.3519', 'acc_train: 0.5800', 'loss_val: 0.6766', 'acc_val: 0.6080', 'time: 16306.3887s')\n",
      "('iter: 56401', 'loss_train: 1.3492', 'acc_train: 0.5740', 'loss_val: 0.6769', 'acc_val: 0.6110', 'time: 16332.2227s')\n",
      "('iter: 56501', 'loss_train: 1.3488', 'acc_train: 0.5730', 'loss_val: 0.6787', 'acc_val: 0.6030', 'time: 16358.1566s')\n",
      "('iter: 56601', 'loss_train: 1.3496', 'acc_train: 0.5850', 'loss_val: 0.6751', 'acc_val: 0.6140', 'time: 16384.0439s')\n",
      "('iter: 56701', 'loss_train: 1.3546', 'acc_train: 0.5800', 'loss_val: 0.6798', 'acc_val: 0.6000', 'time: 16409.9518s')\n",
      "('iter: 56801', 'loss_train: 1.3521', 'acc_train: 0.5760', 'loss_val: 0.6814', 'acc_val: 0.5960', 'time: 16435.4513s')\n",
      "('iter: 56901', 'loss_train: 1.3541', 'acc_train: 0.5680', 'loss_val: 0.6841', 'acc_val: 0.5920', 'time: 16461.4924s')\n",
      "('iter: 57001', 'loss_train: 1.3560', 'acc_train: 0.5650', 'loss_val: 0.6808', 'acc_val: 0.5970', 'time: 16487.2616s')\n",
      "('iter: 57101', 'loss_train: 1.3523', 'acc_train: 0.5650', 'loss_val: 0.6811', 'acc_val: 0.5930', 'time: 16513.2192s')\n",
      "('iter: 57201', 'loss_train: 1.3522', 'acc_train: 0.5660', 'loss_val: 0.6813', 'acc_val: 0.5910', 'time: 16539.2334s')\n",
      "('iter: 57301', 'loss_train: 1.3536', 'acc_train: 0.5600', 'loss_val: 0.6817', 'acc_val: 0.5890', 'time: 16565.2120s')\n",
      "('iter: 57401', 'loss_train: 1.3502', 'acc_train: 0.5690', 'loss_val: 0.6793', 'acc_val: 0.5970', 'time: 16591.0656s')\n",
      "('iter: 57501', 'loss_train: 1.3502', 'acc_train: 0.5750', 'loss_val: 0.6791', 'acc_val: 0.5980', 'time: 16616.8760s')\n",
      "('iter: 57601', 'loss_train: 1.3521', 'acc_train: 0.5730', 'loss_val: 0.6805', 'acc_val: 0.5910', 'time: 16642.5536s')\n",
      "('iter: 57701', 'loss_train: 1.3470', 'acc_train: 0.5790', 'loss_val: 0.6821', 'acc_val: 0.5830', 'time: 16668.4736s')\n",
      "('iter: 57801', 'loss_train: 1.3483', 'acc_train: 0.5920', 'loss_val: 0.6807', 'acc_val: 0.5840', 'time: 16694.3051s')\n",
      "('iter: 57901', 'loss_train: 1.3486', 'acc_train: 0.5790', 'loss_val: 0.6786', 'acc_val: 0.5840', 'time: 16719.9560s')\n",
      "('iter: 58001', 'loss_train: 1.3497', 'acc_train: 0.5750', 'loss_val: 0.6785', 'acc_val: 0.5860', 'time: 16745.6631s')\n",
      "('iter: 58101', 'loss_train: 1.3517', 'acc_train: 0.5720', 'loss_val: 0.6775', 'acc_val: 0.5940', 'time: 16771.3754s')\n",
      "('iter: 58601', 'loss_train: 1.3559', 'acc_train: 0.5570', 'loss_val: 0.6847', 'acc_val: 0.5760', 'time: 16900.0831s')\n",
      "('iter: 58701', 'loss_train: 1.3573', 'acc_train: 0.5640', 'loss_val: 0.6817', 'acc_val: 0.5920', 'time: 16925.6500s')\n",
      "('iter: 58801', 'loss_train: 1.3585', 'acc_train: 0.5640', 'loss_val: 0.6798', 'acc_val: 0.5950', 'time: 16951.1980s')\n",
      "('iter: 58901', 'loss_train: 1.3625', 'acc_train: 0.5620', 'loss_val: 0.6784', 'acc_val: 0.6010', 'time: 16976.7864s')\n",
      "('iter: 59001', 'loss_train: 1.3635', 'acc_train: 0.5570', 'loss_val: 0.6793', 'acc_val: 0.5880', 'time: 17002.2124s')\n",
      "('iter: 59101', 'loss_train: 1.3626', 'acc_train: 0.5580', 'loss_val: 0.6780', 'acc_val: 0.5920', 'time: 17028.0208s')\n",
      "('iter: 59201', 'loss_train: 1.3607', 'acc_train: 0.5560', 'loss_val: 0.6770', 'acc_val: 0.5970', 'time: 17053.5180s')\n",
      "('iter: 59301', 'loss_train: 1.3594', 'acc_train: 0.5640', 'loss_val: 0.6771', 'acc_val: 0.5940', 'time: 17079.0422s')\n",
      "('iter: 59401', 'loss_train: 1.3597', 'acc_train: 0.5820', 'loss_val: 0.6758', 'acc_val: 0.5940', 'time: 17104.9501s')\n",
      "('iter: 59501', 'loss_train: 1.3593', 'acc_train: 0.5920', 'loss_val: 0.6753', 'acc_val: 0.5970', 'time: 17130.8004s')\n",
      "('iter: 59601', 'loss_train: 1.3601', 'acc_train: 0.5830', 'loss_val: 0.6755', 'acc_val: 0.5970', 'time: 17156.5046s')\n",
      "('iter: 59701', 'loss_train: 1.3580', 'acc_train: 0.5900', 'loss_val: 0.6767', 'acc_val: 0.5910', 'time: 17182.4181s')\n",
      "('iter: 59801', 'loss_train: 1.3553', 'acc_train: 0.5950', 'loss_val: 0.6793', 'acc_val: 0.5890', 'time: 17208.0887s')\n",
      "('iter: 59901', 'loss_train: 1.3511', 'acc_train: 0.5960', 'loss_val: 0.6786', 'acc_val: 0.5940', 'time: 17233.7196s')\n",
      "('iter: 60001', 'loss_train: 1.3497', 'acc_train: 0.5910', 'loss_val: 0.6807', 'acc_val: 0.5960', 'time: 17259.3827s')\n",
      "('iter: 60101', 'loss_train: 1.3510', 'acc_train: 0.5990', 'loss_val: 0.6812', 'acc_val: 0.5900', 'time: 17285.1434s')\n",
      "('iter: 60201', 'loss_train: 1.3529', 'acc_train: 0.5820', 'loss_val: 0.6803', 'acc_val: 0.5880', 'time: 17311.1032s')\n",
      "('iter: 60301', 'loss_train: 1.3516', 'acc_train: 0.5800', 'loss_val: 0.6795', 'acc_val: 0.5940', 'time: 17336.8433s')\n",
      "('iter: 60401', 'loss_train: 1.3479', 'acc_train: 0.5780', 'loss_val: 0.6775', 'acc_val: 0.6040', 'time: 17362.7069s')\n",
      "('iter: 60501', 'loss_train: 1.3465', 'acc_train: 0.5860', 'loss_val: 0.6748', 'acc_val: 0.6070', 'time: 17388.4512s')\n",
      "('iter: 60601', 'loss_train: 1.3463', 'acc_train: 0.5800', 'loss_val: 0.6743', 'acc_val: 0.6070', 'time: 17414.1151s')\n",
      "('iter: 60701', 'loss_train: 1.3531', 'acc_train: 0.5800', 'loss_val: 0.6719', 'acc_val: 0.6100', 'time: 17440.1083s')\n",
      "('iter: 60801', 'loss_train: 1.3543', 'acc_train: 0.5790', 'loss_val: 0.6726', 'acc_val: 0.6090', 'time: 17466.1981s')\n",
      "('iter: 60901', 'loss_train: 1.3531', 'acc_train: 0.5780', 'loss_val: 0.6728', 'acc_val: 0.6040', 'time: 17492.1423s')\n",
      "('iter: 61001', 'loss_train: 1.3525', 'acc_train: 0.5700', 'loss_val: 0.6687', 'acc_val: 0.6140', 'time: 17517.9174s')\n",
      "('iter: 61101', 'loss_train: 1.3481', 'acc_train: 0.5810', 'loss_val: 0.6721', 'acc_val: 0.6050', 'time: 17543.9410s')\n",
      "('iter: 61201', 'loss_train: 1.3476', 'acc_train: 0.5800', 'loss_val: 0.6717', 'acc_val: 0.6090', 'time: 17569.7657s')\n",
      "('iter: 61301', 'loss_train: 1.3478', 'acc_train: 0.5760', 'loss_val: 0.6732', 'acc_val: 0.6010', 'time: 17595.6793s')\n",
      "('iter: 61401', 'loss_train: 1.3473', 'acc_train: 0.5790', 'loss_val: 0.6760', 'acc_val: 0.5890', 'time: 17621.6270s')\n",
      "('iter: 61501', 'loss_train: 1.3479', 'acc_train: 0.5780', 'loss_val: 0.6794', 'acc_val: 0.5840', 'time: 17647.4799s')\n",
      "('iter: 61601', 'loss_train: 1.3493', 'acc_train: 0.5790', 'loss_val: 0.6822', 'acc_val: 0.5750', 'time: 17673.3781s')\n",
      "('iter: 61701', 'loss_train: 1.3432', 'acc_train: 0.5950', 'loss_val: 0.6842', 'acc_val: 0.5750', 'time: 17700.9478s')\n",
      "('iter: 61801', 'loss_train: 1.3460', 'acc_train: 0.5860', 'loss_val: 0.6841', 'acc_val: 0.5700', 'time: 17733.0873s')\n",
      "('iter: 61901', 'loss_train: 1.3476', 'acc_train: 0.5810', 'loss_val: 0.6858', 'acc_val: 0.5680', 'time: 17764.7104s')\n",
      "('iter: 62001', 'loss_train: 1.3464', 'acc_train: 0.5880', 'loss_val: 0.6899', 'acc_val: 0.5580', 'time: 17792.4085s')\n",
      "('iter: 62101', 'loss_train: 1.3511', 'acc_train: 0.5750', 'loss_val: 0.6872', 'acc_val: 0.5690', 'time: 17824.7852s')\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, acc, val_loss, val_acc, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f\\n' % (n_iter, loss, acc, val_loss, val_acc)\n",
    "    log_file_stream.write(log_text)\n",
    "    if n_iter % 100 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "\n",
    "acc_q = deque(maxlen=1000)\n",
    "loss_q = deque(maxlen=1000)\n",
    "val_acc_q = deque(maxlen=1000)\n",
    "val_loss_q = deque(maxlen=1000)\n",
    "criterion = nn.BCELoss()\n",
    "# \n",
    "model = net\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "#\n",
    "interval = 100\n",
    "t = time.time()\n",
    "print 'start training.'\n",
    "best_acc  = 0\n",
    "best_loss = float('inf')\n",
    "for i in range(1,1000000):\n",
    "    with open('log.txt', 'a') as f:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "    #     positive\n",
    "        q,k,q_f,k_f = next(pos_G)\n",
    "        q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "        q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "        output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda())\n",
    "        acc = 1 if output.flatten().item() > 0.5 else 0\n",
    "        acc_q.append(acc)\n",
    "        pos_loss = criterion(output, torch.FloatTensor([[1]]).cuda() )\n",
    "\n",
    "#         negative\n",
    "        q,k,q_f,k_f = next(neg_G)\n",
    "        q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "        q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "        \n",
    "        output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda())\n",
    "        acc = 1 if output.flatten().item() < 0.5 else 0\n",
    "        acc_q.append(acc)\n",
    "        neg_loss = criterion(output, torch.FloatTensor([[0]]).cuda())\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss_q.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #     val\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_i = i % val_Q.shape[0]\n",
    "            q,k = val_Q[val_i,:], val_K[val_i,:]\n",
    "            q_f,k_f = val_Q_fea[val_i,:], val_K_fea[val_i,:]\n",
    "            q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "            q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "        \n",
    "            output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda())\n",
    "            val_acc = 1 if output.flatten().item() > 0.5 else 0\n",
    "            val_acc_q.append(val_acc)\n",
    "\n",
    "            val_loss = criterion(output, torch.FloatTensor([[1]]).cuda() )\n",
    "            val_loss_q.append(val_loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        acc = float(np.mean(acc_q))\n",
    "        loss = float(np.mean(loss_q))\n",
    "        val_acc = float(np.mean(val_acc_q))\n",
    "        val_loss = float(np.mean(val_loss_q))\n",
    "\n",
    "        if i % interval == 0:\n",
    "            print('iter: {:04d}'.format(i+1),\n",
    "                  'loss_train: {:.4f}'.format(loss),\n",
    "                  'acc_train: {:.4f}'.format(acc),\n",
    "                  'loss_val: {:.4f}'.format(val_loss),\n",
    "                  'acc_val: {:.4f}'.format(val_acc),\n",
    "                  'time: {:.4f}s'.format((time.time() - t)))\n",
    "        if i > 100:\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model, './best_acc.pt')\n",
    "                with open('./best.txt', 'a') as g:\n",
    "                    g.write('best acc at %d with %.5f\\n' % (i+1, best_acc))\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                torch.save(model, './best_loss.pt')\n",
    "                with open('./best.txt', 'a') as g:\n",
    "                    g.write('best loss at %d with %.5f\\n' % (i+1, best_loss))\n",
    "            \n",
    "        dump_log(model, i+1, loss, acc, val_loss, val_acc, f, './tmp.pt')\n",
    "\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# batch_size = 128\n",
    "# # xml_id_map[113].shape\n",
    "# def positive_bootsrap_generator(edges, xml_id_map):\n",
    "#     num_edge = len(edges)\n",
    "        \n",
    "#     while True:\n",
    "#         for idx in np.random.permutation(num_edge):\n",
    "#             src, dst = edges[idx, :]\n",
    "#             Q = xml_id_map[dst]\n",
    "#             K = xml_id_map[src]\n",
    "#             yield Q, K\n",
    "# def negative_bootsrap_generator(adj_mat, idx_map, xml_id_map, training_node_list):\n",
    "#     exist_node_list = xml_id_map.keys()\n",
    "#     exist_N = len(training_node_list)\n",
    "        \n",
    "#     while True:\n",
    "#         src = training_node_list[np.random.randint(exist_N)]\n",
    "#         dst = training_node_list[np.random.randint(exist_N)]\n",
    "#         while adj_mat[idx_map[src], idx_map[dst]] == 1:\n",
    "#             dst = training_node_list[np.random.randint(exist_N)]\n",
    "#         Q = xml_id_map[dst]\n",
    "#         K = xml_id_map[src]\n",
    "#         yield Q, K\n",
    "# def val_data(edges, xml_id_map):\n",
    "#     Q, K = [],[]\n",
    "    \n",
    "#     for idx in range(edges.shape[0]):\n",
    "#         src, dst = edges[idx, :]\n",
    "#         q = xml_id_map[dst]\n",
    "#         k = xml_id_map[src]\n",
    "#         Q.append(q)\n",
    "#         K.append(k)\n",
    "#     Q = np.vstack(Q)\n",
    "#     K = np.vstack(K)\n",
    "    \n",
    "#     return Q, K\n",
    "    \n",
    "# N = edges.shape[0]\n",
    "# idx = np.random.permutation(N)\n",
    "# train_idx = idx[N//10:]\n",
    "# val_idx = idx[:N//10]\n",
    "\n",
    "# pos_G = positive_bootsrap_generator(edges[train_idx,:], xml_id_map)\n",
    "# training_node_list = list(set(edges[train_idx,:].flatten().tolist()))\n",
    "# neg_G = negative_bootsrap_generator(adj_mat, idx_map, xml_id_map, training_node_list)\n",
    "# val_Q, val_K = val_data(edges[val_idx,:], xml_id_map)\n",
    "# q,k = next(pos_G)\n",
    "# print(q.shape,k.shape)\n",
    "# q,k = next(neg_G)\n",
    "# print(q.shape,k.shape)\n",
    "# print(val_Q.shape,val_K.shape)\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
