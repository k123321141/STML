{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate fake link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 161619/173364 [00:00<00:00, 262344.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 161619\n"
     ]
    }
   ],
   "source": [
    "# randomly sample test link\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from os.path import join\n",
    "def get_node_set(path):\n",
    "    # training data\n",
    "    edges_unordered = np.genfromtxt(path,\n",
    "                                    dtype=np.int32)\n",
    "    id_set = set(edges_unordered.flatten().tolist())\n",
    "    return id_set\n",
    "\n",
    "data_path = join('./','kaggle')\n",
    "# training data\n",
    "train_node_set = get_node_set(join(data_path,'t2-train.txt'))\n",
    "test_node_set = get_node_set(join(data_path,'t2-test.txt'))\n",
    "node_set = set.union(train_node_set, test_node_set)\n",
    "idx_map = {k:i for i,k in enumerate(list(node_set))}\n",
    "N = len(node_set)\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t2-train.txt'), dtype=np.int32)\n",
    "for i in range(links.shape[0]):\n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "out_degree = np.sum(adj_mat, axis=1).flatten()\n",
    "\n",
    "means, std = np.mean(out_degree), np.std(out_degree)\n",
    "\n",
    "rev_map = {v:k for k,v in idx_map.items()}\n",
    "total_link_num = links.shape[0] + int(np.sum(out_degree))\n",
    "with tqdm(total=total_link_num) as pbar:\n",
    "    with open(join(data_path,'t2-fake.txt'), 'w') as f:\n",
    "        for i in range(links.shape[0]):\n",
    "            src, dst = links[i].tolist()\n",
    "            s = '%d %d\\n' % (src, dst)\n",
    "            f.write(s)\n",
    "            pbar.update(1)\n",
    "        train_node_list = list(train_node_set)\n",
    "        for node_id in list(test_node_set):\n",
    "            i = idx_map[node_id]\n",
    "            d = int(np.round(np.random.normal(means, std)))\n",
    "            d = max(1, d)\n",
    "            \n",
    "            for j in range(d):\n",
    "                idx = np.random.randint(len(train_node_list))\n",
    "                dst = idx_map[train_node_list[idx]]\n",
    "                while adj_mat[i, dst] == 1 or dst == i:\n",
    "                    idx = np.random.randint(len(train_node_list))\n",
    "                    dst = idx_map[train_node_list[idx]]\n",
    "                \n",
    "                adj_mat[i, dst] = 1\n",
    "                s = '%d %d\\n' % (rev_map[i], rev_map[dst])\n",
    "                f.write(s)\n",
    "            \n",
    "                pbar.update(1)\n",
    "    \n",
    "print 'done', np.sum(adj_mat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read all 17500 xml files.\n",
      "Found 82709 unique tokens.\n",
      "Preparing embedding matrix.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from os.path import join\n",
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from constants import MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def quote_title_abstract(xml_path):\n",
    "    with open(xml_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    soup = BS(data)\n",
    "    title, abstract = soup.find('title').text, soup.find('abstract').text\n",
    "    return title.strip(), abstract.strip()\n",
    "\n",
    "# text preprocessing\n",
    "data_path = join('./','kaggle/')\n",
    "xml_dir = join(data_path, 't2-doc')\n",
    "xml_list = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "# print(len(xml_list))\n",
    "\n",
    "\n",
    "texts = []\n",
    "\n",
    "for xml in xml_list:\n",
    "    path = join(xml_dir,xml)\n",
    "    title, abstract = quote_title_abstract(path)\n",
    "    text = title + '' + abstract\n",
    "    texts.append(text)\n",
    "#     texts.append(title)\n",
    "#     texts.append(abstract)\n",
    "print('read all %d xml files.' % len(xml_list))\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "xml_id_map = {}\n",
    "for i,xml in enumerate(xml_list):\n",
    "    node_id = int(xml.replace('.xml',''))\n",
    "    xml_id_map[node_id] = data[i,:]\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "embeddings_index = {}\n",
    "# with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r', encoding='utf8') as f:\n",
    "with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb\n",
    "buf = np.genfromtxt('./t2.emb', skip_header=1, dtype=np.float32)\n",
    "nodes = buf[:,0].astype(np.int32)\n",
    "emb = buf[:,1:]\n",
    "\n",
    "node_emb_dict = {}\n",
    "for i in range(emb.shape[0]):\n",
    "    node_id = nodes[i]\n",
    "    x = emb[i,:]\n",
    "    node_emb_dict[node_id] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "torch.Size([1, 1])\n",
      "4263425\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "from constants import D_MODEL, STACKED_NUM,DK, DV, H, P_DROP, D_FF, MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "# environment\n",
    "with_gpu = torch.cuda.is_available()\n",
    "# with_gpu = False\n",
    "device = torch.device(\"cuda:0\" if with_gpu else \"cpu\")\n",
    "\n",
    "def positional_encoding(pos):\n",
    "    assert D_MODEL % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,D_MODEL], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "def get_pos_mat(length):\n",
    "    if length > MAX_SEQUENCE_LENGTH:\n",
    "        print('sequence length reach PE_MAT_CACHE. %d ' % length)\n",
    "        ret = torch.cat([positional_encoding(i) for i in range(length)], dim=0).to(device)\n",
    "        ret.requires_grad = False\n",
    "        global PE_CACHE_MATRIX\n",
    "        PE_CACHE_MATRIX = ret\n",
    "        return ret\n",
    "    else:\n",
    "        return PE_CACHE_MATRIX[:length]\n",
    "    \n",
    "PE_CACHE_MATRIX = torch.cat([positional_encoding(i) for i in range(0,MAX_SEQUENCE_LENGTH)], dim=0).to(device)\n",
    "PE_CACHE_MATRIX.requires_grad = False\n",
    "\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask=None):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).to(device)\n",
    "    out = torch.matmul(Q,K.t()) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out = out.masked_fill_(mask, -float('inf'))\n",
    "        \n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "                            \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, emb_matrix):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.emb = Word_Embedding(emb_matrix)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "        self.encoder = Stack_Encoder(layer_num, dk, dv, dm, h)\n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h)\n",
    "        \n",
    "        self.summary_weight = nn.Parameter(torch.FloatTensor(1, dm))\n",
    "        torch.nn.init.xavier_uniform_(self.summary_weight)\n",
    "        \n",
    "        self.output_linear = nn.Linear(3*dm, 1)\n",
    "\n",
    "    def forward(self, Q, K, Q_fea, K_fea):\n",
    "        \n",
    "#         encoder\n",
    "        K = self.emb(K)\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        K = K + get_pos_mat(MAX_SEQUENCE_LENGTH)\n",
    "        K = self.emb_drop(K)\n",
    "        \n",
    "        en_out = self.encoder(K)\n",
    "        \n",
    "#         decoder\n",
    "        Q = self.emb(Q)\n",
    "        seq_len, d = Q.size()\n",
    "        \n",
    "        Q = Q + get_pos_mat(MAX_SEQUENCE_LENGTH)\n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q, en_out)\n",
    "        \n",
    "        \n",
    "        summary = scaled_dot_attention(self.summary_weight, de_out, de_out)\n",
    "        x = torch.cat([summary, Q_fea.view([1,-1]), K_fea.view([1,-1])], dim=-1)\n",
    "        out = self.output_linear(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "class Word_Embedding(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super(Word_Embedding, self).__init__()\n",
    "        self.emb = nn.Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, padding_idx=0)\n",
    "        self.emb.weight = nn.parameter.Parameter(torch.FloatTensor(emb_matrix))\n",
    "        self.emb.weight.requires_grad_(False)\n",
    "        \n",
    "        self.linear = nn.Linear(EMBEDDING_DIM, D_MODEL, bias=False)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class Stack_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h):\n",
    "        super(Stack_Encoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([Encoder(dk, dv, dm, h) for i in range(layer_num)])\n",
    "\n",
    "    def forward(self, K):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.encoders:\n",
    "            K = lay(K)\n",
    "        return K               \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Encoder, self).__init__()\n",
    "#         attention residual block\n",
    "        self.multi_head_attention_layer = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.attention_norm_lay = nn.LayerNorm([dm,])\n",
    "        self.att_drop = nn.Dropout(P_DROP)\n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(D_MODEL, D_FF)\n",
    "        self.linear_drop = nn.Dropout(P_DROP)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        \n",
    "\n",
    "    def forward(self, K):\n",
    "#         attention\n",
    "        attention_out = self.multi_head_attention_layer(K, K, K)\n",
    "        attention_out = self.att_drop(attention_out)\n",
    "        att_out = self.attention_norm_lay(K + attention_out)\n",
    "#         feed forward\n",
    "        linear_out = self.fcn(att_out)\n",
    "        linear_out = self.linear_drop(linear_out)\n",
    "        out = self.ff_norm_lay(att_out + linear_out)\n",
    "        out = att_out + linear_out\n",
    "    \n",
    "        return out\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q, encoder_out):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        Q_len, d = Q.size()\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q, encoder_out, mask=None)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Decoder, self).__init__()\n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.Q_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.Q_att_drop = nn.Dropout(P_DROP)\n",
    "    \n",
    "#         query key attention residual block\n",
    "        self.QK_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.QK_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.QK_att_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "    \n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(D_MODEL, D_FF)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.linear_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "\n",
    "    def forward(self, Q, encoder_out, mask):\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(Q, Q, Q, mask)\n",
    "        Q_attention_out = self.Q_att_drop(Q_attention_out)\n",
    "        Q_att_out = self.Q_attention_norm_lay(Q + Q_attention_out)\n",
    "#         query key attention\n",
    "        QK_attention_out = self.QK_attention_lay(Q_att_out, encoder_out, encoder_out)\n",
    "        QK_attention_out = self.QK_att_drop(QK_attention_out)\n",
    "        QK_att_out = self.QK_attention_norm_lay(Q_att_out + QK_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(QK_att_out)\n",
    "        out = self.ff_norm_lay(QK_att_out + linear_out)\n",
    "        return out\n",
    "\n",
    "class Multi_Head_attention_layer(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Multi_Head_attention_layer, self).__init__()\n",
    "        self.Q_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.K_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.V_linears = nn.ModuleList([nn.Linear(dm, dv) for i in range(h)])\n",
    "        self.output_linear = nn.Linear(h*dv, dm)\n",
    "                            \n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask=None):\n",
    "        buf = []\n",
    "        for Q_linear, K_linear, V_linear in zip(self.Q_linears, self.K_linears, self.V_linears):\n",
    "            Q = Q_linear(Q_input)\n",
    "            K = K_linear(K_input)\n",
    "            V = V_linear(V_input)\n",
    "            buf.append(scaled_dot_attention(Q, K, V, mask))\n",
    "            \n",
    "        buf = torch.cat(buf,dim=-1)\n",
    "        out = self.output_linear(buf)\n",
    "        \n",
    "        return out      \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        self.cnn2 = nn.Conv1d(d_ff, d_model, 1)\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len,_ = x.size()\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        return x      \n",
    "    \n",
    "# encoder = Stack_Encoder(6, 64,64,20,8)\n",
    "# # print net\n",
    "Q = torch.randint(10000,[MAX_SEQUENCE_LENGTH,], dtype=torch.long).to(device)\n",
    "V = torch.randint(10000,[MAX_SEQUENCE_LENGTH,], dtype=torch.long).to(device)\n",
    "Q_fea = torch.rand([D_MODEL,]).to(device)\n",
    "K_fea = torch.rand([D_MODEL,]).to(device)\n",
    "net = Transformer(STACKED_NUM, DK, DV, D_MODEL, H, embedding_matrix).to(device)\n",
    "print(Q.dtype)\n",
    "o = net(Q, V, Q_fea, K_fea)\n",
    "# print t\n",
    "print(o.size())\n",
    "# print o\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_m = torch.load('./best_acc.pt')\n",
    "# tmp_m\n",
    "# net.load_state_dict(tmp_m.state_dict())\n",
    "# torch.nn.init.xavier_uniform_(net.output_linear.weight)\n",
    "# print 'load weight done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t2-fake.txt'), dtype=np.int32)\n",
    "idx_map = {node:idx for idx, node in enumerate(list(set(links.flatten().tolist())))}\n",
    "N = links.shape[0]\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "for i in range(links.shape[0]):\n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0]\n",
      " [0 0 1 1 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "[[0 0 1 1 1 0]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "[0 0 0 1 1 3]\n",
      "[4 3 2 5 4 2]\n",
      "  (0, 1)\tTrue\n",
      "  (0, 2)\tTrue\n",
      "  (0, 3)\tTrue\n",
      "  (0, 4)\tTrue\n",
      "  (1, 2)\tTrue\n",
      "  (1, 3)\tTrue\n",
      "  (1, 4)\tTrue\n",
      "  (1, 5)\tTrue\n",
      "  (3, 2)\tTrue\n",
      "  (3, 4)\tTrue\n",
      "  (3, 5)\tTrue\n",
      "  (5, 2)\tTrue\n",
      "[[0 1 1 1 1 0]\n",
      " [0 0 1 1 1 1]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 1 0 1 1]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0]]\n",
      "[0 1 1 1 3 3 5]\n",
      "  (0, 1)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 5)\t1\n",
      "  (5, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "# adj_sp = sp.coo_matrix((np.ones(links.shape[0]), (links[:, 0], links[:, 1])),\n",
    "#                         shape=(N, N),\n",
    "#                         dtype=np.int8)\n",
    "# adj_csr = adj_sp.tocsr() \n",
    "# r = adj_csr.multiply(adj_csr)\n",
    "l = np.asarray([[0,1],[1,2],[1,3],[1,4],[3,4],[3,5],[5,2]])\n",
    "\n",
    "adj_sp = sp.coo_matrix((np.ones(l.shape[0]), (l[:, 0], l[:, 1])),\n",
    "                        shape=(6, 6),\n",
    "                        dtype=np.int8)\n",
    "adj_csr = adj_sp.tocsr() \n",
    "# r = sp.csr_matrix.dot(adj_csr,adj_csr)\n",
    "r = sp.coo_matrix.dot(adj_sp,adj_sp)\n",
    "print adj_sp.todense() \n",
    "print r.todense()\n",
    "print r.tocoo().row\n",
    "print r.tocoo().col\n",
    "\n",
    "r2 = adj_sp+r > 0\n",
    "print r2\n",
    "print r2.astype(np.uint8).todense()\n",
    "print adj_sp.row\n",
    "print adj_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((150,), (150,), (128,), (128,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2003515/2003515 [00:06<00:00, 299428.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((150,), (150,), (128,), (128,))\n",
      "((16161, 150), (16161, 150), (16161, 128), (16161, 128))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def adj_iter(adj_mat):\n",
    "    \n",
    "    N = adj_mat.shape[0]\n",
    "    ret = adj_mat.copy()\n",
    "    with tqdm(total=N*N) as pbar:\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if adj[i, j] == 1:\n",
    "                    for k in range(N):\n",
    "                        if adj[j, k] == 1:\n",
    "                            ret[i, k] = 1\n",
    "                pbar.update(1)\n",
    "def positive_bootsrap_generator(edges, xml_id_map, node_emb_dict):\n",
    "    num_edge = len(edges)\n",
    "        \n",
    "    while True:\n",
    "        for idx in np.random.permutation(num_edge):\n",
    "            src, dst = edges[idx, :]\n",
    "            Q = xml_id_map[dst]\n",
    "            K = xml_id_map[src]\n",
    "            Q_fea = node_emb_dict[dst]\n",
    "            K_fea = node_emb_dict[src]\n",
    "            yield Q, K, Q_fea, K_fea\n",
    "def negative_bootsrap_generator(adj_mat, links, idx_map, xml_id_map, training_node_list, node_emb_dict, neighbor_link_rate=0.8):\n",
    "    \n",
    "    \n",
    "    exist_node_list = xml_id_map.keys()\n",
    "    exist_N = len(training_node_list)\n",
    "    N = adj_mat.shape[0]\n",
    "    \n",
    "#     adj mat\n",
    "    links = np.array(list(map(idx_map.get, links.flatten())),\n",
    "                     dtype=np.int32).reshape(links.shape)\n",
    "    \n",
    "    adj_sp = sp.coo_matrix((np.ones(links.shape[0]), (links[:, 0], links[:, 1])),\n",
    "                        shape=(N, N),\n",
    "                        dtype=np.uint8)\n",
    "    adj_sp_2 = (sp.coo_matrix.dot(adj_sp,adj_sp) + adj_sp).tocoo()\n",
    "    \n",
    "    rev_map = {v:k for k,v in idx_map.items()}\n",
    "    adj_map = {i:[] for i in range(N)}\n",
    "    with tqdm(total=len(adj_sp_2.row)) as pbar:\n",
    "        for i,j,v in zip(adj_sp_2.row, adj_sp_2.col, adj_sp_2.data):\n",
    "            if adj_mat[i, j] != 1 and v == 1:\n",
    "                adj_map[i].append(j)\n",
    "            pbar.update(1)\n",
    "#             print i,N\n",
    "                \n",
    "    while True:\n",
    "        src = training_node_list[np.random.randint(exist_N)]\n",
    "        \n",
    "#         choose neighbor link\n",
    "        if np.random.rand(1) <= neighbor_link_rate:\n",
    "        \n",
    "            i = idx_map[src]\n",
    "            high = len(adj_map[i])\n",
    "            while high == 0:\n",
    "                src = training_node_list[np.random.randint(exist_N)]\n",
    "                i = idx_map[src]\n",
    "                high = len(adj_map[i])\n",
    "                \n",
    "            idx = np.random.randint(high)\n",
    "            dst = adj_map[i][idx]\n",
    "            dst = rev_map[dst]\n",
    "        else:\n",
    "            dst = training_node_list[np.random.randint(exist_N)]\n",
    "            while adj_mat[idx_map[src], idx_map[dst]] == 1:\n",
    "                dst = training_node_list[np.random.randint(exist_N)]\n",
    "        Q = xml_id_map[dst]\n",
    "        K = xml_id_map[src]\n",
    "        Q_fea = node_emb_dict[dst]\n",
    "        K_fea = node_emb_dict[src]\n",
    "        yield Q, K, Q_fea, K_fea\n",
    "\n",
    "def val_data(edges, xml_id_map):\n",
    "    Q, K = [],[]\n",
    "    Q_f, K_f = [],[]\n",
    "    \n",
    "    for idx in range(edges.shape[0]):\n",
    "        src, dst = edges[idx, :]\n",
    "        q = xml_id_map[dst]\n",
    "        k = xml_id_map[src]\n",
    "        q_fea = node_emb_dict[dst]\n",
    "        k_fea = node_emb_dict[src]\n",
    "        \n",
    "        Q.append(q)\n",
    "        K.append(k)\n",
    "        Q_f.append(q_fea)\n",
    "        K_f.append(k_fea)\n",
    "        \n",
    "    Q = np.vstack(Q)\n",
    "    K = np.vstack(K)\n",
    "    Q_fea = np.vstack(Q_f)\n",
    "    K_fea = np.vstack(K_f)\n",
    "    \n",
    "    return Q, K, Q_fea, K_fea\n",
    "    \n",
    "N = links.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "train_idx = idx[N//10:]\n",
    "val_idx = idx[:N//10]\n",
    "\n",
    "pos_G = positive_bootsrap_generator(links[train_idx,:], xml_id_map, node_emb_dict)\n",
    "training_node_list = list(set(links[train_idx,:].flatten().tolist()))\n",
    "neg_G = negative_bootsrap_generator(adj_mat, links, idx_map, xml_id_map, training_node_list, node_emb_dict)\n",
    "val_Q, val_K, val_Q_fea, val_K_fea = val_data(links[val_idx,:], xml_id_map)\n",
    "q,k,q_f,k_f = next(pos_G)\n",
    "print(q.shape,k.shape, q_f.shape, k_f.shape)\n",
    "q,k,q_f,k_f = next(neg_G)\n",
    "print(q.shape,k.shape, q_f.shape, k_f.shape)\n",
    "print(val_Q.shape,val_K.shape, val_Q_fea.shape, val_K_fea.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training.\n",
      "('iter: 0101', 'loss_train: 1.3817', 'acc_train: 0.5350', 'loss_val: 0.6835', 'acc_val: 0.5600', 'time: 15.4182s')\n",
      "('iter: 0201', 'loss_train: 1.3931', 'acc_train: 0.5150', 'loss_val: 0.6881', 'acc_val: 0.5450', 'time: 30.3636s')\n",
      "('iter: 0301', 'loss_train: 1.3954', 'acc_train: 0.5017', 'loss_val: 0.6917', 'acc_val: 0.5367', 'time: 45.3021s')\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, acc, val_loss, val_acc, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f\\n' % (n_iter, loss, acc, val_loss, val_acc)\n",
    "    log_file_stream.write(log_text)\n",
    "    if n_iter % 100 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "\n",
    "acc_q = deque(maxlen=1000)\n",
    "loss_q = deque(maxlen=1000)\n",
    "val_acc_q = deque(maxlen=1000)\n",
    "val_loss_q = deque(maxlen=1000)\n",
    "criterion = nn.BCELoss()\n",
    "# \n",
    "model = net\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "#\n",
    "interval = 100\n",
    "t = time.time()\n",
    "print 'start training.'\n",
    "best_acc  = 0\n",
    "best_loss = float('inf')\n",
    "for i in range(1,1000000):\n",
    "    with open('log.txt', 'a') as f:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "    #     positive\n",
    "        q,k,q_f,k_f = next(pos_G)\n",
    "        q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "        q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "        output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda())\n",
    "        acc = 1 if output.flatten().item() > 0.5 else 0\n",
    "        acc_q.append(acc)\n",
    "        pos_loss = criterion(output, torch.FloatTensor([[1]]).cuda() )\n",
    "\n",
    "#         negative\n",
    "        q,k,q_f,k_f = next(neg_G)\n",
    "        q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "        q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "        \n",
    "        output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda())\n",
    "        acc = 1 if output.flatten().item() < 0.5 else 0\n",
    "        acc_q.append(acc)\n",
    "        neg_loss = criterion(output, torch.FloatTensor([[0]]).cuda())\n",
    "        loss = pos_loss + neg_loss\n",
    "        loss_q.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    #     val\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_i = i % val_Q.shape[0]\n",
    "            q,k = val_Q[val_i,:], val_K[val_i,:]\n",
    "            q_f,k_f = val_Q_fea[val_i,:], val_K_fea[val_i,:]\n",
    "            q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "            q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "        \n",
    "            output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda())\n",
    "            val_acc = 1 if output.flatten().item() > 0.5 else 0\n",
    "            val_acc_q.append(val_acc)\n",
    "\n",
    "            val_loss = criterion(output, torch.FloatTensor([[1]]).cuda() )\n",
    "            val_loss_q.append(val_loss.item())\n",
    "\n",
    "        model.eval()\n",
    "        acc = float(np.mean(acc_q))\n",
    "        loss = float(np.mean(loss_q))\n",
    "        val_acc = float(np.mean(val_acc_q))\n",
    "        val_loss = float(np.mean(val_loss_q))\n",
    "\n",
    "        if i % interval == 0:\n",
    "            print('iter: {:04d}'.format(i+1),\n",
    "                  'loss_train: {:.4f}'.format(loss),\n",
    "                  'acc_train: {:.4f}'.format(acc),\n",
    "                  'loss_val: {:.4f}'.format(val_loss),\n",
    "                  'acc_val: {:.4f}'.format(val_acc),\n",
    "                  'time: {:.4f}s'.format((time.time() - t)))\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            torch.save(model, './best_acc.pt')\n",
    "            with open('./best.txt', 'a') as g:\n",
    "                g.write('best acc at %d with %.5f\\n' % (i+1, best_acc))\n",
    "                \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            torch.save(model, './best_loss.pt')\n",
    "            with open('./best.txt', 'a') as g:\n",
    "                g.write('best loss at %d with %.5f\\n' % (i+1, best_loss))\n",
    "            \n",
    "        dump_log(model, i+1, loss, acc, val_loss, val_acc, f, './tmp.pt')\n",
    "\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "edges_unordered = np.genfromtxt('./kaggle/t2-test.txt', dtype=np.int32)\n",
    "with torch.no_grad():\n",
    "    with open('pred.txt.csv', 'w') as f:\n",
    "        f.write('query_id,prediction\\n')\n",
    "        for i in range(edges_unordered.shape[0]):\n",
    "            src, dst = edges_unordered[i, :]\n",
    "            if src not in node_emb_dict or dst not in node_emb_dict:\n",
    "                f.write('%d,%d\\n' % (1 + i, out))\n",
    "                continue\n",
    "            q = xml_id_map[dst]\n",
    "            k = xml_id_map[src]\n",
    "            q_f = node_emb_dict[dst]\n",
    "            k_f = node_emb_dict[src]\n",
    "\n",
    "            q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "            q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "\n",
    "            output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda()).flatten().item()\n",
    "\n",
    "            out = 1 if output >= 0.5 else 0\n",
    "            f.write('%d,%d\\n' % (1 + i, out))\n",
    "print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# batch_size = 128\n",
    "# # xml_id_map[113].shape\n",
    "# def positive_bootsrap_generator(edges, xml_id_map):\n",
    "#     num_edge = len(edges)\n",
    "        \n",
    "#     while True:\n",
    "#         for idx in np.random.permutation(num_edge):\n",
    "#             src, dst = edges[idx, :]\n",
    "#             Q = xml_id_map[dst]\n",
    "#             K = xml_id_map[src]\n",
    "#             yield Q, K\n",
    "# def negative_bootsrap_generator(adj_mat, idx_map, xml_id_map, training_node_list):\n",
    "#     exist_node_list = xml_id_map.keys()\n",
    "#     exist_N = len(training_node_list)\n",
    "        \n",
    "#     while True:\n",
    "#         src = training_node_list[np.random.randint(exist_N)]\n",
    "#         dst = training_node_list[np.random.randint(exist_N)]\n",
    "#         while adj_mat[idx_map[src], idx_map[dst]] == 1:\n",
    "#             dst = training_node_list[np.random.randint(exist_N)]\n",
    "#         Q = xml_id_map[dst]\n",
    "#         K = xml_id_map[src]\n",
    "#         yield Q, K\n",
    "# def val_data(edges, xml_id_map):\n",
    "#     Q, K = [],[]\n",
    "    \n",
    "#     for idx in range(edges.shape[0]):\n",
    "#         src, dst = edges[idx, :]\n",
    "#         q = xml_id_map[dst]\n",
    "#         k = xml_id_map[src]\n",
    "#         Q.append(q)\n",
    "#         K.append(k)\n",
    "#     Q = np.vstack(Q)\n",
    "#     K = np.vstack(K)\n",
    "    \n",
    "#     return Q, K\n",
    "    \n",
    "# N = edges.shape[0]\n",
    "# idx = np.random.permutation(N)\n",
    "# train_idx = idx[N//10:]\n",
    "# val_idx = idx[:N//10]\n",
    "\n",
    "# pos_G = positive_bootsrap_generator(edges[train_idx,:], xml_id_map)\n",
    "# training_node_list = list(set(edges[train_idx,:].flatten().tolist()))\n",
    "# neg_G = negative_bootsrap_generator(adj_mat, idx_map, xml_id_map, training_node_list)\n",
    "# val_Q, val_K = val_data(edges[val_idx,:], xml_id_map)\n",
    "# q,k = next(pos_G)\n",
    "# print(q.shape,k.shape)\n",
    "# q,k = next(neg_G)\n",
    "# print(q.shape,k.shape)\n",
    "# print(val_Q.shape,val_K.shape)\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
