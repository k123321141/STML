{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process text info and time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "100%|██████████| 17500/17500 [00:09<00:00, 1868.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read all 17500 xml files.\n",
      "error count 0\n",
      "min weeks in training set : 68, avg weeks : 396\n",
      "Found 82615 unique tokens.\n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from os.path import join\n",
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from constants import MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "np.random.seed(1337)\n",
    "def replace_timezone_fn(s):\n",
    "    return re.sub('(\\d){2}:(\\d){2}:(\\d){2} [A-Z]{3}', _sub_tz, s)\n",
    "def _sub_tz(matched):\n",
    "    return re.sub('[A-Za-z]{3}', 'GMT', matched.group(0))\n",
    "     \n",
    "def retrive_date(pat_obj_1, pat_obj_2, pat_obj_3, s):\n",
    "    s = s.lower()\n",
    "    \n",
    "    match_1 = pat_obj_1.search(s)\n",
    "    match_2 = None\n",
    "    match_3 = None\n",
    "    \n",
    "    if match_1 is None:\n",
    "        match_2 = pat_obj_2.search(s)\n",
    "    if match_2 is None:\n",
    "        match_3 = pat_obj_3.search(s)\n",
    "    assert (match_1 is not None) | (match_2 is not None) | (match_3 is not None)\n",
    "    \n",
    "        \n",
    "    if match_1 is not None:\n",
    "        days, months, years = match_1.groups()\n",
    "        date_str = '%s %s %s' % (days, months, years)\n",
    "    if match_2 is not None:\n",
    "        days, months, years = match_2.groups()\n",
    "        date_str = '%s %s 19%s' % (days, months, years)\n",
    "    if match_3 is not None:\n",
    "        months, days, years = match_3.groups()\n",
    "        date_str = '%s %s %s' % (days, months, years)\n",
    "    \n",
    "    if len(months) > 3:\n",
    "        return datetime.strptime(date_str, '%d %B %Y')\n",
    "    else:\n",
    "        return datetime.strptime(date_str, '%d %b %Y')\n",
    "    \n",
    "\n",
    "def prep_date(date):\n",
    "#     min date in training set 1991-12-31\n",
    "    secs = (date - datetime.strptime('1990-9-9', '%Y-%m-%d')).total_seconds()\n",
    "    weeks = np.round(secs // (60*60*24*7))\n",
    "    return weeks\n",
    "\n",
    "def quote_title_date_abstract(xml_path):\n",
    "    with open(xml_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    soup = BS(data)\n",
    "    title, abstract, date = soup.find('title').text, soup.find('abstract').text, soup.find('date').text\n",
    "    \n",
    "    return title.strip(), abstract.strip(), date.strip()\n",
    "\n",
    "# text preprocessing\n",
    "data_path = join('./','kaggle/')\n",
    "xml_dir = join(data_path, 't3-doc')\n",
    "xml_list = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "# print(len(xml_list))\n",
    "\n",
    "\n",
    "texts = []\n",
    "weeks = []\n",
    "c = 0\n",
    "# \n",
    "month_pattern = '(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|january|february|march|april|may|june|july|august|september|october|november|december)'\n",
    "pat_obj_1 = re.compile('([0-9]+) %s ([0-9]{4}|20[0-9]{3})' % month_pattern)\n",
    "pat_obj_2 = re.compile('([0-9]+) %s (9[0-9])' % month_pattern)\n",
    "pat_obj_3 = re.compile('%s ([0-9]+) .* ([0-9]{4}|20[0-9]{3})' % month_pattern)\n",
    "# \n",
    "\n",
    "with tqdm(total=len(xml_list)) as pbar:\n",
    "    for xml in xml_list:\n",
    "        \n",
    "        \n",
    "        path = join(xml_dir,xml)\n",
    "        title, abstract, date_str = quote_title_date_abstract(path)\n",
    "        text = title + '' + abstract\n",
    "#         date special case\n",
    "#         01/01/93 13:35:33 GMT+0100 12330.xml\n",
    "        if xml == '12330.xml':\n",
    "            date_str = '1 Jan 93'\n",
    "\n",
    "        try :\n",
    "            date = retrive_date(pat_obj_1, pat_obj_2, pat_obj_3, date_str)\n",
    "        except AssertionError:\n",
    "            print date_str, xml\n",
    "            c+=1 \n",
    "            continue\n",
    "        texts.append(text)\n",
    "        weeks.append(prep_date(date))\n",
    "        pbar.update(1)\n",
    "print('read all %d xml files.' % len(xml_list))\n",
    "print 'error count %d' % c\n",
    "min_w, avg_w, std_w = (min(weeks), np.mean(weeks), np.std(weeks))\n",
    "print 'min weeks in training set : %.0f, avg weeks : %.0f' % (min_w, avg_w)\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "xml_id_map = {}\n",
    "for i,xml in enumerate(xml_list):\n",
    "    node_id = int(xml.replace('.xml',''))\n",
    "    w = float(weeks[i] - avg_w)/std_w \n",
    "    w = np.array([w,])\n",
    "    xml_id_map[node_id] = (data[i,:], w)\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emb\n",
    "buf = np.genfromtxt('./t3.emb', skip_header=1, dtype=np.float32)\n",
    "nodes = buf[:,0].astype(np.int32)\n",
    "emb = buf[:,1:]\n",
    "\n",
    "node_emb_dict = {}\n",
    "for i in range(emb.shape[0]):\n",
    "    node_id = nodes[i]\n",
    "    x = emb[i,:]\n",
    "    node_emb_dict[node_id] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "links = np.genfromtxt(join(data_path,'t3-fake.txt'), dtype=np.int32)\n",
    "idx_map = {node:idx for idx, node in enumerate(list(set(links.flatten().tolist())))}\n",
    "N = links.shape[0]\n",
    "adj_mat = np.zeros([N,N], dtype=np.uint8)\n",
    "for i in range(links.shape[0]):\n",
    "    src, dst = links[i].tolist()\n",
    "    adj_mat[idx_map[src], idx_map[dst]] = 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "from constants import D_MODEL, STACKED_NUM,DK, DV, H, P_DROP, D_FF, MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "# environment\n",
    "with_gpu = torch.cuda.is_available()\n",
    "# with_gpu = False\n",
    "device = torch.device(\"cuda:0\" if with_gpu else \"cpu\")\n",
    "\n",
    "def positional_encoding(pos):\n",
    "    assert D_MODEL % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,D_MODEL], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "def get_pos_mat(length):\n",
    "    if length > MAX_SEQUENCE_LENGTH:\n",
    "        print('sequence length reach PE_MAT_CACHE. %d ' % length)\n",
    "        ret = torch.cat([positional_encoding(i) for i in range(length)], dim=0).to(device)\n",
    "        ret.requires_grad = False\n",
    "        global PE_CACHE_MATRIX\n",
    "        PE_CACHE_MATRIX = ret\n",
    "        return ret\n",
    "    else:\n",
    "        return PE_CACHE_MATRIX[:length]\n",
    "    \n",
    "PE_CACHE_MATRIX = torch.cat([positional_encoding(i) for i in range(0,MAX_SEQUENCE_LENGTH)], dim=0).to(device)\n",
    "PE_CACHE_MATRIX.requires_grad = False\n",
    "\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask=None):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).to(device)\n",
    "    out = torch.matmul(Q,K.t()) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out = out.masked_fill_(mask, -float('inf'))\n",
    "        \n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "                            \n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, emb_matrix):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.emb = Word_Embedding(emb_matrix)\n",
    "        \n",
    "        self.emb_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "        self.encoder = Stack_Encoder(layer_num, dk, dv, dm, h)\n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h)\n",
    "        self.summary_decoder = Stack_Decoder(2, dk, dv, dm, h)\n",
    "        \n",
    "        self.summary_weight = nn.Parameter(torch.FloatTensor(1, dm))\n",
    "        torch.nn.init.xavier_uniform_(self.summary_weight)\n",
    "        \n",
    "#         self.q_weeks_linear = nn.Linear(1, dm//2)\n",
    "#         self.k_weeks_linear = nn.Linear(1, dm//2)\n",
    "        self.output_linear = nn.Linear(3*dm+2, dm)\n",
    "        self.output_linear2 = nn.Linear(dm, dm)\n",
    "        self.output_linear3 = nn.Linear(dm, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, Q, K, Q_fea, K_fea, Q_w, K_w):\n",
    "        \n",
    "#         encoder\n",
    "        K = self.emb(K)\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        K = K + get_pos_mat(MAX_SEQUENCE_LENGTH)\n",
    "        K = self.emb_drop(K)\n",
    "        \n",
    "        en_out = self.encoder(K)\n",
    "        \n",
    "#         decoder\n",
    "        Q = self.emb(Q)\n",
    "        seq_len, d = Q.size()\n",
    "        \n",
    "        Q = Q + get_pos_mat(MAX_SEQUENCE_LENGTH)\n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q, en_out)\n",
    "        summary = self.summary_decoder(self.summary_weight, de_out)\n",
    "\n",
    "#         q_w_out = F.selu(self.q_weeks_linear(Q_w))\n",
    "#         k_w_out = F.selu(self.k_weeks_linear(K_w))\n",
    "        \n",
    "        \n",
    "#         x = torch.cat([summary, Q_fea.view([1,-1]), K_fea.view([1,-1]), q_w_out.view([1,-1]), k_w_out.view([1,-1])], dim=-1)\n",
    "        x = torch.cat([summary, Q_fea.view([1,-1]), K_fea.view([1,-1]), Q_w.view([1,-1]), K_w.view([1,-1])], dim=-1)\n",
    "        out = self.output_linear(x)\n",
    "        \n",
    "        out = self.output_linear2(F.selu(out))\n",
    "        out = self.output_linear3(F.selu(out))\n",
    "        \n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "class Word_Embedding(nn.Module):\n",
    "    def __init__(self, emb_matrix):\n",
    "        super(Word_Embedding, self).__init__()\n",
    "        self.emb = nn.Embedding(MAX_NUM_WORDS, EMBEDDING_DIM, padding_idx=0)\n",
    "        self.emb.weight = nn.parameter.Parameter(torch.FloatTensor(emb_matrix))\n",
    "        self.emb.weight.requires_grad_(False)\n",
    "        \n",
    "        self.linear = nn.Linear(EMBEDDING_DIM, D_MODEL, bias=False)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "    \n",
    "class Stack_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h):\n",
    "        super(Stack_Encoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([Encoder(dk, dv, dm, h) for i in range(layer_num)])\n",
    "\n",
    "    def forward(self, K):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.encoders:\n",
    "            K = lay(K)\n",
    "        return K               \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Encoder, self).__init__()\n",
    "#         attention residual block\n",
    "        self.multi_head_attention_layer = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.attention_norm_lay = nn.LayerNorm([dm,])\n",
    "        self.att_drop = nn.Dropout(P_DROP)\n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(D_MODEL, D_FF)\n",
    "        self.linear_drop = nn.Dropout(P_DROP)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        \n",
    "\n",
    "    def forward(self, K):\n",
    "#         attention\n",
    "        attention_out = self.multi_head_attention_layer(K, K, K)\n",
    "        attention_out = self.att_drop(attention_out)\n",
    "        att_out = self.attention_norm_lay(K + attention_out)\n",
    "#         feed forward\n",
    "        linear_out = self.fcn(att_out)\n",
    "        linear_out = self.linear_drop(linear_out)\n",
    "        out = self.ff_norm_lay(att_out + linear_out)\n",
    "        out = att_out + linear_out\n",
    "    \n",
    "        return out\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Encoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q, encoder_out):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        Q_len, d = Q.size()\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q, encoder_out, mask=None)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Decoder, self).__init__()\n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.Q_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.Q_att_drop = nn.Dropout(P_DROP)\n",
    "    \n",
    "#         query key attention residual block\n",
    "        self.QK_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.QK_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.QK_att_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "    \n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(D_MODEL, D_FF)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.linear_drop = nn.Dropout(P_DROP)\n",
    "        \n",
    "\n",
    "    def forward(self, Q, encoder_out, mask):\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(Q, Q, Q, mask)\n",
    "        Q_attention_out = self.Q_att_drop(Q_attention_out)\n",
    "        Q_att_out = self.Q_attention_norm_lay(Q + Q_attention_out)\n",
    "#         query key attention\n",
    "        QK_attention_out = self.QK_attention_lay(Q_att_out, encoder_out, encoder_out)\n",
    "        QK_attention_out = self.QK_att_drop(QK_attention_out)\n",
    "        QK_att_out = self.QK_attention_norm_lay(Q_att_out + QK_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(QK_att_out)\n",
    "        out = self.ff_norm_lay(QK_att_out + linear_out)\n",
    "        return out\n",
    "\n",
    "class Multi_Head_attention_layer(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Multi_Head_attention_layer, self).__init__()\n",
    "        self.Q_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.K_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.V_linears = nn.ModuleList([nn.Linear(dm, dv) for i in range(h)])\n",
    "        self.output_linear = nn.Linear(h*dv, dm)\n",
    "                            \n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask=None):\n",
    "        buf = []\n",
    "        for Q_linear, K_linear, V_linear in zip(self.Q_linears, self.K_linears, self.V_linears):\n",
    "            Q = Q_linear(Q_input)\n",
    "            K = K_linear(K_input)\n",
    "            V = V_linear(V_input)\n",
    "            buf.append(scaled_dot_attention(Q, K, V, mask))\n",
    "            \n",
    "        buf = torch.cat(buf,dim=-1)\n",
    "        out = self.output_linear(buf)\n",
    "        \n",
    "        return out      \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        self.cnn2 = nn.Conv1d(d_ff, d_model, 1)\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len,_ = x.size()\n",
    "        x = x.unsqueeze(0)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = x.squeeze(0)\n",
    "        \n",
    "        return x      \n",
    "    \n",
    "# encoder = Stack_Encoder(6, 64,64,20,8)\n",
    "# # print net\n",
    "Q = torch.randint(10000,[MAX_SEQUENCE_LENGTH,], dtype=torch.long).to(device)\n",
    "V = torch.randint(10000,[MAX_SEQUENCE_LENGTH,], dtype=torch.long).to(device)\n",
    "Q_fea = torch.rand([D_MODEL,]).to(device)\n",
    "K_fea = torch.rand([D_MODEL,]).to(device)\n",
    "Q_w = torch.rand([1,]).to(device)\n",
    "K_w = torch.rand([1,]).to(device)\n",
    "\n",
    "\n",
    "print 'model '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 15808/74798 [10:36<38:56, 25.25it/s]"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "model = torch.load('./best_loss.pt')\n",
    "model.eval()\n",
    "\n",
    "edges_unordered = np.genfromtxt('./kaggle/t3-test.txt', dtype=np.int32)\n",
    "c = 1\n",
    "with torch.no_grad():\n",
    "    with open('pred.txt.csv', 'w') as f:\n",
    "        with tqdm(total=edges_unordered.shape[0]) as pbar:\n",
    "\n",
    "            f.write('query_id,prediction\\n')\n",
    "            for i in range(edges_unordered.shape[0]):\n",
    "\n",
    "                src, dst = edges_unordered[i, :]\n",
    "\n",
    "                q, q_w = xml_id_map[dst]\n",
    "                k, k_w = xml_id_map[src]\n",
    "\n",
    "                q_f = node_emb_dict[dst]\n",
    "                k_f = node_emb_dict[src]\n",
    "                if k_w < q_w:\n",
    "                    f.write('%d,%d\\n' % (1 + i, 0))\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "\n",
    "\n",
    "\n",
    "                q,k = torch.LongTensor(q), torch.LongTensor(k)\n",
    "                q_f,k_f = torch.FloatTensor(q_f), torch.FloatTensor(k_f)\n",
    "                q_w,k_w = torch.FloatTensor(q_w), torch.FloatTensor(k_w)\n",
    "\n",
    "                output = model(q.cuda(), k.cuda(), q_f.cuda(), k_f.cuda(), q_w.cuda(), k_w.cuda()).flatten().item()\n",
    "\n",
    "                out = 1 if output >= 0.5 else 0\n",
    "                f.write('%d,%d\\n' % (1 + i, out))\n",
    "                pbar.update(1)\n",
    "print c\n",
    "print 'done'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDDDone\n"
     ]
    }
   ],
   "source": [
    "print 'DDDDone'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find best threshold, test cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((150,), (150,), (128,), (128,), (1,), (1,))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 880464/880464 [00:03<00:00, 263946.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((150,), (150,), (128,), (128,), (1,), (1,))\n",
      "((11058, 150), (11058, 150), (11058, 128), (11058, 128), (11058, 1), (11058, 1))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def positive_bootsrap_generator(edges, xml_id_map, node_emb_dict):\n",
    "    num_edge = len(edges)\n",
    "        \n",
    "    while True:\n",
    "        for idx in np.random.permutation(num_edge):\n",
    "            src, dst = edges[idx, :]\n",
    "            Q, Q_w = xml_id_map[dst]\n",
    "            K, K_w = xml_id_map[src]\n",
    "            Q_fea = node_emb_dict[dst]\n",
    "            K_fea = node_emb_dict[src]\n",
    "            yield Q, K, Q_fea, K_fea, Q_w, K_w\n",
    "def negative_bootsrap_generator(adj_mat, links, idx_map, xml_id_map, training_node_list, node_emb_dict, neighbor_link_rate=0.8):\n",
    "    \n",
    "    \n",
    "    exist_node_list = xml_id_map.keys()\n",
    "    exist_N = len(training_node_list)\n",
    "    N = adj_mat.shape[0]\n",
    "    \n",
    "#     adj mat\n",
    "    links = np.array(list(map(idx_map.get, links.flatten())),\n",
    "                     dtype=np.int32).reshape(links.shape)\n",
    "    \n",
    "    adj_sp = sp.coo_matrix((np.ones(links.shape[0]), (links[:, 0], links[:, 1])),\n",
    "                        shape=(N, N),\n",
    "                        dtype=np.uint8)\n",
    "    adj_sp_2 = (sp.coo_matrix.dot(adj_sp,adj_sp) + adj_sp).tocoo()\n",
    "    \n",
    "    rev_map = {v:k for k,v in idx_map.items()}\n",
    "    adj_map = {i:[] for i in range(N)}\n",
    "    with tqdm(total=len(adj_sp_2.row)) as pbar:\n",
    "        for i,j,v in zip(adj_sp_2.row, adj_sp_2.col, adj_sp_2.data):\n",
    "            if adj_mat[i, j] != 1 and v == 1:\n",
    "                adj_map[i].append(j)\n",
    "            pbar.update(1)\n",
    "#             print i,N\n",
    "                \n",
    "    while True:\n",
    "        src = training_node_list[np.random.randint(exist_N)]\n",
    "        \n",
    "#         choose neighbor link\n",
    "        if np.random.rand(1) <= neighbor_link_rate:\n",
    "        \n",
    "            i = idx_map[src]\n",
    "            high = len(adj_map[i])\n",
    "            while high == 0:\n",
    "                src = training_node_list[np.random.randint(exist_N)]\n",
    "                i = idx_map[src]\n",
    "                high = len(adj_map[i])\n",
    "                \n",
    "            idx = np.random.randint(high)\n",
    "            dst = adj_map[i][idx]\n",
    "            dst = rev_map[dst]\n",
    "        else:\n",
    "            dst = training_node_list[np.random.randint(exist_N)]\n",
    "            while adj_mat[idx_map[src], idx_map[dst]] == 1:\n",
    "                dst = training_node_list[np.random.randint(exist_N)]\n",
    "        Q, Q_w = xml_id_map[dst]\n",
    "        K, K_w = xml_id_map[src]\n",
    "        Q_fea = node_emb_dict[dst]\n",
    "        K_fea = node_emb_dict[src]\n",
    "        yield Q, K, Q_fea, K_fea, Q_w, K_w\n",
    "\n",
    "def val_data(edges, xml_id_map):\n",
    "    Q, K = [],[]\n",
    "    Q_f, K_f = [],[]\n",
    "    Q_w, K_w = [],[]\n",
    "    \n",
    "    for idx in range(edges.shape[0]):\n",
    "        src, dst = edges[idx, :]\n",
    "        q, q_w = xml_id_map[dst]\n",
    "        k, k_w = xml_id_map[src]\n",
    "        q_fea = node_emb_dict[dst]\n",
    "        k_fea = node_emb_dict[src]\n",
    "        \n",
    "        Q.append(q)\n",
    "        K.append(k)\n",
    "        Q_f.append(q_fea)\n",
    "        K_f.append(k_fea)\n",
    "        Q_w.append(q_w)\n",
    "        K_w.append(k_w)\n",
    "        \n",
    "        \n",
    "    Q = np.vstack(Q)\n",
    "    K = np.vstack(K)\n",
    "    Q_fea = np.vstack(Q_f)\n",
    "    K_fea = np.vstack(K_f)\n",
    "    Q_w = np.vstack(Q_w)\n",
    "    K_w = np.vstack(K_w)\n",
    "    \n",
    "    return Q, K, Q_fea, K_fea, Q_w, K_w\n",
    "    \n",
    "N = links.shape[0]\n",
    "idx = np.random.permutation(N)\n",
    "train_idx = idx[N//10:]\n",
    "val_idx = idx[:N//10]\n",
    "\n",
    "pos_G = positive_bootsrap_generator(links[train_idx,:], xml_id_map, node_emb_dict)\n",
    "training_node_list = list(set(links[train_idx,:].flatten().tolist()))\n",
    "neg_G = negative_bootsrap_generator(adj_mat, links, idx_map, xml_id_map, training_node_list, node_emb_dict)\n",
    "val_Q, val_K, val_Q_fea, val_K_fea, val_Q_w, val_K_w = val_data(links[val_idx,:], xml_id_map)\n",
    "q,k,q_f,k_f,q_w,k_w = next(pos_G)\n",
    "print(q.shape,k.shape, q_f.shape, k_f.shape, q_w.shape, k_w.shape)\n",
    "\n",
    "q,k,q_f,k_f,q_w,k_w = next(neg_G)\n",
    "print(q.shape,k.shape, q_f.shape, k_f.shape, q_w.shape, k_w.shape)\n",
    "print(val_Q.shape,val_K.shape, val_Q_fea.shape, val_K_fea.shape, val_Q_w.shape, val_K_w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 11208/209527 [00:00<00:01, 112063.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209527, 128) (209527, 128) (209527,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 209527/209527 [00:01<00:00, 108935.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6151522238184101 0.4040404040404041\n",
      "0.37809783\n",
      "0.5267053888043068 Y 0.7105766798551022\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cos_d(x,y):\n",
    "    assert x.shape==y.shape==(128,)\n",
    "    return np.dot(x,y)/(np.linalg.norm(x, ord=2) * np.linalg.norm(y, ord=2))\n",
    "\n",
    "links = np.genfromtxt('./kaggle/t3-fake.txt', dtype=np.int32)\n",
    "\n",
    "N = links.shape[0]\n",
    "print N\n",
    "Q = []\n",
    "K = []\n",
    "Y = []\n",
    "for i in range(N):\n",
    "    q,k,q_f,k_f,q_w,k_w = next(pos_G)\n",
    "    if q_w > k_w:\n",
    "        continue\n",
    "    Q.append(q_f)\n",
    "    K.append(k_f)\n",
    "    Y.append(1)\n",
    "    \n",
    "    q,k,q_f,k_f,q_w,k_w = next(neg_G)\n",
    "    if q_w > k_w:\n",
    "        continue\n",
    "    Q.append(q_f)\n",
    "    K.append(k_f)\n",
    "    Y.append(0)\n",
    "    \n",
    "Q = np.vstack(Q)\n",
    "K = np.vstack(K)\n",
    "Y = np.array(Y)\n",
    "\n",
    "\n",
    "print Q.shape, K.shape, Y.shape\n",
    "\n",
    "    \n",
    "def cos_d(x,y):\n",
    "    assert x.shape==y.shape and len(x.shape)==1\n",
    "    return np.dot(x,y)/(np.linalg.norm(x, ord=2) * np.linalg.norm(y, ord=2))\n",
    "\n",
    "def foo(Q, K, Y):\n",
    "    N,dim = Q.shape\n",
    "    out = []\n",
    "    \n",
    "    with tqdm(total=N) as pbar:\n",
    "        for i in range(N):\n",
    "            q,k = Q[i,:], K[i,:]\n",
    "            d = cos_d(q, k)\n",
    "            out.append(d)\n",
    "            pbar.update(1)\n",
    "    out = np.array(out)\n",
    "    label = Y.flatten().astype(np.bool)\n",
    "    best_acc = best_threshold = 0\n",
    "    for threshold in np.linspace(0,1,100):\n",
    "        buf = out >= threshold\n",
    "        acc = np.mean(buf == label)\n",
    "#         print '%.2f%%' % acc, threshold\n",
    "    \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_threshold = threshold\n",
    "    return best_acc, best_threshold, out\n",
    "\n",
    "\n",
    "acc, threshold, out = foo(Q,K,Y)\n",
    "print acc, threshold\n",
    "print np.sort(out)[len(out)//4]\n",
    "print np.mean(Y),'Y', np.mean(out>=threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74798/74798 [00:01<00:00, 50817.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 0.10827829621112864 101 74798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# random\n",
    "import torch \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "edges_unordered = np.genfromtxt('./kaggle/t3-test.txt', dtype=np.int32)\n",
    "# links = np.genfromtxt('./kaggle/t3-train.txt', dtype=np.int32)\n",
    "# training_node_set = set(links.flatten().tolist())\n",
    "Y = []\n",
    "c = 0\n",
    "with open('guess.csv', 'w') as g:\n",
    "    with tqdm(total=edges_unordered.shape[0]) as pbar:\n",
    "\n",
    "        g.write('query_id,prediction\\n')\n",
    "        for i in range(edges_unordered.shape[0]):\n",
    "\n",
    "            src, dst = edges_unordered[i, :]\n",
    "            src_v, src_w = xml_id_map[src]\n",
    "            dst_v, dst_w = xml_id_map[dst]\n",
    "            src_f = node_emb_dict[src]\n",
    "            dst_f = node_emb_dict[dst]\n",
    "            \n",
    "            \n",
    "#             if src_w < dst_w or abs(abs(src_w-dst_w) - 95) > 87*2:\n",
    "            if src_w < dst_w :\n",
    "#             if False:\n",
    "                g.write('%d,%d\\n' % (1 + i, 0))\n",
    "                Y.append(0)\n",
    "                c +=1\n",
    "            else:\n",
    "                out = 1 if cos_d(src_f,dst_f) > 0.39393939393939 else 0\n",
    "                g.write('%d,%d\\n' % (1 + i, out))\n",
    "                Y.append(out)\n",
    "#                 if src not in training_node_set and dst not in training_node_set:\n",
    "#                     g.write('%d,%d\\n' % (1 + i, 0))\n",
    "            pbar.update(1)\n",
    "print 'done', np.mean(Y), c, len(Y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
