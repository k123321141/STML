{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681494/2681494 [00:20<00:00, 130704.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 5532)\n",
      "(5532,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as  np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "np.random.seed(1337)\n",
    "with open('./kaggle/rating_train.csv', 'r') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "u_map = {}\n",
    "\n",
    "dates = []\n",
    "foods = []\n",
    "users = []\n",
    "    \n",
    "\n",
    "\n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        date_str, user, food = l.strip().split(',')\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        user, food = int(user), int(food)\n",
    "        if user not in u_map:\n",
    "            u_map[user] = []\n",
    "        u_map[user].append( (date, food) )\n",
    "        \n",
    "        dates.append(date)\n",
    "        users.append(user)\n",
    "        foods.append(food)\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "user_map = {u:i for i, u in enumerate(set(users))}        \n",
    "food_map = {f:i for i, f in enumerate(set(foods))}\n",
    "\n",
    "\n",
    "# for ranking sparse matrix\n",
    "rows = [user_map[u] for u in users]\n",
    "cols = [food_map[f] for f in foods]\n",
    "R = csr_matrix((np.ones([len(rows), ]), (rows, cols)), shape=(len(user_map), len(food_map)))\n",
    "weight =  np.array(1./ np.sum(R, axis=0)).flatten()\n",
    "print R.shape\n",
    "print weight.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 165, 1000])\n",
      "tensor(0.3374)\n",
      "tensor(0.3374)\n",
      "tensor(0.3374)\n"
     ]
    }
   ],
   "source": [
    "bat = 7\n",
    "seq = 165\n",
    "m = 1000\n",
    "a = torch.rand([bat, seq, m])\n",
    "a = torch.sigmoid(a)\n",
    "\n",
    "l = torch.rand([bat, seq, m])\n",
    "l = torch.sigmoid(l)\n",
    "\n",
    "# w = torch.ones([seq, m])\n",
    "w = torch.rand([m,])\n",
    "c1 = nn.BCELoss(weight=w, reduction='mean')\n",
    "c2 = nn.BCELoss(reduction='none')\n",
    "c3 = nn.BCELoss(weight=w, reduction='none')\n",
    "\n",
    "l1 = c1(a,l)\n",
    "l2 = c2(a,l)\n",
    "l3 = c3(a,l)\n",
    "print l2.shape\n",
    "m = w.unsqueeze(0).unsqueeze(0).repeat(bat, seq,1)\n",
    "l2 = m*l2\n",
    "l3 = torch.mean(l3)\n",
    "\n",
    "# print l\n",
    "print l1\n",
    "print torch.mean(l2)\n",
    "print l3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 23 95.21165644171779 34.48409857182605\n",
      "5569 194 1028.1802147239264 599.6642539977294\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for k,v in u_map.items():\n",
    "    ds = [d for d,f in v]\n",
    "    \n",
    "    set_ds = set(ds)\n",
    "    a.append(len(set_ds))\n",
    "    b.append(len(ds))\n",
    "print max(a), min(a),np.mean(a), np.std(a)\n",
    "print max(b), min(b),np.mean(b), np.std(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from constants import MAX_TEXT_SEQ_LEN, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "csv = pd.read_csv('./kaggle/user.csv')\n",
    "print csv.columns\n",
    "# print 'userid,username,age,gender,location,city,state,title,about_me,reasons,inspirations,friends_count'\n",
    "# for csv.iterrows\n",
    "texts = []\n",
    "print 'Starting read texts.'\n",
    "def append_text(texts, value):\n",
    "    if not pd.isnull(value):\n",
    "        texts.append(value)\n",
    "for row in csv.iterrows():\n",
    "    r = row[1]\n",
    "    append_text(texts, r['about_me'])\n",
    "    append_text(texts, r['reasons'])\n",
    "    append_text(texts, r['inspirations'])\n",
    "\n",
    "n = csv['about_me'][2580]\n",
    "print np.isnan(n)\n",
    "\n",
    "buf = [len(s) for s in texts]\n",
    "print np.max(buf), np.mean(buf), np.std(buf)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_TEXT_SEQ_LEN, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=MAX_TEXT_SEQ_LEN)\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# embeddings_index = {}\n",
    "# # with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r', encoding='utf8') as f:\n",
    "# with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r') as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         coefs = np.asarray(values[1:], dtype='float32')\n",
    "#         embeddings_index[word] = coefs\n",
    "# # prepare embedding matrix\n",
    "# num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "# embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "# for word, i in word_index.items():\n",
    "#     if i > MAX_NUM_WORDS:\n",
    "#         continue\n",
    "#     embedding_vector = embeddings_index.get(word)\n",
    "#     if embedding_vector is not None:\n",
    "#         # words not found in embedding index will be all-zeros.\n",
    "#         embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "n = csv['about_me'][2580]\n",
    "print pd.isnull(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare sample generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 165, 5532) (32, 165, 5532)\n",
      "(16, 165, 5532) (16, 165, 5532)\n",
      "(165, 5532)\n"
     ]
    }
   ],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "\n",
    "idx = np.random.permutation(len(u_map))\n",
    "val_num = len(u_map)//10\n",
    "train_idx, val_idx = idx[val_num:], idx[:val_num]\n",
    "train_u_map = {k:u_map[k] for k in u_map.keys()[val_num:]}\n",
    "val_u_map = {k:u_map[k] for k in u_map.keys()[:val_num]}\n",
    "def batch_boostrap_generator(batch_size, u_map, food_map, max_history_len):\n",
    "    G = boostrap_generator(u_map, food_map, max_history_len)\n",
    "    while True:\n",
    "        X = []\n",
    "        pad_masks = []\n",
    "        for i in range(batch_size):\n",
    "            x, x_len = next(G)\n",
    "            X.append(np.expand_dims(x, axis=0))\n",
    "            pad_mask = np.ones_like(x)\n",
    "            pad_mask[x_len:, :] = 0\n",
    "            pad_masks.append(np.expand_dims(pad_mask, axis=0))\n",
    "        yield np.vstack(X), np.vstack(pad_masks)\n",
    "def boostrap_generator(u_map, food_map, max_history_len):\n",
    "    while True:\n",
    "        keys = u_map.keys()\n",
    "        for user_idx in np.random.permutation(len(u_map)):\n",
    "            user = keys[user_idx]\n",
    "            X = np.zeros([max_history_len, len(food_map)])\n",
    "            history = u_map[user]\n",
    "            ds = np.array([d for d,f in history])\n",
    "            fs = np.array([f for d,f in history])\n",
    "            sorted_idx = np.argsort(ds)\n",
    "            ds = ds[sorted_idx]\n",
    "            fd = fs[sorted_idx]\n",
    "            \n",
    "            date_idx = 0\n",
    "            now_date = ds[0]\n",
    "            for food, date in zip(fs,ds):\n",
    "                if date != now_date:\n",
    "                    date_idx+=1\n",
    "                    now_date = date\n",
    "                X[date_idx, food_map[food]] = 1\n",
    "            x_len = date_idx+1\n",
    "            yield X, x_len\n",
    "            \n",
    "    \n",
    "\n",
    "G = batch_boostrap_generator(32, train_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "val_G = batch_boostrap_generator(32//2, val_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "\n",
    "x, pad_mask = next(G)\n",
    "print x.shape, pad_mask.shape\n",
    "x, pad_mask = next(val_G)\n",
    "print x.shape, pad_mask.shape\n",
    "\n",
    "G2 = boostrap_generator(train_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "x, x_len = next(G2)\n",
    "print x.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer model with one-for-all BCE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Decoder',\n",
       " 'Encoder',\n",
       " 'F',\n",
       " 'Multi_Head_attention_layer',\n",
       " 'PositionwiseFeedForward',\n",
       " 'Stack_Decoder',\n",
       " 'Stack_Encoder',\n",
       " 'Transformer',\n",
       " '__builtins__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " 'nn',\n",
       " 'positional_encoding',\n",
       " 'scaled_dot_attention',\n",
       " 'torch',\n",
       " 'xavier_normal_']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import imp\n",
    "# from os.path import expanduser\n",
    "# home = expanduser(\"~\")\n",
    "# Transformer = imp.load_source('Transformer', '%s/git/grandchallenge/ASR/Transformer.py' % home)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 200 \n",
      "torch.Size([3, 13, 128])\n",
      "torch.Size([3, 47, 128])\n",
      "1189632\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    assert len(Q.size()) == 3 and len(K.size()) == 3 and len(V.size()) == 3\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).cuda()\n",
    "    out = torch.matmul(Q,K.permute(0,2,1)) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out.masked_fill_(mask, -float('inf'))\n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "\n",
    "def positional_encoding(d_model, pos):\n",
    "    assert d_model % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,d_model], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "                            \n",
    "class Transformer_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask, use_cuda=True, posi_cache_length=200):\n",
    "        super(Transformer_v2, self).__init__()\n",
    "#         for construct cache positional encoding matrix.\n",
    "        self.d_model = dm\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask)\n",
    "        self.emb_drop = nn.Dropout(p_drop)\n",
    "        self.init_pos_mat(posi_cache_length)\n",
    "\n",
    "    def forward(self, Q):\n",
    "    \n",
    "        \n",
    "#         decoder\n",
    "        batch, Q_len, d = Q.size()\n",
    "        \n",
    "        try:\n",
    "            Q = Q + self.get_pos_mat(Q_len)\n",
    "        except RuntimeError, e:\n",
    "            if e.message == 'TensorIterator expected type torch.cuda.FloatTensor but got torch.FloatTensor':\n",
    "                if Q.is_cuda != self.get_pos_mat(K_len).is_cuda:\n",
    "                    print('Make sure cache positional matrix is same type of tensor with input, both cuda tensor or not.\\nBy setting argument use_cuda=True to set cache positional encoding matrix as a cuda tensor.')\n",
    "            raise\n",
    "        \n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q)\n",
    "        return de_out\n",
    "    \n",
    "#     To speed up the positional encoding by construct an cache matrix. \n",
    "    def init_pos_mat(self, cache_length):\n",
    "        print('init postional matrix with length : %d ' % cache_length)\n",
    "        self.positional_matrix = torch.cat([positional_encoding(self.d_model, i) for i in range(0,cache_length)], dim=0)\n",
    "        self.positional_matrix.requires_grad = False\n",
    "        if self.use_cuda:\n",
    "            self.positional_matrix = self.positional_matrix.cuda()\n",
    "            \n",
    "        \n",
    "    def get_pos_mat(self, length):\n",
    "        if length > self.positional_matrix.shape[0]:\n",
    "            print('input sequence length reach positional matrix maximum length. %d ' % length)\n",
    "            ret = torch.cat([positional_encoding(self.d_model, i) for i in range(length)], dim=0)\n",
    "            ret.requires_grad = False\n",
    "            print('Increase positional matrix maximum length. %d ' % length)\n",
    "            self.positional_matrix = ret\n",
    "            if self.use_cuda:\n",
    "                self.positional_matrix = self.positional_matrix.cuda()\n",
    "            return ret\n",
    "        else:\n",
    "            return self.positional_matrix[:length]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h, p_drop, d_ff, use_mask) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.Q_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.Q_att_drop = nn.Dropout(p_drop)\n",
    "\n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(dm, d_ff)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.linear_drop = nn.Dropout(p_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, Q):\n",
    "        if self.use_mask:\n",
    "            batch, Q_len, d = Q.size()\n",
    "            mask = self.mask_matrix(batch, Q_len)\n",
    "        else:\n",
    "            mask = None\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(Q, Q, Q, mask=mask)\n",
    "        Q_attention_out = self.Q_att_drop(Q_attention_out)\n",
    "        Q_att_out = self.Q_attention_norm_lay(Q + Q_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(Q_att_out)\n",
    "        out = self.ff_norm_lay(Q_att_out + linear_out)\n",
    "        return out\n",
    "    def mask_matrix(self, batch, Q_len):\n",
    "#         ByteTensor\n",
    "        mask = torch.zeros([1, Q_len, Q_len], dtype=torch.uint8, requires_grad=False)\n",
    "        for i in range(Q_len):\n",
    "            mask[0,i,i+1:] = 1\n",
    "        return mask.repeat(batch,1, 1).cuda()\n",
    "\n",
    "\n",
    "class Multi_Head_attention_layer(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Multi_Head_attention_layer, self).__init__()\n",
    "        self.Q_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.K_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.V_linears = nn.ModuleList([nn.Linear(dm, dv) for i in range(h)])\n",
    "        self.output_linear = nn.Linear(h*dv, dm)\n",
    "                            \n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask):\n",
    "        buf = []\n",
    "        for Q_linear, K_linear, V_linear in zip(self.Q_linears, self.K_linears, self.V_linears):\n",
    "            Q = Q_linear(Q_input)\n",
    "            K = K_linear(K_input)\n",
    "            V = V_linear(V_input)\n",
    "            buf.append(scaled_dot_attention(Q, K, V, mask))\n",
    "        \n",
    "        buf = torch.cat(buf,dim=-1)\n",
    "        out = self.output_linear(buf)\n",
    "        \n",
    "        return out      \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        self.cnn2 = nn.Conv1d(d_ff, d_model, 1)\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        bat,seq_len,d = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        return x      \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Transformer paper baseline hyper-parameters\n",
    "STACKED_NUM = 6\n",
    "H = 8\n",
    "D_MODEL = 128\n",
    "DK = DV = D_MODEL//H\n",
    "P_DROP = 0.1\n",
    "D_FF = D_MODEL*4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "bat = 3\n",
    "Q = torch.rand([bat, 13, D_MODEL]).cuda()\n",
    "model = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "o = model(Q)\n",
    "print(o.size())\n",
    "\n",
    "Q = torch.rand([bat, 47, D_MODEL]).cuda()\n",
    "o = model(Q)\n",
    "print(o.size())\n",
    "# # print o\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 200 \n",
      "torch.Size([7, 18, 5532])\n",
      "2627996\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F### Transformer with ALS embedding Training\n",
    "# import Transformer/\n",
    "\n",
    "import numpy as np\n",
    "from constants import FOOD_NUM, USER_NUM\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, p_drop):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.food_emb = Food_embedding(FOOD_NUM, dm, 2, p_drop)\n",
    "        self.transformer = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "\n",
    "        self.output_linear = nn.Linear(dm, FOOD_NUM)\n",
    "\n",
    "    def forward(self, history):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        \n",
    "        x = self.food_emb(history)\n",
    "        batch, x_len, d = x.size()\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = self.output_linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "class Food_embedding(nn.Module):\n",
    "    def __init__(self, c_in, dm, layer_num, p_drop, activation_fn=F.selu):\n",
    "        super(Food_embedding, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        assert layer_num >= 1\n",
    "        self.first_linear = nn.Linear(c_in, dm)\n",
    "        self.linears = nn.ModuleList([nn.Linear(dm, dm) for i in range(layer_num-1)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        x = self.first_linear(x)\n",
    "        for lay in self.linears:\n",
    "            x = self.activation_fn(lay(x))\n",
    "            if lay != self.linears[-1]:\n",
    "                x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "batch = 7\n",
    "dm = D_MODEL\n",
    "Q = torch.rand([batch, 18, FOOD_NUM]).cuda()\n",
    "model = Net(dm, 0.1).cuda()\n",
    "o = model(Q)\n",
    "# print t\n",
    "print(o.size())\n",
    "# print o\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 160/100000000 [00:08<1472:20:09, 18.87it/s, acc : 0.467, val_acc : 0.447, loss : 8.918, val_loss : 8.290] /home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Food_embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Transformer_v2. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Stack_Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Multi_Head_attention_layer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "  0%|          | 32800/100000000 [20:26<981:28:05, 28.29it/s, acc : 0.035, val_acc : 0.035, loss : 0.035, val_loss : 0.033] "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, acc, val_loss, val_acc, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f\\n' % (n_iter, loss, acc, val_loss, val_acc)\n",
    "    log_file_stream.write(log_text)\n",
    "    if n_iter % 10 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "def normal_acc(pred, label):\n",
    "    mask = label.type(torch.uint8)\n",
    "    buf = pred == mask\n",
    "    buf = buf.masked_select(mask)\n",
    "    \n",
    "    acc = torch.sum(buf).item() / float(torch.sum(mask).item())\n",
    "    return acc\n",
    "def rev_mask(m):\n",
    "    out = torch.ones_like(m, dtype=torch.uint8, requires_grad=False)\n",
    "    out.masked_fill_(m, 0)\n",
    "    return out\n",
    "        \n",
    "acc_q = deque(maxlen=10000)\n",
    "loss_q = deque(maxlen=10000)\n",
    "val_acc_q = deque(maxlen=10000)\n",
    "val_loss_q = deque(maxlen=10000)\n",
    "t = time.time()\n",
    "best_acc  = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "G = batch_boostrap_generator(batch_size, train_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "val_G = batch_boostrap_generator(batch_size, val_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "criterion = nn.BCELoss(weight=torch.FloatTensor(weight).cuda(), reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "print 'start training.'\n",
    "with open('log.txt', 'a') as f:\n",
    "    with open('best.txt', 'w') as best_log:\n",
    "        iters = 100000000\n",
    "        with tqdm(total=iters) as pbar:\n",
    "            for it in range(iters):\n",
    "                optimizer.zero_grad()\n",
    "                model.train()\n",
    "                seq, pad_mask = next(G)\n",
    "                seq = torch.FloatTensor(seq).cuda()\n",
    "                pad_mask = torch.FloatTensor(pad_mask).cuda()\n",
    "                seq.requires_grad_(False)\n",
    "                pad_mask.requires_grad_(False)\n",
    "\n",
    "                x = seq[:,:-1,:]\n",
    "                y = seq[:,1:,:]\n",
    "\n",
    "                output = model(x)\n",
    "#                     a,b = foo(output, y)\n",
    "#                     loss = normal_loss(criterion, output, y)\n",
    "\n",
    "                loss = torch.sum(criterion(output, y) * pad_mask[:,1:, :]) / torch.sum(pad_mask[:,1:,:])\n",
    "                pred = output > 0.5\n",
    "                label = y\n",
    "\n",
    "\n",
    "                acc = normal_acc(pred, label)\n",
    "                acc_q.append(acc)\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    seq, pad_mask = next(val_G)\n",
    "                    seq = torch.FloatTensor(seq).cuda()\n",
    "                    pad_mask = torch.FloatTensor(pad_mask).cuda()\n",
    "                    seq.requires_grad_(False)\n",
    "                    pad_mask.requires_grad_(False)\n",
    "\n",
    "                    x = seq[:,:-1,:]\n",
    "                    y = seq[:,1:,:]\n",
    "                    output = model(x)\n",
    "#                     c,d = foo(output, y)\n",
    "#                         val_loss = normal_loss(criterion, output, y)\n",
    "#                         val_loss = criterion(output, y)\n",
    "#                         pred = output > 0.5\n",
    "                    val_loss = torch.sum(criterion(output, y)* pad_mask[:,1:, :]) / torch.sum(pad_mask[:,1:,:])\n",
    "    \n",
    "                    pred = output > 0.5\n",
    "\n",
    "                    label = y\n",
    "\n",
    "                    val_acc = normal_acc(pred, label)\n",
    "                    val_acc_q.append(val_acc)\n",
    "                acc = np.mean(acc_q)\n",
    "                val_acc = np.mean(val_acc_q)\n",
    "\n",
    "#                     pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f \\t %.3f, %.3f, %.3f, %.3f' % (acc, val_acc, loss.item(), val_loss.item(), a,b,c,d), refresh=False)\n",
    "                pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f' % (acc, val_acc, loss.item()*1000., val_loss.item()*1000.), refresh=False)\n",
    "                pbar.update(batch_size)\n",
    "                dump_log(model, (it+1)*batch_size, loss, acc, val_loss, val_acc, f, './tmp.pt')\n",
    "                if val_acc > best_acc and it > 100:\n",
    "                    torch.save(model, './best.pt')\n",
    "                    best_acc = val_acc\n",
    "                    best_log.write('%d\\t%.5f\\n' % ((it+1)*batch_size, best_acc))\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14427456 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 320/14427456 [00:05<70:08:44, 57.13it/s, acc : 0.000, val_acc : 0.000, loss : 2622.345, val_loss : 2432.507 \t 0.017, 0.084, -0.013, -0.001] /home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Food_embedding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "  4%|▎         | 507136/14427456 [2:46:27<115:52:09, 33.37it/s, acc : 0.401, val_acc : 0.154, loss : 1218.665, val_loss : 2220.557 \t 0.063, 0.005, 0.206, 0.015]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5b45ad6d307b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;31m#                 print 'x1', np.sum(x[0,:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-22309915df5d>\u001b[0m in \u001b[0;36mbatch_boostrap_generator\u001b[0;34m(batch_size, u_map, food_map, Y_map, max_history_len, flip)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_history_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-22309915df5d>\u001b[0m in \u001b[0;36mboostrap_generator\u001b[0;34m(u_map, food_map, Y_map, max_history_len, flip)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mdate_idx\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mnow_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdate_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfood_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfood\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, acc, val_loss, val_acc, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f\\n' % (n_iter, loss, acc, val_loss, val_acc)\n",
    "    log_file_stream.write(log_text)\n",
    "    if n_iter % 10 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "def normal_acc(pred, label):\n",
    "    mask = label.type(torch.uint8)\n",
    "    buf = pred == mask\n",
    "    buf = buf.masked_select(mask)\n",
    "    \n",
    "    acc = torch.sum(buf).item() / float(torch.sum(mask).item())\n",
    "    return acc\n",
    "def rev_mask(m):\n",
    "    out = torch.ones_like(m, dtype=torch.uint8, requires_grad=False)\n",
    "    out.masked_fill_(m, 0)\n",
    "    return out\n",
    "def normal_loss(criterion, output, y):\n",
    "    loss = criterion(output*10., y*10.)\n",
    "    label = y.type(torch.uint8)\n",
    "    \n",
    "    \n",
    "    dim = label.shape[-1]\n",
    "    sum_1s = torch.sum(label)\n",
    "    loss_mask = y.clone()*(dim-sum_1s)\n",
    "\n",
    "    loss_mask.masked_fill_(rev_mask(label), sum_1s)\n",
    "    return torch.mean(loss_mask*loss)\n",
    "def foo(output, y):\n",
    "    output = output[0,:].flatten()\n",
    "    y = y[0,:].flatten()\n",
    "    pos_i = neg_i = -1\n",
    "    for i in np.random.permutation(len(y)):\n",
    "        if y[i] == 1 and pos_i == -1:\n",
    "            pos_i = i\n",
    "        if y[i] == 0 and neg_i == -1:\n",
    "            neg_i = i\n",
    "        if pos_i != -1 and neg_i != -1:\n",
    "            break\n",
    "    return output[pos_i], output[neg_i]\n",
    "        \n",
    "acc_q = deque(maxlen=1000)\n",
    "loss_q = deque(maxlen=1000)\n",
    "val_acc_q = deque(maxlen=1000)\n",
    "val_loss_q = deque(maxlen=1000)\n",
    "t = time.time()\n",
    "best_acc  = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "G = batch_boostrap_generator(batch_size, train_u_map, food_map, Y_map, max_history_len=MAX_SEQ_LEN, flip=False)\n",
    "val_G = batch_boostrap_generator(batch_size, val_u_map, food_map, Y_map, max_history_len=MAX_SEQ_LEN, flip=False)\n",
    "criterion = nn.MSELoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "print 'start training.'\n",
    "with open('log.txt', 'a') as f:\n",
    "    with open('best.txt', 'w') as best_log:\n",
    "        for e in range(epochs):\n",
    "            iters = len(food_map) * len(user_map)\n",
    "            with tqdm(total=iters) as pbar:\n",
    "                for it in range(iters):\n",
    "                    optimizer.zero_grad()\n",
    "                    model.train()\n",
    "                    x, y = next(G)\n",
    "    #                 print 'x1', np.sum(x[0,:])\n",
    "                    x = torch.FloatTensor(x).cuda()\n",
    "                    y = torch.FloatTensor(y).cuda()\n",
    "                    x.requires_grad_(False)\n",
    "                    y.requires_grad_(False)\n",
    "\n",
    "                    output = model(x)\n",
    "                    a,b = foo(output, y)\n",
    "                    loss = normal_loss(criterion, output, y)\n",
    "                    pred = output > 0.5\n",
    "                    label = y\n",
    "                    \n",
    "\n",
    "                    acc = normal_acc(pred, label)\n",
    "                    acc_q.append(acc)\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        x, y = next(val_G)\n",
    "    #                     print 'x2', np.sum(x[0,:])\n",
    "                        x = torch.FloatTensor(x).cuda()\n",
    "                        y = torch.FloatTensor(y).cuda()\n",
    "                        x.requires_grad_(False)\n",
    "                        y.requires_grad_(False)\n",
    "                        output = model(x)\n",
    "                        c,d = foo(output, y)\n",
    "                    \n",
    "                        val_loss = normal_loss(criterion, output, y)\n",
    "                        pred = output > 0.5\n",
    "                        label = y\n",
    "\n",
    "                        val_acc = normal_acc(pred, label)\n",
    "                        val_acc_q.append(val_acc)\n",
    "                    acc = np.mean(acc_q)\n",
    "                    val_acc = np.mean(val_acc_q)\n",
    "\n",
    "                    pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f \\t %.3f, %.3f, %.3f, %.3f' % (acc, val_acc, loss.item(), val_loss.item(), a,b,c,d), refresh=False)\n",
    "                    pbar.update(batch_size)\n",
    "                    dump_log(model, (it+1)*batch_size, loss, acc, val_loss, val_acc, f, './tmp.pt')\n",
    "                    if val_acc > best_acc and it > 100:\n",
    "                        torch.save(model, './best.pt')\n",
    "                        best_acc = val_acc\n",
    "                        best_log.write('%d\\t%.5f\\n' % ((it+1)*batch_size, best_acc))\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 0, 0], dtype=torch.uint8)\n",
      "tensor([0, 0, 0, 1, 0, 1], dtype=torch.uint8)\n",
      "tensor([0., 0., 1., 0., 0., 1.])\n",
      "tensor([2., 2., 2., 4., 2., 4.])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([0,0,1,1,0,0])\n",
    "torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([0,0,1,1,0,0])\n",
    "b = torch.ByteTensor([0,0,0,1,0,1])\n",
    "a.requires_grad_(False)\n",
    "print a.clone().requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:21<00:00, 120.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "    with open('predict.csv', 'w') as f_out:\n",
    "        f_out.write('userid,foodid\\n')\n",
    "\n",
    "        with tqdm(total=len(u_map)) as pbar:\n",
    "            for user in u_map.keys():\n",
    "                x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "                history = u_map[user]\n",
    "                ds = np.array([d for d,f in history])\n",
    "                fs = np.array([f for d,f in history])\n",
    "                sorted_idx = np.argsort(ds)\n",
    "                ds = ds[sorted_idx]\n",
    "                fd = fs[sorted_idx]\n",
    "\n",
    "                date_idx = 0\n",
    "                now_date = ds[0]\n",
    "                for food, date in zip(fs,ds):\n",
    "                    if date != now_date:\n",
    "                        date_idx+=1\n",
    "                        now_date = date\n",
    "                    x[date_idx, food_map[food]] = 1\n",
    "                x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "                out = model(x)\n",
    "\n",
    "                arr = out.cpu().numpy()[0,-1,:]\n",
    "                k20 = reversed(np.argsort(arr)[-20:])\n",
    "                s = ''\n",
    "                for food_idx in k20:\n",
    "                    s += ' %d' % rev_food_map[food_idx]\n",
    "                f_out.write('%d,%s\\n' % (user, s) )\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "print 'done'\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "def batch_boostrap_generator(batch_size, u_map, food_map, Y_map, max_history_len, flip):\n",
    "    G = boostrap_generator(u_map, food_map, Y_map, max_history_len, flip)\n",
    "    while True:\n",
    "        X = []\n",
    "        pos_Y = []\n",
    "        neg_Y = []\n",
    "        for i in range(batch_size):\n",
    "            x, pos_y, neg_y = next(G)\n",
    "            x = np.pad(x, ((0,max_history_len-x.shape[0]),(0,0)), 'constant', constant_values=0)\n",
    "            X.append(np.expand_dims(x, axis=0))\n",
    "            pos_Y.append(np.expand_dims(pos_y, axis=0))\n",
    "            neg_Y.append(np.expand_dims(neg_y, axis=0))\n",
    "        yield np.vstack(X), np.vstack(pos_Y), np.vstack(neg_Y) \n",
    "\n",
    "        \n",
    "flip = True       \n",
    "model = torch.load('./best.pt')\n",
    "model.eval()\n",
    "batch_size = 256\n",
    "food_buf = torch.LongTensor(np.arange(len(food_map))).view(1,-1).repeat(batch_size, 1).cuda()\n",
    "answer_sheet = np.zeros([len(u_map), len(food_map)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(u_map)*len(food_map)) as pbar:\n",
    "        keys = u_map.keys()\n",
    "        ran = range((len(keys) // batch_size) +1) if len(keys) % batch_size != 0 else range(len(keys) // batch_size)\n",
    "        for i in ran:\n",
    "            a = i*batch_size\n",
    "            b = (i+1)*batch_size if (i+1)*batch_size < len(keys) else len(keys)\n",
    "            X = []\n",
    "            for user in keys[a:b]:\n",
    "                x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "                history = u_map[user]\n",
    "                ds = np.array([d for d,f in history])\n",
    "                fs = np.array([f for d,f in history])\n",
    "                sorted_idx = np.flip(np.argsort(ds), axis=-1) if flip else np.argsort(ds)\n",
    "                ds = ds[sorted_idx]\n",
    "                fd = fs[sorted_idx]\n",
    "\n",
    "                date_idx = 0\n",
    "                now_date = ds[0]\n",
    "                for food, date in zip(fs,ds):\n",
    "                    if date != now_date:\n",
    "                        date_idx+=1\n",
    "                        now_date = date\n",
    "                    x[date_idx, food_map[food]] = 1\n",
    "                X.append(np.expand_dims(x,axis=0))\n",
    "            \n",
    "            X = torch.FloatTensor(np.vstack(X)).cuda()\n",
    "            for food_idx  in range(len(food_map)):\n",
    "                target = food_buf[:b-a, food_idx:food_idx+1]\n",
    "                output = model(target, X)\n",
    "                answer_sheet[a:b, food_idx:food_idx+1] =  output.cpu()\n",
    "                pbar.update(batch_size)\n",
    "                \n",
    "np.save('./output_sheet', answer_sheet)\n",
    "print answer_sheet.shape\n",
    "print'Done'\n",
    "\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "k=20\n",
    "a = ''\n",
    "buf = []\n",
    "with open('predict.csv', 'w') as f:\n",
    "    f.write('userid,foodid\\n')\n",
    "    for i,user in enumerate(u_map.keys()):\n",
    "        s = ''\n",
    "        for food_idx in reversed(np.argsort(answer_sheet[i,:])[-k:]):\n",
    "            s += ' %d' % rev_food_map[food_idx]\n",
    "        f.write('%d,%s\\n' % (user, s) )\n",
    "        buf.append(a == s)\n",
    "        a = s\n",
    "print buf\n",
    "print 'done'\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "print 'V2'        \n",
    "flip = True       \n",
    "model = torch.load('./best_v2.pt')\n",
    "model.eval()\n",
    "batch_size = 256\n",
    "food_buf = torch.LongTensor(np.arange(len(food_map))).view(1,-1).repeat(batch_size, 1).cuda()\n",
    "answer_sheet = np.zeros([len(u_map), len(food_map)])\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(u_map)*len(food_map)) as pbar:\n",
    "        keys = u_map.keys()\n",
    "        ran = range((len(keys) // batch_size) +1) if len(keys) % batch_size != 0 else range(len(keys) // batch_size)\n",
    "        for i in ran:\n",
    "            a = i*batch_size\n",
    "            b = (i+1)*batch_size if (i+1)*batch_size < len(keys) else len(keys)\n",
    "            X = []\n",
    "            for user in keys[a:b]:\n",
    "                x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "                history = u_map[user]\n",
    "                ds = np.array([d for d,f in history])\n",
    "                fs = np.array([f for d,f in history])\n",
    "                sorted_idx = np.flip(np.argsort(ds), axis=-1) if flip else np.argsort(ds)\n",
    "                ds = ds[sorted_idx]\n",
    "                fd = fs[sorted_idx]\n",
    "\n",
    "                date_idx = 0\n",
    "                now_date = ds[0]\n",
    "                for food, date in zip(fs,ds):\n",
    "                    if date != now_date:\n",
    "                        date_idx+=1\n",
    "                        now_date = date\n",
    "                    x[date_idx, food_map[food]] = 1\n",
    "                X.append(np.expand_dims(x,axis=0))\n",
    "            \n",
    "            X = torch.FloatTensor(np.vstack(X)).cuda()\n",
    "            for food_idx  in range(len(food_map)):\n",
    "                target = food_buf[:b-a, food_idx:food_idx+1]\n",
    "                output = model(target, X)\n",
    "                answer_sheet[a:b, food_idx:food_idx+1] =  output.cpu()\n",
    "                pbar.update(batch_size)\n",
    "                \n",
    "np.save('./output_sheet2', answer_sheet)\n",
    "print answer_sheet.shape\n",
    "print'Done'\n",
    "\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "k=20\n",
    "a = ''\n",
    "buf = []\n",
    "with open('predict2.csv', 'w') as f:\n",
    "    f.write('userid,foodid\\n')\n",
    "    for i,user in enumerate(u_map.keys()):\n",
    "        s = ''\n",
    "        for food_idx in reversed(np.argsort(answer_sheet[i,:])[-k:]):\n",
    "            s += ' %d' % rev_food_map[food_idx]\n",
    "        f.write('%d,%s\\n' % (user, s) )\n",
    "        buf.append(a == s)\n",
    "        a = s\n",
    "print buf\n",
    "print 'done'\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "a = [b for b in reversed(np.argsort(answer_sheet[i,:])[-k:])]\n",
    "print a\n",
    "\n",
    "# i=1\n",
    "a = [b for b in reversed(np.argsort(answer_sheet[i,:])[-k:])]\n",
    "print a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print answer_sheet[0,:]\n",
    "print answer_sheet[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print acc_q\n",
    "# print output.shape\n",
    "# c = output < 0.5\n",
    "# print output < 0.5\n",
    "\n",
    "print pred.shape\n",
    "print label.shape\n",
    "print torch.sum(pred == label.type(torch.uint8)).item() / float(output.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[0.49]])\n",
    "b = torch.zeros_like(a)\n",
    "print a.shape\n",
    "print a > 0.5\n",
    "c = a > 0.5\n",
    "print b,c\n",
    "print b.type(torch.uint8) == c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salnj;kvahjk\n",
    "# print u_map[6]\n",
    "buf  = [] \n",
    "def ck(ds):\n",
    "    l = max(ds) - min(ds)\n",
    "    min_d = min(ds)\n",
    "    \n",
    "#     d_list = [(d-min_d) for d in ds]\n",
    "#     d_list = sorted(set(d_list))\n",
    "    d_list = sorted(set(ds))\n",
    "#     print l, len(d_list)\n",
    "    for i,d in enumerate(d_list):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        d_ = d_list[i-1]\n",
    "        if (d - d_).days.real != 1:\n",
    "            d_diff = (d - d_).days.real\n",
    "            buf.append(d_diff)\n",
    "#             print d_diff\n",
    "#             assert d_diff < 10\n",
    "def cf(ds):\n",
    "    l = max(ds) - min(ds)\n",
    "    min_d = min(ds)\n",
    "    \n",
    "#     d_list = [(d-min_d) for d in ds]\n",
    "#     d_list = sorted(set(d_list))\n",
    "    d_list = sorted(set(ds))\n",
    "    \n",
    "#     print l, len(d_list)\n",
    "    for i,d in enumerate(d_list):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        d_ = d_list[i-1]\n",
    "        if (d - d_).days.real != 1:\n",
    "            d_diff = (d - d_).days.real\n",
    "            buf.append(d_diff)\n",
    "#             print d_diff\n",
    "#             assert d_diff < 10\n",
    "                        \n",
    "    \n",
    "# for i in [6,8,12]:\n",
    "with tqdm(total=len(u_map)) as pbar:\n",
    "    buf = []\n",
    "    buf2 = []\n",
    "    for i in u_map.keys():\n",
    "        ds = [d for d,f in u_map[i]]\n",
    "        threshold = ((max(ds) - min(ds)).days.real * (4./5.))\n",
    "        min_d = min(ds)\n",
    "        fl = []\n",
    "        m = {}\n",
    "        for d,f in u_map[i]:\n",
    "            if (d - min_d).days.real > threshold:\n",
    "                fl.append(f)\n",
    "            if d not in m:\n",
    "                m[d] = []\n",
    "            m[d].append(f)\n",
    "        for k,v in m.items():    \n",
    "            buf2.append(len(set(v)))\n",
    "#         print len(u_map[i]), len(fl), len(set(fl))\n",
    "        buf.append(len(set(fl)))\n",
    "        #     print i, min(ds), max(ds), len(ds)\n",
    "#         ck(ds)\n",
    "        pbar.update(1)\n",
    "# d6 = [d for d,f in u_map[6]]\n",
    "# ck(d6)\n",
    "    \n",
    "#     print ck(ds)\n",
    "# print u_map[33]\n",
    "print np.mean(buf), np.std(buf)\n",
    "print np.mean(buf2), np.std(buf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print min(dates), max(dates),  max(dates)- min(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print min(dates)\n",
    "m = min(dates)\n",
    "print max(dates)\n",
    "a = dates[0]\n",
    "print dir(a - min(dates))\n",
    "print a\n",
    "# def normalize_date(min_date, max_date, date):\n",
    "# for k in u_map.keys()[:10]:\n",
    "#     print len(u_map[k])\n",
    "ds = [(d-m).total_seconds() / (60*60*24) for d in dates]\n",
    "# import numpy as np\n",
    "print np.mean(ds)\n",
    "print np.min(ds), np.max(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from os.path import join\n",
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from constants import MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "def quote_title_abstract(xml_path):\n",
    "    with open(xml_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    soup = BS(data)\n",
    "    title, abstract = soup.find('title').text, soup.find('abstract').text\n",
    "    return title.strip(), abstract.strip()\n",
    "\n",
    "# text preprocessing\n",
    "data_path = join('./','kaggle/')\n",
    "xml_dir = join(data_path, 't2-doc')\n",
    "xml_list = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "# print(len(xml_list))\n",
    "\n",
    "\n",
    "texts = []\n",
    "\n",
    "for xml in xml_list:\n",
    "    path = join(xml_dir,xml)\n",
    "    title, abstract = quote_title_abstract(path)\n",
    "    text = title + '' + abstract\n",
    "    texts.append(text)\n",
    "#     texts.append(title)\n",
    "#     texts.append(abstract)\n",
    "print('read all %d xml files.' % len(xml_list))\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "xml_id_map = {}\n",
    "for i,xml in enumerate(xml_list):\n",
    "    node_id = int(xml.replace('.xml',''))\n",
    "    xml_id_map[node_id] = data[i,:]\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "embeddings_index = {}\n",
    "# with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r', encoding='utf8') as f:\n",
    "with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
