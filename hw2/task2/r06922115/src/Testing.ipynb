{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681494/2681494 [00:20<00:00, 131026.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 5532)\n",
      "(5532,)\n",
      "(5532,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as  np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "np.random.seed(1337)\n",
    "with open('./kaggle/rating_train.csv', 'r') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "u_map = {}\n",
    "\n",
    "dates = []\n",
    "foods = []\n",
    "users = []\n",
    "    \n",
    "\n",
    "\n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        date_str, user, food = l.strip().split(',')\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        user, food = int(user), int(food)\n",
    "        if user not in u_map:\n",
    "            u_map[user] = []\n",
    "        u_map[user].append( (date, food) )\n",
    "        \n",
    "        dates.append(date)\n",
    "        users.append(user)\n",
    "        foods.append(food)\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "user_map = {u:i for i, u in enumerate(set(users))}        \n",
    "food_map = {f:i for i, f in enumerate(set(foods))}\n",
    "\n",
    "\n",
    "# for ranking sparse matrix\n",
    "rows = [user_map[u] for u in users]\n",
    "cols = [food_map[f] for f in foods]\n",
    "R = csr_matrix((np.ones([len(rows), ]), (rows, cols)), shape=(len(user_map), len(food_map)))\n",
    "\n",
    "pos_count = np.array(np.sum(R, axis=0)).flatten()\n",
    "neg_count = len(ls) - pos_count\n",
    "\n",
    "class_weight =  1. / pos_count\n",
    "final_weight = neg_count*class_weight\n",
    "pos_weight = neg_count / pos_count\n",
    "print R.shape\n",
    "print neg_count.shape\n",
    "print pos_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'userid', u'username', u'age', u'gender', u'location', u'city',\n",
      "       u'state', u'title', u'about_me', u'reasons', u'inspirations',\n",
      "       u'friends_count'],\n",
      "      dtype='object')\n",
      "Starting read texts.\n",
      "6990 567.4152607361963 737.8887977118112\n",
      "Found 13852 unique tokens.\n",
      "Preparing embedding matrix.\n",
      "(2,) (2000,)\n",
      "[-0.53151114  1.        ] [  4 372  19 ...   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from constants import MAX_TEXT_SEQ_LEN, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "csv = pd.read_csv('./kaggle/user.csv')\n",
    "print csv.columns\n",
    "# print 'userid,username,age,gender,location,city,state,title,about_me,reasons,inspirations,friends_count'\n",
    "# for csv.iterrows\n",
    "texts = []\n",
    "id_list = []\n",
    "age_list = []\n",
    "gender_list = []\n",
    "print 'Starting read texts.'\n",
    "for row in csv.iterrows():\n",
    "    r = row[1]\n",
    "    s = ''\n",
    "    s += r['about_me'] if not pd.isnull(r['about_me']) else ''\n",
    "    s += r['reasons'] if not pd.isnull(r['reasons']) else ''\n",
    "    s += r['inspirations'] if not pd.isnull(r['inspirations']) else ''\n",
    "    id_list.append(r['userid'])\n",
    "    age_list.append(r['age'])\n",
    "    gender_list.append(r['gender'])\n",
    "    texts.append(s)\n",
    "\n",
    "# normalize age\n",
    "valid_age_list = [age for age in age_list if not np.isnan(age)]\n",
    "m, std = np.mean(valid_age_list), np.std(valid_age_list)\n",
    "for i,age in enumerate(age_list):\n",
    "    if not np.isnan(age):\n",
    "        age_list[i] = float(age-m) / std\n",
    "    else:\n",
    "        age_list[i] = 0\n",
    "buf = [len(s) for s in texts]\n",
    "print np.max(buf), np.mean(buf), np.std(buf)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "pad_data = data = pad_sequences(sequences, maxlen=MAX_TEXT_SEQ_LEN, padding='post', truncating='post')\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "# \n",
    "userid_map = {user:i for i, user in enumerate(id_list)}\n",
    "def get_user_feature_fn(userid):\n",
    "    idx = userid_map[userid]\n",
    "    age = age_list[idx]\n",
    "    gender = 1 if gender_list[idx] == 'Female' else 0\n",
    "    text_seq = pad_data[idx, :]\n",
    "    x = np.array([age, gender])\n",
    "    return x, text_seq\n",
    "u, u_text = get_user_feature_fn(8526)\n",
    "print u.shape, u_text.shape\n",
    "print u, u_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    assert len(Q.size()) == 3 and len(K.size()) == 3 and len(V.size()) == 3\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).cuda()\n",
    "    out = torch.matmul(Q,K.permute(0,2,1)) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out.masked_fill_(mask, -float('inf'))\n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "\n",
    "def positional_encoding(d_model, pos):\n",
    "    assert d_model % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,d_model], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "                            \n",
    "class Transformer_v3(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask, use_cuda=True, posi_cache_length=200):\n",
    "        super(Transformer_v3, self).__init__()\n",
    "#         for construct cache positional encoding matrix.\n",
    "        self.d_model = dm\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask)\n",
    "        self.emb_drop = nn.Dropout(p_drop)\n",
    "        self.init_pos_mat(posi_cache_length)\n",
    "\n",
    "    def forward(self, Q):\n",
    "    \n",
    "        \n",
    "#         decoder\n",
    "        batch, Q_len, d = Q.size()\n",
    "        \n",
    "        try:\n",
    "            Q = Q + self.get_pos_mat(Q_len)\n",
    "        except RuntimeError, e:\n",
    "            if e.message == 'TensorIterator expected type torch.cuda.FloatTensor but got torch.FloatTensor':\n",
    "                if Q.is_cuda != self.get_pos_mat(K_len).is_cuda:\n",
    "                    print('Make sure cache positional matrix is same type of tensor with input, both cuda tensor or not.\\nBy setting argument use_cuda=True to set cache positional encoding matrix as a cuda tensor.')\n",
    "            raise\n",
    "        \n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q)\n",
    "        return de_out\n",
    "    \n",
    "#     To speed up the positional encoding by construct an cache matrix. \n",
    "    def init_pos_mat(self, cache_length):\n",
    "        print('init postional matrix with length : %d ' % cache_length)\n",
    "        self.positional_matrix = torch.cat([positional_encoding(self.d_model, i) for i in range(0,cache_length)], dim=0)\n",
    "        self.positional_matrix.requires_grad = False\n",
    "        if self.use_cuda:\n",
    "            self.positional_matrix = self.positional_matrix.cuda()\n",
    "            \n",
    "        \n",
    "    def get_pos_mat(self, length):\n",
    "        if length > self.positional_matrix.shape[0]:\n",
    "            print('input sequence length reach positional matrix maximum length. %d ' % length)\n",
    "            ret = torch.cat([positional_encoding(self.d_model, i) for i in range(length)], dim=0)\n",
    "            ret.requires_grad = False\n",
    "            print('Increase positional matrix maximum length. %d ' % length)\n",
    "            self.positional_matrix = ret\n",
    "            if self.use_cuda:\n",
    "                self.positional_matrix = self.positional_matrix.cuda()\n",
    "            return ret\n",
    "        else:\n",
    "            return self.positional_matrix[:length]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h, p_drop, d_ff, use_mask) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = nn.Linear(dm, dm)\n",
    "        self.Q_att_drop = nn.Dropout(p_drop)\n",
    "\n",
    "#         feed forward residual block\n",
    "        self.fcn = nn.Linear(dm, dm)\n",
    "        self.linear_drop = nn.Dropout(p_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, Q):\n",
    "        if self.use_mask:\n",
    "            batch, Q_len, d = Q.size()\n",
    "            mask = self.mask_matrix(batch, Q_len)\n",
    "        else:\n",
    "            mask = None\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(scaled_dot_attention(Q, Q, Q, mask=mask))\n",
    "        Q_att_out = self.Q_att_drop(Q_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(Q_att_out)\n",
    "        return linear_out\n",
    "    def mask_matrix(self, batch, Q_len):\n",
    "#         ByteTensor\n",
    "        mask = torch.zeros([1, Q_len, Q_len], dtype=torch.uint8, requires_grad=False)\n",
    "        for i in range(Q_len):\n",
    "            mask[0,i,i+1:] = 1\n",
    "        return mask.repeat(batch,1, 1).cuda()\n",
    "\n",
    "\n",
    "    \n",
    "# Transformer paper baseline hyper-parameters\n",
    "STACKED_NUM = 1\n",
    "H = 4\n",
    "D_MODEL = 128\n",
    "DK = DV = D_MODEL//H\n",
    "P_DROP = 0.05\n",
    "D_FF = D_MODEL*4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# bat = 3\n",
    "# Q = torch.rand([bat, 13, D_MODEL]).cuda()\n",
    "# model = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "# o = model(Q)\n",
    "# print(o.size())\n",
    "\n",
    "# Q = torch.rand([bat, 47, D_MODEL]).cuda()\n",
    "# o = model(Q)\n",
    "# print(o.size())\n",
    "# # # print o\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(count_parameters(model))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F### Transformer with ALS embedding Training\n",
    "# import Transformer/\n",
    "\n",
    "import numpy as np\n",
    "from constants import FOOD_NUM, USER_NUM\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, p_drop, emb_mat):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.food_emb = Food_embedding(FOOD_NUM, dm, 1, p_drop)\n",
    "        self.user_emb = User_embedding(dm, 3, emb_mat, p_drop, activation_fn=F.relu)\n",
    "        self.transformer = Transformer_v3(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "\n",
    "        self.output_linear = nn.Linear(2*dm, FOOD_NUM)\n",
    "\n",
    "    def forward(self, history, u, u_text):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        \n",
    "        x = self.food_emb(history)\n",
    "        batch, x_len, d = x.size()\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "        u_out = self.user_emb(u, u_text).unsqueeze(1).repeat(1, x_len, 1)\n",
    "        x = self.output_linear(torch.cat([x, u_out], dim=-1))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class Food_embedding(nn.Module):\n",
    "    def __init__(self, c_in, dm, layer_num, p_drop, activation_fn=F.selu):\n",
    "        super(Food_embedding, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        assert layer_num >= 1\n",
    "        self.first_linear = nn.Linear(c_in, dm)\n",
    "        self.linears = nn.ModuleList([nn.Linear(dm, dm) for i in range(layer_num-1)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        x = self.first_linear(x)\n",
    "        for lay in self.linears:\n",
    "            x = self.activation_fn(lay(x))\n",
    "            if lay != self.linears[-1]:\n",
    "                x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class User_embedding(nn.Module):\n",
    "    def __init__(self, dm, layer_num, emb_mat, p_drop, activation_fn=F.selu):\n",
    "        super(User_embedding, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        assert layer_num >= 1\n",
    "        self.emb = nn.Embedding(emb_mat.shape[0], emb_mat.shape[1], padding_idx=0)\n",
    "        self.emb.weight = nn.Parameter(torch.FloatTensor(emb_mat))\n",
    "        self.emb.weight.requires_grad_(False)\n",
    "        self.emb_linear = nn.Linear(emb_mat.shape[1], dm)\n",
    "        self.att_weight = nn.Parameter(torch.zeros([1, dm-2**(layer_num+1), ], dtype=torch.float))\n",
    "        torch.nn.init.xavier_normal_(self.att_weight)\n",
    "        self.linears = nn.ModuleList([nn.Linear(2**(i+1), 2**(i+2)) for i in range(layer_num)])\n",
    "        \n",
    "\n",
    "    def forward(self, u, u_text):\n",
    "        for lay in self.linears:\n",
    "            u = self.activation_fn(lay(u))\n",
    "#         for text\n",
    "        u_text = self.emb(u_text)\n",
    "        u_text = self.activation_fn(self.emb_linear(u_text))\n",
    "        batch, seq, d = u_text.size()\n",
    "        att_w = self.att_weight.view(1,1, -1).repeat(batch, 1, 1)\n",
    "        Q = torch.cat([u.unsqueeze(1),att_w], dim=-1)\n",
    "        u_att = scaled_dot_attention(Q, u_text, u_text, mask=None)\n",
    "        u_att.squeeze_(1)\n",
    "        \n",
    "        return u_att\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:19<00:00, 131.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last time 0\n",
      "pre_time 4 0\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best-hybrid.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "    with open('predict.csv', 'w') as f_out:\n",
    "        f_out.write('userid,foodid\\n')\n",
    "\n",
    "        with tqdm(total=len(u_map)) as pbar:\n",
    "            for user in u_map.keys():\n",
    "                x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "                history = u_map[user]\n",
    "                ds = np.array([d for d,f in history])\n",
    "                fs = np.array([f for d,f in history])\n",
    "                sorted_idx = np.argsort(ds)\n",
    "                ds = ds[sorted_idx]\n",
    "                fd = fs[sorted_idx]\n",
    "\n",
    "                date_idx = 0\n",
    "                now_date = ds[0]\n",
    "                for food, date in zip(fs,ds):\n",
    "                    if date != now_date:\n",
    "                        date_idx+=1\n",
    "                        now_date = date\n",
    "                    x[date_idx, food_map[food]] = 1\n",
    "                x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "#                 out = torch.sigmoid(model(x))\n",
    "                u, u_text = get_user_feature_fn(user)\n",
    "                u = torch.FloatTensor(u).unsqueeze(0).cuda()\n",
    "                u_text = torch.LongTensor(u_text).unsqueeze(0).cuda()\n",
    "                out = model(x, u, u_text)\n",
    "\n",
    "                arr = out[0,date_idx,:].flatten()\n",
    "                k20 = reversed(torch.argsort(arr)[-20:])\n",
    "                s = ''\n",
    "                for food_idx in k20:\n",
    "                    s += ' %d' % rev_food_map[food_idx.item()]\n",
    "                f_out.write('%d,%s\\n' % (user, s) )\n",
    "\n",
    "                pbar.update(1)\n",
    "t = 0.5\n",
    "buf = out[0,date_idx,:]>t\n",
    "print 'last time' ,torch.sum(buf).item()\n",
    "\n",
    "buf = out[0,date_idx-1,:]>t\n",
    "buf2 = x[0,date_idx-1,:]>t\n",
    "print 'pre_time',torch.sum(buf2).item(), torch.sum(buf).item()\n",
    "print 'done'\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:23<00:00, 109.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5532/5532 [00:30<00:00, 179.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071, 0.5657, 0.5051, 0.5253, 0.5455, 0.5556, 0.2323, 0.3535, 0.5960,\n",
      "        0.3939, 0.5556, 0.5051, 0.4747, 0.8081, 0.5455, 0.4141, 0.7475, 0.3939,\n",
      "        0.5051, 0.5152, 0.3131, 0.3131, 0.5657, 0.3333, 0.4141, 0.4949, 0.0606,\n",
      "        0.7172, 0.5455, 0.3131, 0.2222, 0.7273, 0.4747, 0.5960, 0.5051, 0.3535,\n",
      "        0.3030, 0.2626, 0.4848, 0.5051, 0.1212, 0.4343, 0.4747, 0.3333, 0.5859,\n",
      "        0.4747, 0.2424, 0.4949, 0.5152, 0.1212, 0.1212, 0.2525, 0.2626, 0.5758,\n",
      "        0.6869, 0.2222, 0.5152, 0.1818, 0.3535, 0.3333, 0.7071, 0.5253, 0.5152,\n",
      "        0.7980, 0.2828, 0.5253, 0.0606, 0.4848, 0.5657, 0.5657, 0.1919, 0.1414,\n",
      "        0.3131, 0.6566, 0.2626, 0.3939, 0.3535, 0.2323, 0.2424, 0.2525, 0.4848,\n",
      "        0.4949, 0.6162, 0.4141, 0.7677, 0.1313, 0.3838, 0.4141, 0.4545, 0.4646,\n",
      "        0.4747, 0.3333, 0.0808, 0.5354, 0.3737, 0.3333, 0.4141, 0.4646, 0.6061,\n",
      "        0.5354])\n",
      "[0.9828220858895705, 0.9512653374233129, 0.9477760736196319, 0.9883819018404908, 0.9695935582822086, 0.9822085889570552, 0.99079754601227, 0.9838957055214724, 0.9843174846625767, 0.9996165644171779, 0.9995398773006134, 0.9876150306748466, 0.995590490797546, 0.9994248466257669, 0.9968941717791411, 0.9995398773006134, 0.998542944785276, 0.9956671779141104, 0.8451687116564417, 0.8713957055214724, 0.9998466257668711, 0.9990414110429447, 0.9762269938650306, 0.9999233128834356, 0.9957438650306748, 0.9045245398773006, 1.0, 0.9929064417177914, 0.9958205521472393, 0.9975843558282208, 0.9984662576687117, 0.9989647239263804, 0.9810966257668712, 0.9902223926380368, 0.8537960122699386, 0.9965490797546013, 0.9998466257668711, 0.9998466257668711, 0.9754217791411043, 0.7378834355828221, 0.9991180981595092, 0.9355444785276074, 0.9011503067484663, 0.9981211656441717, 0.9947469325153374, 0.9852377300613497, 0.9994248466257669, 0.967829754601227, 0.9863880368098159, 0.998888036809816, 0.9997315950920246, 0.9989263803680981, 0.9907592024539877, 0.9760736196319019, 0.9993865030674847, 0.9999616564417177, 0.9962806748466257, 0.999808282208589, 0.9999616564417177, 0.9992331288343558, 0.9976610429447853, 0.9236963190184049, 0.9891871165644172, 0.9993098159509203, 0.9999233128834356, 0.9894171779141104, 0.9998466257668711, 0.9524923312883435, 0.9835122699386503, 0.9434049079754602, 0.9989647239263804, 0.9983128834355828, 0.997430981595092, 0.9763036809815951, 0.9999616564417177, 0.9993481595092024, 0.9885352760736197, 0.9994248466257669, 0.9993481595092024, 0.9924846625766871, 0.9475076687116565, 0.9129217791411043, 0.9962039877300614, 0.9655674846625767, 0.9962423312883436, 0.9991947852760736, 0.9867331288343558, 0.9944401840490797, 0.988228527607362, 0.9999233128834356, 0.9950536809815951, 0.9996932515337423, 0.9998849693251534, 0.9973926380368098, 0.9879984662576687, 0.9998849693251534, 0.9904141104294478, 0.9832438650306748, 0.9996549079754601, 0.9910659509202454]\n",
      "tensor(7, device='cuda:0') tensor(7, device='cuda:0') tensor(15., device='cuda:0')\n",
      "tensor(2, device='cuda:0') tensor(3, device='cuda:0') tensor(15., device='cuda:0')\n",
      "tensor(5, device='cuda:0') tensor(3, device='cuda:0') tensor(17., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(16., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(12., device='cuda:0')\n",
      "tensor(5, device='cuda:0') tensor(4, device='cuda:0') tensor(14., device='cuda:0')\n",
      "tensor(5, device='cuda:0') tensor(5, device='cuda:0') tensor(14., device='cuda:0')\n",
      "tensor(7, device='cuda:0') tensor(7, device='cuda:0') tensor(16., device='cuda:0')\n",
      "tensor(6, device='cuda:0') tensor(6, device='cuda:0') tensor(14., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(16., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "outputs = []\n",
    "labels = []\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(u_map)) as pbar:\n",
    "        for user in u_map.keys():\n",
    "            x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "            history = u_map[user]\n",
    "            ds = np.array([d for d,f in history])\n",
    "            fs = np.array([f for d,f in history])\n",
    "            sorted_idx = np.argsort(ds)\n",
    "            ds = ds[sorted_idx]\n",
    "            fd = fs[sorted_idx]\n",
    "\n",
    "            date_idx = 0\n",
    "            now_date = ds[0]\n",
    "            for food, date in zip(fs,ds):\n",
    "                if date != now_date:\n",
    "                    date_idx+=1\n",
    "                    now_date = date\n",
    "                x[date_idx, food_map[food]] = 1\n",
    "            x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "            out = torch.sigmoid(model(x))\n",
    "            period = 10\n",
    "            outputs.append(out[0,date_idx-period:date_idx,:].cpu().numpy())\n",
    "            labels.append(x[0,date_idx-period+1:date_idx+1,:].cpu().numpy())\n",
    "#             outputs.append(out[0,date_idx-period:date_idx,:])\n",
    "#             labels.append(x[0,date_idx-period+1:date_idx+1,:])\n",
    "            pbar.update(1)\n",
    "print 'done'\n",
    "outputs = np.vstack(outputs)\n",
    "labels = np.vstack(labels)\n",
    "# \n",
    "outputs = torch.FloatTensor(np.vstack(outputs)).cuda()\n",
    "labels = torch.FloatTensor(np.vstack(labels)).cuda()\n",
    "\n",
    "# \n",
    "best_threshold =[]\n",
    "accs = []\n",
    "with tqdm(total=len(food_map)) as pbar:\n",
    "    for i in range(len(food_map)):\n",
    "        o = outputs[:,i]\n",
    "        l = labels[:,i].type(torch.uint8)\n",
    "        best_acc = 0\n",
    "        best_t = 0\n",
    "        for t in np.linspace(0,1, 100):\n",
    "#         for t in set(o):\n",
    "            a = o > float(t)\n",
    "            acc = torch.sum(a == l).item() / float(outputs.shape[0])\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_t = t\n",
    "        best_threshold.append(best_t)\n",
    "        accs.append(best_acc)\n",
    "        pbar.update(1)\n",
    "best_threshold = torch.FloatTensor(best_threshold)\n",
    "print best_threshold[:100]\n",
    "print accs[:100]\n",
    "    \n",
    "# for test\n",
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "tt = best_threshold.cuda()\n",
    "with torch.no_grad():\n",
    "    for user in u_map.keys():\n",
    "        x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "        history = u_map[user]\n",
    "        ds = np.array([d for d,f in history])\n",
    "        fs = np.array([f for d,f in history])\n",
    "        sorted_idx = np.argsort(ds)\n",
    "        ds = ds[sorted_idx]\n",
    "        fd = fs[sorted_idx]\n",
    "\n",
    "        date_idx = 0\n",
    "        now_date = ds[0]\n",
    "        for food, date in zip(fs,ds):\n",
    "            if date != now_date:\n",
    "                date_idx+=1\n",
    "                now_date = date\n",
    "            x[date_idx, food_map[food]] = 1\n",
    "        x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "        out = torch.sigmoid(model(x))\n",
    "        \n",
    "        for i in range(date_idx-10, date_idx+1, 1):\n",
    "            a = out[0,i,:]\n",
    "            b = x[0,i+1,:]\n",
    "            print torch.sum(a>tt), torch.sum(a>0.5), torch.sum(b)\n",
    "        break\n",
    "\n",
    "            \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
