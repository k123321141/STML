{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681494/2681494 [00:20<00:00, 131350.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 5532)\n",
      "(5532,)\n",
      "(5532,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as  np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "np.random.seed(1337)\n",
    "with open('./kaggle/rating_train.csv', 'r') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "u_map = {}\n",
    "\n",
    "dates = []\n",
    "foods = []\n",
    "users = []\n",
    "    \n",
    "\n",
    "\n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        date_str, user, food = l.strip().split(',')\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        user, food = int(user), int(food)\n",
    "        if user not in u_map:\n",
    "            u_map[user] = []\n",
    "        u_map[user].append( (date, food) )\n",
    "        \n",
    "        dates.append(date)\n",
    "        users.append(user)\n",
    "        foods.append(food)\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "user_map = {u:i for i, u in enumerate(set(users))}        \n",
    "food_map = {f:i for i, f in enumerate(set(foods))}\n",
    "\n",
    "\n",
    "# for ranking sparse matrix\n",
    "rows = [user_map[u] for u in users]\n",
    "cols = [food_map[f] for f in foods]\n",
    "R = csr_matrix((np.ones([len(rows), ]), (rows, cols)), shape=(len(user_map), len(food_map)))\n",
    "\n",
    "pos_count = np.array(np.sum(R, axis=0)).flatten()\n",
    "neg_count = len(ls) - pos_count\n",
    "\n",
    "class_weight =  1. / pos_count\n",
    "final_weight = neg_count*class_weight\n",
    "pos_weight = neg_count / pos_count\n",
    "print R.shape\n",
    "print neg_count.shape\n",
    "print pos_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    assert len(Q.size()) == 3 and len(K.size()) == 3 and len(V.size()) == 3\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).cuda()\n",
    "    out = torch.matmul(Q,K.permute(0,2,1)) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out.masked_fill_(mask, -float('inf'))\n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "\n",
    "def positional_encoding(d_model, pos):\n",
    "    assert d_model % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,d_model], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "                            \n",
    "class Transformer_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask, use_cuda=True, posi_cache_length=200):\n",
    "        super(Transformer_v2, self).__init__()\n",
    "#         for construct cache positional encoding matrix.\n",
    "        self.d_model = dm\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask)\n",
    "        self.emb_drop = nn.Dropout(p_drop)\n",
    "        self.init_pos_mat(posi_cache_length)\n",
    "\n",
    "    def forward(self, Q):\n",
    "    \n",
    "        \n",
    "#         decoder\n",
    "        batch, Q_len, d = Q.size()\n",
    "        \n",
    "        try:\n",
    "            Q = Q + self.get_pos_mat(Q_len)\n",
    "        except RuntimeError, e:\n",
    "            if e.message == 'TensorIterator expected type torch.cuda.FloatTensor but got torch.FloatTensor':\n",
    "                if Q.is_cuda != self.get_pos_mat(K_len).is_cuda:\n",
    "                    print('Make sure cache positional matrix is same type of tensor with input, both cuda tensor or not.\\nBy setting argument use_cuda=True to set cache positional encoding matrix as a cuda tensor.')\n",
    "            raise\n",
    "        \n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q)\n",
    "        return de_out\n",
    "    \n",
    "#     To speed up the positional encoding by construct an cache matrix. \n",
    "    def init_pos_mat(self, cache_length):\n",
    "        print('init postional matrix with length : %d ' % cache_length)\n",
    "        self.positional_matrix = torch.cat([positional_encoding(self.d_model, i) for i in range(0,cache_length)], dim=0)\n",
    "        self.positional_matrix.requires_grad = False\n",
    "        if self.use_cuda:\n",
    "            self.positional_matrix = self.positional_matrix.cuda()\n",
    "            \n",
    "        \n",
    "    def get_pos_mat(self, length):\n",
    "        if length > self.positional_matrix.shape[0]:\n",
    "            print('input sequence length reach positional matrix maximum length. %d ' % length)\n",
    "            ret = torch.cat([positional_encoding(self.d_model, i) for i in range(length)], dim=0)\n",
    "            ret.requires_grad = False\n",
    "            print('Increase positional matrix maximum length. %d ' % length)\n",
    "            self.positional_matrix = ret\n",
    "            if self.use_cuda:\n",
    "                self.positional_matrix = self.positional_matrix.cuda()\n",
    "            return ret\n",
    "        else:\n",
    "            return self.positional_matrix[:length]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h, p_drop, d_ff, use_mask) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.Q_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.Q_att_drop = nn.Dropout(p_drop)\n",
    "\n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(dm, d_ff)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.linear_drop = nn.Dropout(p_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, Q):\n",
    "        if self.use_mask:\n",
    "            batch, Q_len, d = Q.size()\n",
    "            mask = self.mask_matrix(batch, Q_len)\n",
    "        else:\n",
    "            mask = None\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(Q, Q, Q, mask=mask)\n",
    "        Q_attention_out = self.Q_att_drop(Q_attention_out)\n",
    "        Q_att_out = self.Q_attention_norm_lay(Q + Q_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(Q_att_out)\n",
    "        out = self.ff_norm_lay(Q_att_out + linear_out)\n",
    "        return out\n",
    "    def mask_matrix(self, batch, Q_len):\n",
    "#         ByteTensor\n",
    "        mask = torch.zeros([1, Q_len, Q_len], dtype=torch.uint8, requires_grad=False)\n",
    "        for i in range(Q_len):\n",
    "            mask[0,i,i+1:] = 1\n",
    "        return mask.repeat(batch,1, 1).cuda()\n",
    "\n",
    "\n",
    "class Multi_Head_attention_layer(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Multi_Head_attention_layer, self).__init__()\n",
    "        self.Q_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.K_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.V_linears = nn.ModuleList([nn.Linear(dm, dv) for i in range(h)])\n",
    "        self.output_linear = nn.Linear(h*dv, dm)\n",
    "                            \n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask):\n",
    "        buf = []\n",
    "        for Q_linear, K_linear, V_linear in zip(self.Q_linears, self.K_linears, self.V_linears):\n",
    "            Q = Q_linear(Q_input)\n",
    "            K = K_linear(K_input)\n",
    "            V = V_linear(V_input)\n",
    "            buf.append(scaled_dot_attention(Q, K, V, mask))\n",
    "        \n",
    "        buf = torch.cat(buf,dim=-1)\n",
    "        out = self.output_linear(buf)\n",
    "        \n",
    "        return out      \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        self.cnn2 = nn.Conv1d(d_ff, d_model, 1)\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        bat,seq_len,d = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        return x      \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Transformer paper baseline hyper-parameters\n",
    "STACKED_NUM = 2\n",
    "H = 4\n",
    "D_MODEL = 128\n",
    "DK = DV = D_MODEL//H\n",
    "P_DROP = 0.05\n",
    "D_FF = D_MODEL*4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# bat = 3\n",
    "# Q = torch.rand([bat, 13, D_MODEL]).cuda()\n",
    "# model = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "# o = model(Q)\n",
    "# print(o.size())\n",
    "\n",
    "# Q = torch.rand([bat, 47, D_MODEL]).cuda()\n",
    "# o = model(Q)\n",
    "# print(o.size())\n",
    "# # # print o\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(count_parameters(model))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F### Transformer with ALS embedding Training\n",
    "# import Transformer/\n",
    "\n",
    "import numpy as np\n",
    "from constants import FOOD_NUM, USER_NUM\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, p_drop):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.food_emb = Food_embedding(FOOD_NUM, dm, 1, p_drop)\n",
    "        self.transformer = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "\n",
    "        self.output_linear = nn.Linear(dm, FOOD_NUM)\n",
    "\n",
    "    def forward(self, history):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        \n",
    "        x = self.food_emb(history)\n",
    "        batch, x_len, d = x.size()\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = self.output_linear(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "class Food_embedding(nn.Module):\n",
    "    def __init__(self, c_in, dm, layer_num, p_drop, activation_fn=F.selu):\n",
    "        super(Food_embedding, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        assert layer_num >= 1\n",
    "        self.first_linear = nn.Linear(c_in, dm)\n",
    "        self.linears = nn.ModuleList([nn.Linear(dm, dm) for i in range(layer_num-1)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        x = self.first_linear(x)\n",
    "        for lay in self.linears:\n",
    "            x = self.activation_fn(lay(x))\n",
    "            if lay != self.linears[-1]:\n",
    "                x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "# batch = 7\n",
    "# dm = D_MODEL\n",
    "# Q = torch.rand([batch, 18, FOOD_NUM]).cuda()\n",
    "# model = Net(dm, 0.1).cuda()\n",
    "# o = model(Q)\n",
    "# # print t\n",
    "# print(o.size())\n",
    "# # print o\n",
    "\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "# print(count_parameters(model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:23<00:00, 109.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5532/5532 [00:30<00:00, 179.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7071, 0.5657, 0.5051, 0.5253, 0.5455, 0.5556, 0.2323, 0.3535, 0.5960,\n",
      "        0.3939, 0.5556, 0.5051, 0.4747, 0.8081, 0.5455, 0.4141, 0.7475, 0.3939,\n",
      "        0.5051, 0.5152, 0.3131, 0.3131, 0.5657, 0.3333, 0.4141, 0.4949, 0.0606,\n",
      "        0.7172, 0.5455, 0.3131, 0.2222, 0.7273, 0.4747, 0.5960, 0.5051, 0.3535,\n",
      "        0.3030, 0.2626, 0.4848, 0.5051, 0.1212, 0.4343, 0.4747, 0.3333, 0.5859,\n",
      "        0.4747, 0.2424, 0.4949, 0.5152, 0.1212, 0.1212, 0.2525, 0.2626, 0.5758,\n",
      "        0.6869, 0.2222, 0.5152, 0.1818, 0.3535, 0.3333, 0.7071, 0.5253, 0.5152,\n",
      "        0.7980, 0.2828, 0.5253, 0.0606, 0.4848, 0.5657, 0.5657, 0.1919, 0.1414,\n",
      "        0.3131, 0.6566, 0.2626, 0.3939, 0.3535, 0.2323, 0.2424, 0.2525, 0.4848,\n",
      "        0.4949, 0.6162, 0.4141, 0.7677, 0.1313, 0.3838, 0.4141, 0.4545, 0.4646,\n",
      "        0.4747, 0.3333, 0.0808, 0.5354, 0.3737, 0.3333, 0.4141, 0.4646, 0.6061,\n",
      "        0.5354])\n",
      "[0.9828220858895705, 0.9512653374233129, 0.9477760736196319, 0.9883819018404908, 0.9695935582822086, 0.9822085889570552, 0.99079754601227, 0.9838957055214724, 0.9843174846625767, 0.9996165644171779, 0.9995398773006134, 0.9876150306748466, 0.995590490797546, 0.9994248466257669, 0.9968941717791411, 0.9995398773006134, 0.998542944785276, 0.9956671779141104, 0.8451687116564417, 0.8713957055214724, 0.9998466257668711, 0.9990414110429447, 0.9762269938650306, 0.9999233128834356, 0.9957438650306748, 0.9045245398773006, 1.0, 0.9929064417177914, 0.9958205521472393, 0.9975843558282208, 0.9984662576687117, 0.9989647239263804, 0.9810966257668712, 0.9902223926380368, 0.8537960122699386, 0.9965490797546013, 0.9998466257668711, 0.9998466257668711, 0.9754217791411043, 0.7378834355828221, 0.9991180981595092, 0.9355444785276074, 0.9011503067484663, 0.9981211656441717, 0.9947469325153374, 0.9852377300613497, 0.9994248466257669, 0.967829754601227, 0.9863880368098159, 0.998888036809816, 0.9997315950920246, 0.9989263803680981, 0.9907592024539877, 0.9760736196319019, 0.9993865030674847, 0.9999616564417177, 0.9962806748466257, 0.999808282208589, 0.9999616564417177, 0.9992331288343558, 0.9976610429447853, 0.9236963190184049, 0.9891871165644172, 0.9993098159509203, 0.9999233128834356, 0.9894171779141104, 0.9998466257668711, 0.9524923312883435, 0.9835122699386503, 0.9434049079754602, 0.9989647239263804, 0.9983128834355828, 0.997430981595092, 0.9763036809815951, 0.9999616564417177, 0.9993481595092024, 0.9885352760736197, 0.9994248466257669, 0.9993481595092024, 0.9924846625766871, 0.9475076687116565, 0.9129217791411043, 0.9962039877300614, 0.9655674846625767, 0.9962423312883436, 0.9991947852760736, 0.9867331288343558, 0.9944401840490797, 0.988228527607362, 0.9999233128834356, 0.9950536809815951, 0.9996932515337423, 0.9998849693251534, 0.9973926380368098, 0.9879984662576687, 0.9998849693251534, 0.9904141104294478, 0.9832438650306748, 0.9996549079754601, 0.9910659509202454]\n",
      "tensor(7, device='cuda:0') tensor(7, device='cuda:0') tensor(15., device='cuda:0')\n",
      "tensor(2, device='cuda:0') tensor(3, device='cuda:0') tensor(15., device='cuda:0')\n",
      "tensor(5, device='cuda:0') tensor(3, device='cuda:0') tensor(17., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(16., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(12., device='cuda:0')\n",
      "tensor(5, device='cuda:0') tensor(4, device='cuda:0') tensor(14., device='cuda:0')\n",
      "tensor(5, device='cuda:0') tensor(5, device='cuda:0') tensor(14., device='cuda:0')\n",
      "tensor(7, device='cuda:0') tensor(7, device='cuda:0') tensor(16., device='cuda:0')\n",
      "tensor(6, device='cuda:0') tensor(6, device='cuda:0') tensor(14., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(16., device='cuda:0')\n",
      "tensor(3, device='cuda:0') tensor(3, device='cuda:0') tensor(0., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "outputs = []\n",
    "labels = []\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(u_map)) as pbar:\n",
    "        for user in u_map.keys():\n",
    "            x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "            history = u_map[user]\n",
    "            ds = np.array([d for d,f in history])\n",
    "            fs = np.array([f for d,f in history])\n",
    "            sorted_idx = np.argsort(ds)\n",
    "            ds = ds[sorted_idx]\n",
    "            fd = fs[sorted_idx]\n",
    "\n",
    "            date_idx = 0\n",
    "            now_date = ds[0]\n",
    "            for food, date in zip(fs,ds):\n",
    "                if date != now_date:\n",
    "                    date_idx+=1\n",
    "                    now_date = date\n",
    "                x[date_idx, food_map[food]] = 1\n",
    "            x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "            out = torch.sigmoid(model(x))\n",
    "            period = 10\n",
    "            outputs.append(out[0,date_idx-period:date_idx,:].cpu().numpy())\n",
    "            labels.append(x[0,date_idx-period+1:date_idx+1,:].cpu().numpy())\n",
    "#             outputs.append(out[0,date_idx-period:date_idx,:])\n",
    "#             labels.append(x[0,date_idx-period+1:date_idx+1,:])\n",
    "            pbar.update(1)\n",
    "print 'done'\n",
    "outputs = np.vstack(outputs)\n",
    "labels = np.vstack(labels)\n",
    "# \n",
    "outputs = torch.FloatTensor(np.vstack(outputs)).cuda()\n",
    "labels = torch.FloatTensor(np.vstack(labels)).cuda()\n",
    "\n",
    "# \n",
    "best_threshold =[]\n",
    "accs = []\n",
    "with tqdm(total=len(food_map)) as pbar:\n",
    "    for i in range(len(food_map)):\n",
    "        o = outputs[:,i]\n",
    "        l = labels[:,i].type(torch.uint8)\n",
    "        best_acc = 0\n",
    "        best_t = 0\n",
    "        for t in np.linspace(0,1, 100):\n",
    "#         for t in set(o):\n",
    "            a = o > float(t)\n",
    "            acc = torch.sum(a == l).item() / float(outputs.shape[0])\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                best_t = t\n",
    "        best_threshold.append(best_t)\n",
    "        accs.append(best_acc)\n",
    "        pbar.update(1)\n",
    "best_threshold = torch.FloatTensor(best_threshold)\n",
    "print best_threshold[:100]\n",
    "print accs[:100]\n",
    "    \n",
    "# for test\n",
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "tt = best_threshold.cuda()\n",
    "with torch.no_grad():\n",
    "    for user in u_map.keys():\n",
    "        x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "        history = u_map[user]\n",
    "        ds = np.array([d for d,f in history])\n",
    "        fs = np.array([f for d,f in history])\n",
    "        sorted_idx = np.argsort(ds)\n",
    "        ds = ds[sorted_idx]\n",
    "        fd = fs[sorted_idx]\n",
    "\n",
    "        date_idx = 0\n",
    "        now_date = ds[0]\n",
    "        for food, date in zip(fs,ds):\n",
    "            if date != now_date:\n",
    "                date_idx+=1\n",
    "                now_date = date\n",
    "            x[date_idx, food_map[food]] = 1\n",
    "        x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "        out = torch.sigmoid(model(x))\n",
    "        \n",
    "        for i in range(date_idx-10, date_idx+1, 1):\n",
    "            a = out[0,i,:]\n",
    "            b = x[0,i+1,:]\n",
    "            print torch.sum(a>tt), torch.sum(a>0.5), torch.sum(b)\n",
    "        break\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 torch.Size([1, 165, 5532]) <type 'int'> torch.Size([5532])\n",
      "[19, 34, 110]\n",
      "[110, 34, 19, 139, 107, 39, 80, 228, 25, 2, 105, 530, 217, 18, 224, 384, 173, 102, 166, 441]\n",
      "110 89.0\n",
      "34 80.0\n",
      "19 15.0\n",
      "139 22.0\n",
      "107 35.0\n",
      "39 93.0\n",
      "80 31.0\n",
      "228 2.0\n",
      "25 65.0\n",
      "2 35.0\n",
      "105 6.0\n",
      "530 27.0\n",
      "217 21.0\n",
      "18 42.0\n",
      "224 0.0\n",
      "384 0.0\n",
      "173 6.0\n",
      "102 19.0\n",
      "166 3.0\n",
      "441 9.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "tt = best_threshold.cuda()\n",
    "with torch.no_grad():\n",
    "    for user in u_map.keys():\n",
    "        x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "        history = u_map[user]\n",
    "        ds = np.array([d for d,f in history])\n",
    "        fs = np.array([f for d,f in history])\n",
    "        sorted_idx = np.argsort(ds)\n",
    "        ds = ds[sorted_idx]\n",
    "        fd = fs[sorted_idx]\n",
    "\n",
    "        date_idx = 0\n",
    "        now_date = ds[0]\n",
    "        for food, date in zip(fs,ds):\n",
    "            if date != now_date:\n",
    "                date_idx+=1\n",
    "                now_date = date\n",
    "            x[date_idx, food_map[food]] = 1\n",
    "        x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "        out = torch.sigmoid(model(x))\n",
    "        print date_idx, out.shape, type(date_idx), out[0, date_idx,:].shape\n",
    "        arr = out[0, date_idx, :].flatten()\n",
    "        k20 = [ i.item() for i in reversed(torch.argsort(arr)[-20:])]\n",
    "        \n",
    "        c = arr > 0.5\n",
    "        idxs =[ i.item() for i in torch.argsort(c)[-int(torch.sum(c)):]]\n",
    "        print idxs\n",
    "        print k20\n",
    "        for i in idxs:\n",
    "            assert i in k20\n",
    "        for i in k20:\n",
    "            print i, R[user_map[user], i]\n",
    "        \n",
    "#         for i in range(date_idx-10, date_idx+1, 1):\n",
    "#             a = out[0,i,:]\n",
    "#             b = x[0,i+1,:]\n",
    "#             print torch.sum(a>tt), torch.sum(a>0.5),torch.sum(b)\n",
    "        break\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)\n",
      "tensor([ 19,  34, 110], device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CPU but got backend CUDA for argument #2 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d56d82ba639a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CPU but got backend CUDA for argument #2 'other'"
     ]
    }
   ],
   "source": [
    "c = a > tt\n",
    "print c\n",
    "idxs = torch.argsort(c)[-int(torch.sum(c)):]\n",
    "print idxs\n",
    "d = torch.FloatTensor([19,34])\n",
    "print d in idxs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:31<00:00, 82.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last time 0\n",
      "pre_time 0 0\n",
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best-mse.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "    with open('predict.csv', 'w') as f_out:\n",
    "        f_out.write('userid,foodid\\n')\n",
    "\n",
    "        with tqdm(total=len(u_map)) as pbar:\n",
    "            for user in u_map.keys():\n",
    "                x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "                history = u_map[user]\n",
    "                ds = np.array([d for d,f in history])\n",
    "                fs = np.array([f for d,f in history])\n",
    "                sorted_idx = np.argsort(ds)\n",
    "                ds = ds[sorted_idx]\n",
    "                fd = fs[sorted_idx]\n",
    "\n",
    "                date_idx = 0\n",
    "                now_date = ds[0]\n",
    "                for food, date in zip(fs,ds):\n",
    "                    if date != now_date:\n",
    "                        date_idx+=1\n",
    "                        now_date = date\n",
    "                    x[date_idx, food_map[food]] = 1\n",
    "                x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "#                 out = torch.sigmoid(model(x))\n",
    "                out = model(x)\n",
    "\n",
    "                arr = out[0,date_idx,:].flatten()\n",
    "                k20 = reversed(torch.argsort(arr)[-20:])\n",
    "                s = ''\n",
    "                for food_idx in k20:\n",
    "                    s += ' %d' % rev_food_map[food_idx.item()]\n",
    "                f_out.write('%d,%s\\n' % (user, s) )\n",
    "\n",
    "                pbar.update(1)\n",
    "t = 1\n",
    "buf = out[0,date_idx,:]>t\n",
    "print 'last time' ,torch.sum(buf).item()\n",
    "\n",
    "buf = out[0,date_idx-1,:]>t\n",
    "buf2 = x[0,date_idx-1,:]>t\n",
    "print 'pre_time',torch.sum(buf2).item(), torch.sum(buf).item()\n",
    "print 'done'\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10) 4.5\n",
      "deque([5, 6, 7, 8, 9, nan, nan, nan, nan, nan], maxlen=10) nan\n",
      "deque([6, 7, 8, 9, nan, nan, nan, nan, nan, 0], maxlen=10) nan\n",
      "deque([7, 8, 9, nan, nan, nan, nan, nan, 0, 1], maxlen=10) nan\n",
      "deque([8, 9, nan, nan, nan, nan, nan, 0, 1, 2], maxlen=10) nan\n",
      "deque([9, nan, nan, nan, nan, nan, 0, 1, 2, 3], maxlen=10) nan\n",
      "deque([nan, nan, nan, nan, nan, 0, 1, 2, 3, 4], maxlen=10) nan\n",
      "deque([nan, nan, nan, nan, 0, 1, 2, 3, 4, 5], maxlen=10) nan\n",
      "deque([nan, nan, nan, 0, 1, 2, 3, 4, 5, 6], maxlen=10) nan\n",
      "deque([nan, nan, 0, 1, 2, 3, 4, 5, 6, 7], maxlen=10) nan\n",
      "deque([nan, 0, 1, 2, 3, 4, 5, 6, 7, 8], maxlen=10) nan\n",
      "deque([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], maxlen=10) 4.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "q = deque(maxlen=10)\n",
    "\n",
    "for i in range(10):\n",
    "    q.append(i)\n",
    "print q,np.mean(q)\n",
    "for i in range(5):\n",
    "    q.append(float('nan'))\n",
    "print q, np.mean(q)\n",
    "for i in range(10):\n",
    "    q.append(i)\n",
    "    print q, np.mean(q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_time 0 13 9\n",
      "pre_time 1 13 4\n",
      "pre_time 2 9 3\n",
      "pre_time 3 7 11\n",
      "pre_time 4 11 9\n",
      "pre_time 5 13 6\n",
      "pre_time 6 7 11\n",
      "pre_time 7 10 1\n",
      "pre_time 8 7 5\n",
      "pre_time 9 10 6\n",
      "pre_time 10 9 9\n",
      "pre_time 11 13 2\n",
      "pre_time 12 10 5\n",
      "pre_time 13 11 1\n",
      "pre_time 14 11 10\n",
      "pre_time 15 12 11\n",
      "pre_time 16 11 11\n",
      "pre_time 17 9 6\n",
      "pre_time 18 10 7\n",
      "pre_time 19 7 3\n",
      "pre_time 20 3 1\n",
      "pre_time 21 2 2\n",
      "pre_time 22 5 5\n",
      "pre_time 23 1 6\n",
      "pre_time 24 3 8\n",
      "pre_time 25 4 6\n",
      "pre_time 26 6 2\n",
      "pre_time 27 5 6\n",
      "pre_time 28 5 6\n",
      "pre_time 29 7 7\n",
      "pre_time 30 9 6\n",
      "pre_time 31 7 6\n",
      "pre_time 32 9 4\n",
      "pre_time 33 13 7\n",
      "pre_time 34 9 7\n",
      "pre_time 35 11 10\n",
      "pre_time 36 11 8\n",
      "pre_time 37 11 3\n",
      "pre_time 38 5 3\n",
      "pre_time 39 4 1\n",
      "pre_time 40 5 10\n",
      "pre_time 41 9 4\n",
      "pre_time 42 8 8\n",
      "pre_time 43 9 3\n",
      "pre_time 44 7 8\n",
      "pre_time 45 11 8\n",
      "pre_time 46 10 5\n",
      "pre_time 47 6 3\n",
      "pre_time 48 9 8\n",
      "pre_time 49 11 7\n",
      "pre_time 50 8 5\n",
      "pre_time 51 4 7\n",
      "pre_time 52 4 6\n",
      "pre_time 53 5 8\n",
      "pre_time 54 3 4\n",
      "pre_time 55 14 10\n",
      "pre_time 56 19 9\n",
      "pre_time 57 9 2\n",
      "pre_time 58 5 4\n",
      "pre_time 59 5 5\n",
      "pre_time 60 4 4\n",
      "pre_time 61 6 5\n",
      "pre_time 62 8 6\n",
      "pre_time 63 9 8\n",
      "pre_time 64 6 3\n",
      "pre_time 65 4 3\n",
      "pre_time 66 5 2\n",
      "pre_time 67 9 11\n",
      "pre_time 68 10 6\n",
      "pre_time 69 10 7\n",
      "pre_time 70 13 9\n",
      "pre_time 71 15 12\n",
      "pre_time 72 6 7\n",
      "pre_time 73 8 8\n",
      "pre_time 74 6 4\n",
      "pre_time 75 10 10\n",
      "pre_time 76 11 4\n",
      "pre_time 77 8 8\n",
      "pre_time 78 14 11\n",
      "pre_time 79 18 9\n",
      "pre_time 80 18 6\n",
      "pre_time 81 9 3\n",
      "pre_time 82 7 4\n",
      "pre_time 83 13 8\n",
      "pre_time 84 6 2\n",
      "pre_time 85 6 8\n",
      "pre_time 86 7 4\n",
      "pre_time 87 11 7\n",
      "pre_time 88 4 2\n",
      "pre_time 89 6 3\n",
      "pre_time 90 4 2\n",
      "pre_time 91 7 3\n",
      "pre_time 92 8 6\n",
      "pre_time 93 8 1\n",
      "pre_time 94 7 4\n",
      "pre_time 95 6 4\n",
      "pre_time 96 12 5\n",
      "pre_time 97 6 8\n",
      "pre_time 98 10 5\n",
      "pre_time 99 8 3\n",
      "pre_time 100 12 2\n",
      "pre_time 101 11 12\n",
      "pre_time 102 16 7\n",
      "pre_time 103 8 5\n",
      "pre_time 104 6 7\n",
      "pre_time 105 7 4\n",
      "pre_time 106 7 2\n",
      "pre_time 107 4 4\n",
      "pre_time 108 5 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(date_idx):\n",
    "    pred = out[0,i,:] > 0.\n",
    "    label = x[0,i+1,:] \n",
    "    print 'pre_time',i , torch.sum(pred).item(), torch.sum(label).item()\n",
    "# print torch.sigmoid(out[0,12,10:50])\n",
    "# print out[0,13,13]\n",
    "# print date_idx\n",
    "# torch.sum(x[0, date_idx+1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "torch.Size([5532])\n",
      "tensor(639, device='cuda:0') tensor(16., device='cuda:0')\n",
      "0.887020968908\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best-light.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "\n",
    "    for user in u_map.keys():\n",
    "        x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "        history = u_map[user]\n",
    "        ds = np.array([d for d,f in history])\n",
    "        fs = np.array([f for d,f in history])\n",
    "        sorted_idx = np.argsort(ds)\n",
    "        ds = ds[sorted_idx]\n",
    "        fd = fs[sorted_idx]\n",
    "\n",
    "        date_idx = 0\n",
    "        now_date = ds[0]\n",
    "        for food, date in zip(fs,ds):\n",
    "            if date != now_date:\n",
    "                date_idx+=1\n",
    "                now_date = date\n",
    "            x[date_idx, food_map[food]] = 1\n",
    "        x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "        out = model(x)\n",
    "\n",
    "        hit_idx = date_idx-1\n",
    "#         hit_idx = 3\n",
    "        arr = out.cpu().numpy()[0,hit_idx,:]\n",
    "        pred = out[0,hit_idx,:] > 0.5\n",
    "        \n",
    "        ans = x[0,hit_idx+1,:]\n",
    "        break\n",
    "                \n",
    "\n",
    "print 'done'\n",
    "print pred.shape\n",
    "print torch.sum(pred), torch.sum(ans)\n",
    "buf = ans.type(torch.uint8) == pred\n",
    "print float(torch.sum(buf)) / float(5532)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 165, 5532) (32, 165, 5532)\n",
      "(16, 165, 5532) (16, 165, 5532)\n",
      "(165, 5532)\n",
      "[31.8 32.1 32.4 32.7 33.  33.3 33.6 33.9 34.2 34.5 34.8 35.1 35.4 35.7\n",
      " 36.  36.3 36.6 36.9 37.2 37.5 37.8 38.1 38.4 38.7 39.  39.3 39.6 39.9\n",
      " 40.2 40.5 40.8 41.1 41.4 41.7 42.  42.3 42.6 42.9 43.2 43.5 43.8 44.1\n",
      " 44.4 44.7 45.  45.3 45.6 45.9 46.2 46.5 46.8 47.1 47.4 47.7 48.  48.3\n",
      " 48.6 48.9 49.2 49.5  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. ]\n"
     ]
    }
   ],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "\n",
    "idx = np.random.permutation(len(u_map))\n",
    "val_num = len(u_map)//10\n",
    "train_idx, val_idx = idx[val_num:], idx[:val_num]\n",
    "train_u_map = {k:u_map[k] for k in u_map.keys()[val_num:]}\n",
    "val_u_map = {k:u_map[k] for k in u_map.keys()[:val_num]}\n",
    "def batch_boostrap_generator(batch_size, u_map, food_map, max_history_len):\n",
    "    G = boostrap_generator(u_map, food_map, max_history_len)\n",
    "    while True:\n",
    "        X = []\n",
    "        pad_masks = []\n",
    "        for i in range(batch_size):\n",
    "            x, x_len = next(G)\n",
    "            X.append(np.expand_dims(x, axis=0))\n",
    "            pad_mask = np.zeros_like(x)\n",
    "            for idx in range(x_len):\n",
    "                pad_mask[idx, :] = (max_history_len-x_len+idx+1)*0.3\n",
    "            pad_masks.append(np.expand_dims(pad_mask, axis=0))\n",
    "        yield np.vstack(X), np.vstack(pad_masks)\n",
    "def boostrap_generator(u_map, food_map, max_history_len):\n",
    "    while True:\n",
    "        keys = u_map.keys()\n",
    "        for user_idx in np.random.permutation(len(u_map)):\n",
    "            user = keys[user_idx]\n",
    "            X = np.zeros([max_history_len, len(food_map)])\n",
    "            history = u_map[user]\n",
    "            ds = np.array([d for d,f in history])\n",
    "            fs = np.array([f for d,f in history])\n",
    "            sorted_idx = np.argsort(ds)\n",
    "            ds = ds[sorted_idx]\n",
    "            fd = fs[sorted_idx]\n",
    "            \n",
    "            date_idx = 0\n",
    "            now_date = ds[0]\n",
    "            for food, date in zip(fs,ds):\n",
    "                if date != now_date:\n",
    "                    date_idx+=1\n",
    "                    now_date = date\n",
    "                X[date_idx, food_map[food]] = 1\n",
    "            x_len = date_idx+1\n",
    "            yield X, x_len\n",
    "            \n",
    "    \n",
    "\n",
    "G = batch_boostrap_generator(32, train_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "val_G = batch_boostrap_generator(32//2, val_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "\n",
    "x, pad_mask = next(G)\n",
    "print x.shape, pad_mask.shape\n",
    "x, pad_mask = next(val_G)\n",
    "print x.shape, pad_mask.shape\n",
    "\n",
    "G2 = boostrap_generator(train_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "x, x_len = next(G2)\n",
    "print x.shape\n",
    "print pad_mask[3,:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start testing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 144/100000000 [00:04<611:27:12, 45.43it/s, val_acc : 0.820]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-872b0016acf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mval_acc_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc_q\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-872b0016acf3>\u001b[0m in \u001b[0;36mnormal_acc\u001b[0;34m(pred, label, pad_mask)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "def normal_acc(pred, label, pad_mask):\n",
    "    label = label.type(torch.uint8)\n",
    "    buf = pred == label\n",
    "    mask = torch.ones_like(pad_mask, dtype=torch.uint8)\n",
    "    buf = buf*mask\n",
    "    buf = buf.masked_select(label)\n",
    "    \n",
    "    acc = torch.sum(buf).item() / float(torch.sum(label).item())\n",
    "    \n",
    "    return acc\n",
    "def rev_mask(m):\n",
    "    out = torch.ones_like(m, dtype=torch.uint8, requires_grad=False)\n",
    "    out.masked_fill_(m, 0)\n",
    "    return out\n",
    "        \n",
    "val_acc_q = deque(maxlen=10000)\n",
    "val_loss_q = deque(maxlen=10000)\n",
    "\n",
    "batch_size = 16\n",
    "val_G = batch_boostrap_generator(batch_size, val_u_map, food_map, max_history_len=MAX_SEQ_LEN)\n",
    "print 'start testing.'\n",
    "iters = 100000000\n",
    "with tqdm(total=iters) as pbar:\n",
    "    for it in range(iters):\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            seq, pad_mask = next(val_G)\n",
    "            seq = torch.FloatTensor(seq).cuda()\n",
    "            pad_mask = torch.FloatTensor(pad_mask).cuda()\n",
    "            seq.requires_grad_(False)\n",
    "            pad_mask.requires_grad_(False)\n",
    "\n",
    "            x = seq[:,:-1,:]\n",
    "            y = seq[:,1:,:]\n",
    "            output = model(x)\n",
    "            pred = output > 0.5\n",
    "\n",
    "            label = y\n",
    "\n",
    "            val_acc = normal_acc(pred, label, pad_mask[:,1:,:])\n",
    "            val_acc_q.append(val_acc)\n",
    "        val_acc = np.mean(val_acc_q)\n",
    "\n",
    "        pbar.set_postfix_str('val_acc : %.3f' % (val_acc), refresh=False)\n",
    "        pbar.update(batch_size)\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 164, 5532])\n",
      "torch.Size([16, 164, 5532])\n",
      "torch.Size([164, 5532])\n",
      "torch.Size([164, 5532])\n",
      "tensor([1, 1, 1,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)\n",
      "tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)\n",
      "tensor([0, 0, 0,  ..., 1, 1, 1], device='cuda:0', dtype=torch.uint8)\n",
      "0.805676066522\n",
      "torch.Size([5532])\n",
      "tensor(1096, device='cuda:0') tensor(23, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print pred.shape\n",
    "print label.shape\n",
    "p = pred[0,:,:]\n",
    "l = label[0,:,:]\n",
    "print p.shape\n",
    "print l.shape\n",
    "a = p[33,:]\n",
    "b = l[33,:].type(torch.uint8)\n",
    "print a\n",
    "print b\n",
    "print a==b\n",
    "print torch.sum(a == b).item() / float(5532)\n",
    "print a.shape\n",
    "print torch.sum(a), torch.sum(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
