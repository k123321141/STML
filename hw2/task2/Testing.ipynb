{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681494/2681494 [00:21<00:00, 126967.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 5532)\n",
      "(5532,)\n",
      "(5532,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as  np\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "np.random.seed(1337)\n",
    "with open('./kaggle/rating_train.csv', 'r') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "u_map = {}\n",
    "\n",
    "dates = []\n",
    "foods = []\n",
    "users = []\n",
    "    \n",
    "\n",
    "\n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        date_str, user, food = l.strip().split(',')\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        user, food = int(user), int(food)\n",
    "        if user not in u_map:\n",
    "            u_map[user] = []\n",
    "        u_map[user].append( (date, food) )\n",
    "        \n",
    "        dates.append(date)\n",
    "        users.append(user)\n",
    "        foods.append(food)\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "user_map = {u:i for i, u in enumerate(set(users))}        \n",
    "food_map = {f:i for i, f in enumerate(set(foods))}\n",
    "\n",
    "\n",
    "# for ranking sparse matrix\n",
    "rows = [user_map[u] for u in users]\n",
    "cols = [food_map[f] for f in foods]\n",
    "R = csr_matrix((np.ones([len(rows), ]), (rows, cols)), shape=(len(user_map), len(food_map)))\n",
    "\n",
    "pos_count = np.array(np.sum(R, axis=0)).flatten()\n",
    "neg_count = len(ls) - pos_count\n",
    "\n",
    "class_weight =  1. / pos_count\n",
    "final_weight = neg_count*class_weight\n",
    "pos_weight = neg_count / pos_count\n",
    "print R.shape\n",
    "print neg_count.shape\n",
    "print pos_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 200 \n",
      "torch.Size([3, 13, 128])\n",
      "torch.Size([3, 47, 128])\n",
      "1189632\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "# construct neuron network\n",
    "\n",
    "def scaled_dot_attention(Q, K, V, mask):\n",
    "    assert Q.size()[-1] == K.size()[-1]\n",
    "    assert len(Q.size()) == 3 and len(K.size()) == 3 and len(V.size()) == 3\n",
    "    dk = torch.tensor(K.size()[-1], dtype=torch.float32, requires_grad=False).cuda()\n",
    "    out = torch.matmul(Q,K.permute(0,2,1)) / torch.sqrt(dk) \n",
    "    if mask is not None:\n",
    "        out.masked_fill_(mask, -float('inf'))\n",
    "    return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "\n",
    "def positional_encoding(d_model, pos):\n",
    "    assert d_model % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,d_model], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(D_MODEL//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(D_MODEL), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe\n",
    "                            \n",
    "class Transformer_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask, use_cuda=True, posi_cache_length=200):\n",
    "        super(Transformer_v2, self).__init__()\n",
    "#         for construct cache positional encoding matrix.\n",
    "        self.d_model = dm\n",
    "        self.use_cuda = use_cuda\n",
    "        \n",
    "        self.decoder = Stack_Decoder(layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask)\n",
    "        self.emb_drop = nn.Dropout(p_drop)\n",
    "        self.init_pos_mat(posi_cache_length)\n",
    "\n",
    "    def forward(self, Q):\n",
    "    \n",
    "        \n",
    "#         decoder\n",
    "        batch, Q_len, d = Q.size()\n",
    "        \n",
    "        try:\n",
    "            Q = Q + self.get_pos_mat(Q_len)\n",
    "        except RuntimeError, e:\n",
    "            if e.message == 'TensorIterator expected type torch.cuda.FloatTensor but got torch.FloatTensor':\n",
    "                if Q.is_cuda != self.get_pos_mat(K_len).is_cuda:\n",
    "                    print('Make sure cache positional matrix is same type of tensor with input, both cuda tensor or not.\\nBy setting argument use_cuda=True to set cache positional encoding matrix as a cuda tensor.')\n",
    "            raise\n",
    "        \n",
    "        Q = self.emb_drop(Q)\n",
    "        \n",
    "        de_out = self.decoder(Q)\n",
    "        return de_out\n",
    "    \n",
    "#     To speed up the positional encoding by construct an cache matrix. \n",
    "    def init_pos_mat(self, cache_length):\n",
    "        print('init postional matrix with length : %d ' % cache_length)\n",
    "        self.positional_matrix = torch.cat([positional_encoding(self.d_model, i) for i in range(0,cache_length)], dim=0)\n",
    "        self.positional_matrix.requires_grad = False\n",
    "        if self.use_cuda:\n",
    "            self.positional_matrix = self.positional_matrix.cuda()\n",
    "            \n",
    "        \n",
    "    def get_pos_mat(self, length):\n",
    "        if length > self.positional_matrix.shape[0]:\n",
    "            print('input sequence length reach positional matrix maximum length. %d ' % length)\n",
    "            ret = torch.cat([positional_encoding(self.d_model, i) for i in range(length)], dim=0)\n",
    "            ret.requires_grad = False\n",
    "            print('Increase positional matrix maximum length. %d ' % length)\n",
    "            self.positional_matrix = ret\n",
    "            if self.use_cuda:\n",
    "                self.positional_matrix = self.positional_matrix.cuda()\n",
    "            return ret\n",
    "        else:\n",
    "            return self.positional_matrix[:length]\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class Stack_Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Stacked Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_num, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Stack_Decoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([Decoder(dk, dv, dm, h, p_drop, d_ff, use_mask) for i in range(layer_num)])\n",
    "        \n",
    "        \n",
    "    def forward(self, Q):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for lay in self.decoders:\n",
    "            Q = lay(Q)\n",
    "        return Q           \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h, p_drop, d_ff, use_mask):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.use_mask = use_mask\n",
    "        \n",
    "#         query attention residual block\n",
    "        self.Q_attention_lay = Multi_Head_attention_layer(dk, dv, dm, h)\n",
    "        self.Q_attention_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.Q_att_drop = nn.Dropout(p_drop)\n",
    "\n",
    "#         feed forward residual block\n",
    "        self.fcn = PositionwiseFeedForward(dm, d_ff)\n",
    "        self.ff_norm_lay = nn.LayerNorm([dm, ])\n",
    "        self.linear_drop = nn.Dropout(p_drop)\n",
    "        \n",
    "\n",
    "    def forward(self, Q):\n",
    "        if self.use_mask:\n",
    "            batch, Q_len, d = Q.size()\n",
    "            mask = self.mask_matrix(batch, Q_len)\n",
    "        else:\n",
    "            mask = None\n",
    "#         query attention\n",
    "        Q_attention_out = self.Q_attention_lay(Q, Q, Q, mask=mask)\n",
    "        Q_attention_out = self.Q_att_drop(Q_attention_out)\n",
    "        Q_att_out = self.Q_attention_norm_lay(Q + Q_attention_out)\n",
    "        \n",
    "#         feed forward\n",
    "        linear_out = self.fcn(Q_att_out)\n",
    "        out = self.ff_norm_lay(Q_att_out + linear_out)\n",
    "        return out\n",
    "    def mask_matrix(self, batch, Q_len):\n",
    "#         ByteTensor\n",
    "        mask = torch.zeros([1, Q_len, Q_len], dtype=torch.uint8, requires_grad=False)\n",
    "        for i in range(Q_len):\n",
    "            mask[0,i,i+1:] = 1\n",
    "        return mask.repeat(batch,1, 1).cuda()\n",
    "\n",
    "\n",
    "class Multi_Head_attention_layer(nn.Module):\n",
    "    def __init__(self, dk, dv, dm, h):\n",
    "        super(Multi_Head_attention_layer, self).__init__()\n",
    "        self.Q_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.K_linears = nn.ModuleList([nn.Linear(dm, dk) for i in range(h)])\n",
    "        self.V_linears = nn.ModuleList([nn.Linear(dm, dv) for i in range(h)])\n",
    "        self.output_linear = nn.Linear(h*dv, dm)\n",
    "                            \n",
    "\n",
    "    def forward(self, Q_input, K_input, V_input, mask):\n",
    "        buf = []\n",
    "        for Q_linear, K_linear, V_linear in zip(self.Q_linears, self.K_linears, self.V_linears):\n",
    "            Q = Q_linear(Q_input)\n",
    "            K = K_linear(K_input)\n",
    "            V = V_linear(V_input)\n",
    "            buf.append(scaled_dot_attention(Q, K, V, mask))\n",
    "        \n",
    "        buf = torch.cat(buf,dim=-1)\n",
    "        out = self.output_linear(buf)\n",
    "        \n",
    "        return out      \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.cnn1 = nn.Conv1d(d_model, d_ff, 1)\n",
    "        self.cnn2 = nn.Conv1d(d_ff, d_model, 1)\n",
    "                            \n",
    "\n",
    "    def forward(self, x):\n",
    "        bat,seq_len,d = x.size()\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.cnn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        return x      \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# Transformer paper baseline hyper-parameters\n",
    "STACKED_NUM = 6\n",
    "H = 8\n",
    "D_MODEL = 128\n",
    "DK = DV = D_MODEL//H\n",
    "P_DROP = 0.1\n",
    "D_FF = D_MODEL*4\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "bat = 3\n",
    "Q = torch.rand([bat, 13, D_MODEL]).cuda()\n",
    "model = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "o = model(Q)\n",
    "print(o.size())\n",
    "\n",
    "Q = torch.rand([bat, 47, D_MODEL]).cuda()\n",
    "o = model(Q)\n",
    "print(o.size())\n",
    "# # print o\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 200 \n",
      "torch.Size([7, 18, 5532])\n",
      "2627996\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F### Transformer with ALS embedding Training\n",
    "# import Transformer/\n",
    "\n",
    "import numpy as np\n",
    "from constants import FOOD_NUM, USER_NUM\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, p_drop):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.food_emb = Food_embedding(FOOD_NUM, dm, 2, p_drop)\n",
    "        self.transformer = Transformer_v2(STACKED_NUM, DK, DV, D_MODEL, H, P_DROP, D_FF, use_mask=True, use_cuda=True).cuda()\n",
    "\n",
    "        self.output_linear = nn.Linear(dm, FOOD_NUM)\n",
    "\n",
    "    def forward(self, history):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        \n",
    "        x = self.food_emb(history)\n",
    "        batch, x_len, d = x.size()\n",
    "        \n",
    "        x = self.transformer(x)\n",
    "        x = self.output_linear(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "class Food_embedding(nn.Module):\n",
    "    def __init__(self, c_in, dm, layer_num, p_drop, activation_fn=F.selu):\n",
    "        super(Food_embedding, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        assert layer_num >= 1\n",
    "        self.first_linear = nn.Linear(c_in, dm)\n",
    "        self.linears = nn.ModuleList([nn.Linear(dm, dm) for i in range(layer_num-1)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        x = self.first_linear(x)\n",
    "        for lay in self.linears:\n",
    "            x = self.activation_fn(lay(x))\n",
    "            if lay != self.linears[-1]:\n",
    "                x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "batch = 7\n",
    "dm = D_MODEL\n",
    "Q = torch.rand([batch, 18, FOOD_NUM]).cuda()\n",
    "model = Net(dm, 0.1).cuda()\n",
    "o = model(Q)\n",
    "# print t\n",
    "print(o.size())\n",
    "# print o\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/backends/cudnn/__init__.py:102: UserWarning: PyTorch was compiled without cuDNN support. To use cuDNN, rebuild PyTorch making sure the library is visible to the build system.\n",
      "  \"PyTorch was compiled without cuDNN support. To use cuDNN, rebuild \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 18, 5532])\n",
      "1636508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/nn/modules/rnn.py:179: RuntimeWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F### Transformer with ALS embedding Training\n",
    "# import Transformer/\n",
    "\n",
    "import numpy as np\n",
    "from constants import FOOD_NUM, USER_NUM\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, p_drop):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.food_emb = Food_embedding(FOOD_NUM, dm, 2, p_drop)\n",
    "        self.rnn_lay_num = 2\n",
    "        self.rnn = nn.GRU(dm, dm, self.rnn_lay_num)\n",
    "        self.output_linear = nn.Linear(dm, FOOD_NUM)\n",
    "\n",
    "    def forward(self, history):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        \n",
    "        x = self.food_emb(history)\n",
    "        batch, x_len, d = x.size()\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        h0 = torch.zeros([self.rnn_lay_num, batch, d], requires_grad=False).cuda()\n",
    "        rnn_out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        x = rnn_out.permute(1,0,2)\n",
    "        x = self.output_linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "class Food_embedding(nn.Module):\n",
    "    def __init__(self, c_in, dm, layer_num, p_drop, activation_fn=F.selu):\n",
    "        super(Food_embedding, self).__init__()\n",
    "        self.activation_fn = activation_fn\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        assert layer_num >= 1\n",
    "        self.first_linear = nn.Linear(c_in, dm)\n",
    "        self.linears = nn.ModuleList([nn.Linear(dm, dm) for i in range(layer_num-1)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        x = self.first_linear(x)\n",
    "#         x = self.activation_fn(x)\n",
    "        x = self.drop(x)\n",
    "        for lay in self.linears:\n",
    "            x = self.drop(self.activation_fn(lay(x)))\n",
    "        return x\n",
    "    \n",
    "batch = 7\n",
    "dm = 128\n",
    "K = torch.rand([batch, 18, FOOD_NUM]).cuda()\n",
    "model = Net(dm, 0.1).cuda()\n",
    "o = model(K)\n",
    "# print t\n",
    "print(o.size())\n",
    "# print o\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:21<00:00, 125.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "model = torch.load('./best2.pt')\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "rev_food_map = {v:k for k,v in food_map.items()}\n",
    "with torch.no_grad():\n",
    "    with open('predict.csv', 'w') as f_out:\n",
    "        f_out.write('userid,foodid\\n')\n",
    "\n",
    "        with tqdm(total=len(u_map)) as pbar:\n",
    "            for user in u_map.keys():\n",
    "                x = np.zeros([MAX_SEQ_LEN, len(food_map)])\n",
    "                history = u_map[user]\n",
    "                ds = np.array([d for d,f in history])\n",
    "                fs = np.array([f for d,f in history])\n",
    "                sorted_idx = np.argsort(ds)\n",
    "                ds = ds[sorted_idx]\n",
    "                fd = fs[sorted_idx]\n",
    "\n",
    "                date_idx = 0\n",
    "                now_date = ds[0]\n",
    "                for food, date in zip(fs,ds):\n",
    "                    if date != now_date:\n",
    "                        date_idx+=1\n",
    "                        now_date = date\n",
    "                    x[date_idx, food_map[food]] = 1\n",
    "                x = torch.FloatTensor(x).unsqueeze(0).cuda()\n",
    "                out = model(x)\n",
    "\n",
    "                arr = out.cpu().numpy()[0,date_idx,:]\n",
    "                k20 = reversed(np.argsort(arr)[-20:])\n",
    "                s = ''\n",
    "                for food_idx in k20:\n",
    "                    s += ' %d' % rev_food_map[food_idx]\n",
    "                f_out.write('%d,%s\\n' % (user, s) )\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "\n",
    "print 'done'\n",
    "\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
