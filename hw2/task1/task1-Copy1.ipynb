{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rating csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2681494/2681494 [00:20<00:00, 129751.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as  np\n",
    "from tqdm import tqdm\n",
    "np.random.seed(1337)\n",
    "with open('./kaggle/rating_train.csv', 'r') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "u_map = {}\n",
    "\n",
    "dates = []\n",
    "foods = []\n",
    "users = []\n",
    "    \n",
    "\n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        date_str, user, food = l.strip().split(',')\n",
    "        date = datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        user, food = int(user), int(food)\n",
    "        if user not in u_map:\n",
    "            u_map[user] = []\n",
    "        u_map[user].append( (date, food) )\n",
    "        \n",
    "        dates.append(date)\n",
    "        foods.append(food)\n",
    "        users.append(user)\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "        \n",
    "food_map = {f:i for i, f in enumerate(set(foods))}\n",
    "user_map = {u:i for i, u in enumerate(set(users))}\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "# normalize datetime\n",
    "# min_date = min(dates)\n",
    "# days = [(d-min_date).total_seconds() / (60*60*24) for d in dates]\n",
    "# mean_days, std_days = np.mean(days), np.std(days)\n",
    "# def normalize_date(min_date, mean_days, std_days, date):\n",
    "#     days = (date-min_date).total_seconds() / (60*60*24)\n",
    "#     return (days-mean_days)/std_days\n",
    "# with tqdm(total=len(u_map)) as pbar:\n",
    "#     for _,v in u_map.items():\n",
    "#         for i, pair in enumerate(v):\n",
    "#             date, food = pair\n",
    "#             v[i] = (normalize_date(min_date, mean_days, std_days, date), food)\n",
    "#         pbar.update(1)\n",
    "        \n",
    "# item-user matrix, for ALS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 23 95.21165644171779 34.48409857182605\n",
      "5569 194 1028.1802147239264 599.6642539977294\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for k,v in u_map.items():\n",
    "    ds = [d for d,f in v]\n",
    "    \n",
    "    set_ds = set(ds)\n",
    "    a.append(len(set_ds))\n",
    "    b.append(len(ds))\n",
    "print max(a), min(a),np.mean(a), np.std(a)\n",
    "print max(b), min(b),np.mean(b), np.std(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2608/2608 [00:01<00:00, 1505.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### ALS embeddingimport numpy as  np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "with tqdm(total=len(u_map)) as pbar:\n",
    "    \n",
    "#     split 1/10 for future\n",
    "    Y_map = {}\n",
    "    pairs = []\n",
    "    history_map = {} # for training and validation\n",
    "    for user,v in u_map.items():\n",
    "        history = v\n",
    "        dates_history = [d for d,f in history]\n",
    "        min_date, max_date = min(dates_history), max(dates_history)\n",
    "        date_list = sorted(set(dates_history))\n",
    "        rest_date_set = set(date_list[-len(date_list)//10:])\n",
    "        \n",
    "        \n",
    "        \n",
    "        history_map[k] = [d for d,f in history if d not in rest_date_set]\n",
    "#         calculate the Y\n",
    "        y = np.zeros([1, len(food_map)])\n",
    "        for d,f in history:\n",
    "            if d in rest_date_set:\n",
    "                y[0, food_map[f]] = 1\n",
    "            else:\n",
    "                pair = (food_map[f], user_map[user])\n",
    "                pairs.append(pair)\n",
    "        Y_map[user] = y\n",
    "        pbar.update(1)\n",
    "\n",
    "        \n",
    "    rows = []\n",
    "    cols = []\n",
    "    pairs = set(pairs)\n",
    "    for pair in pairs:\n",
    "        f, u = pair\n",
    "        rows.append(f)\n",
    "        cols.append(u)\n",
    "    data = np.ones([len(rows),])\n",
    "    item_user_matrix = csr_matrix((data, (rows,cols)), shape=(len(food_map), len(user_map)))\n",
    "    \n",
    "print len(Y_map)    \n",
    "        \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100.0/100 [00:28<00:00,  3.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import implicit\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = '1'\n",
    "als_model = implicit.als.AlternatingLeastSquares(factors=128, iterations=100)\n",
    "als_model.fit(item_user_matrix)\n",
    "print len(rows)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer with ALS embedding Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([7, 1])\n",
      "3694593\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import Transformer\n",
    "import numpy as np\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, p_drop):\n",
    "        super(Net, self).__init__()\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        \n",
    "        self.transformer = Transformer.Transformer(2, dm, dm, dm, 8, p_drop, d_ff=dm*4)\n",
    "        self.summary_weight = nn.Parameter(torch.FloatTensor(1, dm))\n",
    "        torch.nn.init.xavier_uniform_(self.summary_weight)\n",
    "        \n",
    "        self.output_linear = nn.Linear(dm, 1)\n",
    "\n",
    "    def forward(self, food, history):\n",
    "#         print(K.size(), get_pos_mat(MAX_SEQUENCE_LENGTH).size())\n",
    "        Q = food\n",
    "        K = history\n",
    "        \n",
    "        batch, K_len, d = K.size()\n",
    "        Q = self.summary_weight.repeat(batch,1).view(batch, 1, -1)\n",
    "        batch, Q_len, d = Q.size()\n",
    "        Q_mask_len = np.ones([batch])\n",
    "        de_out = self.transformer(Q, K, Q_mask_len)\n",
    "        de_out = de_out.view(batch, d)\n",
    "        \n",
    "        out = self.output_linear(de_out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "batch = 7\n",
    "dm = 128\n",
    "K = torch.rand([batch, 18, dm]).cuda()\n",
    "Q = torch.rand([dm]).cuda()\n",
    "model = Net(dm, 0.1).cuda()\n",
    "print(Q.dtype)\n",
    "o = model(Q, K)\n",
    "# print t\n",
    "print(o.size())\n",
    "# print o\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare sample generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2608, 128) (5532, 128)\n",
      "(32, 5600, 128) (32, 128) (32, 128)\n",
      "(16, 5600, 128) (16, 128) (16, 128)\n",
      "(498, 128) (128,) (128,)\n"
     ]
    }
   ],
   "source": [
    "user_emb = als_model.user_factors\n",
    "food_emb = als_model.item_factors\n",
    "print user_emb.shape, food_emb.shape\n",
    "\n",
    "val_num = len(u_map) // 10\n",
    "idx = np.random.permutation(len(u_map))\n",
    "train_idx, val_idx = idx[val_num:], idx[:val_num]\n",
    "train_u_map = {k:u_map[k] for k in u_map.keys()[val_num:]}\n",
    "val_u_map = {k:u_map[k] for k in u_map.keys()[:val_num]}\n",
    "def batch_boostrap_generator(batch_size, u_map, food_map, Y_map, food_emb, max_history_len):\n",
    "    G = boostrap_generator(u_map, food_map, Y_map, food_emb, max_history_len)\n",
    "    while True:\n",
    "        X = []\n",
    "        pos_Y = []\n",
    "        neg_Y = []\n",
    "        for i in range(batch_size):\n",
    "            x, pos_y, neg_y = next(G)\n",
    "            x = np.pad(x, ((0,max_history_len-x.shape[0]),(0,0)), 'constant', constant_values=0)\n",
    "            X.append(np.expand_dims(x, axis=0))\n",
    "            pos_Y.append(np.expand_dims(pos_y, axis=0))\n",
    "            neg_Y.append(np.expand_dims(neg_y, axis=0))\n",
    "        yield np.vstack(X), np.vstack(pos_Y), np.vstack(neg_Y) \n",
    "def boostrap_generator(u_map, food_map, Y_map, food_emb, max_history_len):\n",
    "    while True:\n",
    "        keys = u_map.keys()\n",
    "        for user_idx in np.random.permutation(len(u_map)):\n",
    "            user = keys[user_idx]\n",
    "            X = []\n",
    "            Y = Y_map[user].flatten()\n",
    "            history = u_map[user]\n",
    "            ds = [d for d,f in history]\n",
    "            fs = [f for d,f in history]\n",
    "            for idx in np.argsort(ds):\n",
    "                food = fs[idx]\n",
    "                x = food_emb[food_map[food], :]\n",
    "                X.append(np.expand_dims(x, axis=0))\n",
    "            X = np.vstack(X)\n",
    "            \n",
    "    #         positive sample\n",
    "            idx = np.random.permutation(len(Y))\n",
    "            pos_i = neg_i = -1\n",
    "            for i in idx:\n",
    "                if Y[i] == 1 and pos_i == -1:\n",
    "                    pos_i = i\n",
    "                if Y[i] == 0 and neg_i == -1:\n",
    "                    neg_i = i\n",
    "                if pos_i != -1 and neg_i != -1:\n",
    "                    break\n",
    "            yield X, food_emb[pos_i], food_emb[neg_i]\n",
    "            \n",
    "    \n",
    "\n",
    "G = batch_boostrap_generator(32, train_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "val_G = batch_boostrap_generator(32//2, val_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "\n",
    "x, pos_y, neg_y = next(G)\n",
    "print x.shape, pos_y.shape, neg_y.shape\n",
    "x, pos_y, neg_y = next(val_G)\n",
    "print x.shape, pos_y.shape, neg_y.shape\n",
    "\n",
    "G = boostrap_generator(train_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "x, pos_y, neg_y = next(G)\n",
    "print x.shape, pos_y.shape, neg_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/14427456 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 346/14427456 [01:04<1086:38:02,  3.69it/s, acc : 0.488, val_acc : 0.500, 0.578, 0.574, 0.574]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b5ace97fde23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#                 negative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-a76467f1885f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, food, history)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mQ_mask_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mde_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_mask_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mde_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mde_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw2/task1/Transformer.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q, K, Q_mask_len)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memb_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0men_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m#         decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw2/task1/Transformer.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, K)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# ModuleList can act as an iterable, or be indexed using ints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlay\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw2/task1/Transformer.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, K)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;31m#         attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mattention_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_head_attention_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mattention_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0matt_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_norm_lay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.12/local/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw2/task1/Transformer.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Q_input, K_input, V_input, mask)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_dot_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/git/STML/hw2/task1/Transformer.pyc\u001b[0m in \u001b[0;36mscaled_dot_attention\u001b[0;34m(Q, K, V, mask)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mdk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm as tqdm\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, acc, val_loss, val_acc, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f\\n' % (n_iter, loss, acc, val_loss, val_acc)\n",
    "    log_file_stream.write(log_text)\n",
    "    if n_iter % 100 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "        \n",
    "acc_q = deque(maxlen=1000)\n",
    "loss_q = deque(maxlen=1000)\n",
    "val_acc_q = deque(maxlen=1000)\n",
    "val_loss_q = deque(maxlen=1000)\n",
    "t = time.time()\n",
    "best_acc  = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "G = boostrap_generator(train_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "val_G = boostrap_generator(val_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "\n",
    "print 'start training.'\n",
    "with open('log.txt', 'a') as f:\n",
    "    for e in range(epochs):\n",
    "        iters = len(food_map) * len(user_map)\n",
    "        with tqdm(total=iters) as pbar:\n",
    "            for it in range(iters):\n",
    "                optimizer.zero_grad()\n",
    "                model.train()\n",
    "                x, pos_y, neg_y = next(G)\n",
    "                x, pos_y, neg_y = np.expand_dims(x, axis=0),np.expand_dims(pos_y, axis=0),np.expand_dims(neg_y, axis=0)\n",
    "                x = torch.FloatTensor(x).cuda()\n",
    "                pos_y,neg_y = torch.LongTensor(pos_y), torch.LongTensor(neg_y)\n",
    "                \n",
    "                if np.random.rand(1) > 0.5:\n",
    "#                 positive\n",
    "                    a=output = model(pos_y.cuda(), x)\n",
    "                    pred = output.flatten() > 0.5\n",
    "                    label = torch.ones_like(output).cuda()\n",
    "                    pos_loss = criterion(output, label)\n",
    "\n",
    "                    acc = torch.sum(pred == label.type(torch.uint8)) / float(output.shape[0])\n",
    "                    acc_q.append(acc)\n",
    "                    pos_loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "\n",
    "#                 negative\n",
    "                    a=output = model(neg_y.cuda(), x)\n",
    "                    pred = output.flatten() > 0.5\n",
    "                    label = torch.zeros_like(output).cuda()\n",
    "                    neg_loss = criterion(output, label )\n",
    "                    acc = torch.sum(pred == label.type(torch.uint8)) / float(output.shape[0])\n",
    "                    acc_q.append(acc)\n",
    "                    neg_loss.backward()\n",
    "#                     loss = pos_loss + neg_loss\n",
    "#                     loss.backward()\n",
    "                    optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "#                 positive\n",
    "                    x, pos_y, neg_y = next(val_G)\n",
    "                    x, pos_y, neg_y = np.expand_dims(x, axis=0),np.expand_dims(pos_y, axis=0),np.expand_dims(neg_y, axis=0)\n",
    "                    x = torch.FloatTensor(x).cuda()\n",
    "                    pos_y,neg_y = torch.LongTensor(pos_y), torch.LongTensor(neg_y)\n",
    "#                       \n",
    "                    b=output = model(pos_y.cuda(), x)\n",
    "                    pred = output.flatten() > 0.5\n",
    "                    label = torch.ones_like(output).cuda()\n",
    "                \n",
    "                    pos_loss = criterion(output, label )\n",
    "                \n",
    "                    val_acc = torch.sum(pred == label.type(torch.uint8)) / float(output.shape[0])\n",
    "                    val_acc_q.append(val_acc)\n",
    "\n",
    "#                 negative\n",
    "                    c=output = model(neg_y.cuda(), x)\n",
    "                    pred = output.flatten() > 0.5\n",
    "                    label = torch.zeros_like(output).cuda()\n",
    "        \n",
    "                    neg_loss = criterion(output, label )\n",
    "                    val_acc = torch.sum(pred == label.type(torch.uint8)) / float(output.shape[0])\n",
    "                    val_acc_q.append(val_acc)\n",
    "                    \n",
    "                    val_loss = pos_loss + neg_loss\n",
    "                acc = np.mean(acc_q)\n",
    "                val_acc = np.mean(val_acc_q)\n",
    "\n",
    "#                 dump_log(model, it+1, loss, acc, val_loss, val_acc, f, './tmp.pt')\n",
    "\n",
    "                pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, %.3f, %.3f, %.3f' % (acc, val_acc,a,b,c), refresh=False)\n",
    "                pbar.update(1)\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "tensor([[0]], dtype=torch.uint8)\n",
      "tensor([[0.]]) tensor([[0]], dtype=torch.uint8)\n",
      "tensor([[1]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[0.49]])\n",
    "b = torch.zeros_like(a)\n",
    "print a.shape\n",
    "print a > 0.5\n",
    "c = a > 0.5\n",
    "print b,c\n",
    "print b.type(torch.uint8) == c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "# import time\n",
    "# def dump_log(model, n_iter, loss, acc, val_loss, val_acc, log_file_stream, tmp_model_path):\n",
    "#     log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f\\n' % (n_iter, loss, acc, val_loss, val_acc)\n",
    "#     log_file_stream.write(log_text)\n",
    "#     if n_iter % 100 == 0 :\n",
    "#         log_file_stream.flush()\n",
    "#         torch.save(model, tmp_model_path)\n",
    "        \n",
    "# acc_q = deque(maxlen=1000)\n",
    "# loss_q = deque(maxlen=1000)\n",
    "# val_acc_q = deque(maxlen=1000)\n",
    "# val_loss_q = deque(maxlen=1000)\n",
    "# t = time.time()\n",
    "# best_acc  = 0\n",
    "# best_loss = float('inf')\n",
    "\n",
    "# epochs = 100\n",
    "# batch_size = 1\n",
    "# G = batch_boostrap_generator(batch_size, train_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "# val_G = batch_boostrap_generator(batch_size//2, val_u_map, food_map, Y_map, food_emb, max_history_len=5600)\n",
    "\n",
    "# print 'start training.'\n",
    "# with open('log.txt', 'a') as f:\n",
    "#     for e in range(epochs):\n",
    "#         iters = len(food_map) * len(user_map)\n",
    "#         for it in range(iters):\n",
    "#             with tqdm(total=iters) as pbar:\n",
    "\n",
    "#                 optimizer.zero_grad()\n",
    "#                 model.train()\n",
    "#                 x, pos_y, neg_y = next(G)\n",
    "#                 x = torch.FloatTensor(x).cuda()\n",
    "#                 pos_y,neg_y = torch.LongTensor(pos_y), torch.LongTensor(neg_y)\n",
    "# #                 positive\n",
    "#                 output = model(pos_y.cuda(), x)\n",
    "#                 label = torch.ones_like(output).cuda()\n",
    "#                 pos_loss = criterion(output, label )\n",
    "#                 acc = torch.sum(output.flatten() == label) / float(output.shape[0])\n",
    "#                 acc_q.append(acc)\n",
    "\n",
    "# #                 negative\n",
    "#                 output = model(neg_y.cuda(), x)\n",
    "#                 label = torch.zeros_like(output).cuda()\n",
    "#                 neg_loss = criterion(output, label )\n",
    "#                 acc = torch.sum(output.flatten() == label) / float(output.shape[0])\n",
    "#                 acc_q.append(acc)\n",
    "\n",
    "#                 loss = pos_loss + neg_loss\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 with torch.no_grad:\n",
    "#                     model.eval()\n",
    "# #                 positive\n",
    "#                     x, pos_y, neg_y = next(val_G)\n",
    "#                     output = model(pos_y.cuda(), x)\n",
    "#                     label = torch.ones_like(output).cuda()\n",
    "#                     pos_loss = criterion(output, label )\n",
    "                \n",
    "#                     val_acc = torch.sum(output.flatten() == label) / float(output.shape[0])\n",
    "#                     val_acc_q.append(val_acc)\n",
    "\n",
    "# #                 negative\n",
    "#                     output = model(neg_y.cuda(), x)\n",
    "#                     label = torch.zeros_like(output).cuda()\n",
    "#                     neg_loss = criterion(output, label )\n",
    "#                     val_acc = torch.sum(output.flatten() == label) / float(output.shape[0])\n",
    "#                     val_acc_q.append(val_acc)\n",
    "                    \n",
    "#                     val_loss = pos_loss + neg_loss\n",
    "#                 acc = np.mean(acc_q)\n",
    "#                 val_acc = np.mean(val_acc_q)\n",
    "\n",
    "#                 dump_log(model, it+1, loss, acc, val_loss, val_acc, f, './tmp.pt')\n",
    "\n",
    "#                 pbar.set_postfix_str('acc : %.3f, val_acc : %.3f' % (acc, val_acc), refresh=False)\n",
    "#                 pbar.update(1)\n",
    "\n",
    "# # Train model\n",
    "# print(\"Optimization Finished!\")\n",
    "# # print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "salnj;kvahjk\n",
    "# print u_map[6]\n",
    "buf  = [] \n",
    "def ck(ds):\n",
    "    l = max(ds) - min(ds)\n",
    "    min_d = min(ds)\n",
    "    \n",
    "#     d_list = [(d-min_d) for d in ds]\n",
    "#     d_list = sorted(set(d_list))\n",
    "    d_list = sorted(set(ds))\n",
    "#     print l, len(d_list)\n",
    "    for i,d in enumerate(d_list):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        d_ = d_list[i-1]\n",
    "        if (d - d_).days.real != 1:\n",
    "            d_diff = (d - d_).days.real\n",
    "            buf.append(d_diff)\n",
    "#             print d_diff\n",
    "#             assert d_diff < 10\n",
    "def cf(ds):\n",
    "    l = max(ds) - min(ds)\n",
    "    min_d = min(ds)\n",
    "    \n",
    "#     d_list = [(d-min_d) for d in ds]\n",
    "#     d_list = sorted(set(d_list))\n",
    "    d_list = sorted(set(ds))\n",
    "    \n",
    "#     print l, len(d_list)\n",
    "    for i,d in enumerate(d_list):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        d_ = d_list[i-1]\n",
    "        if (d - d_).days.real != 1:\n",
    "            d_diff = (d - d_).days.real\n",
    "            buf.append(d_diff)\n",
    "#             print d_diff\n",
    "#             assert d_diff < 10\n",
    "                        \n",
    "    \n",
    "# for i in [6,8,12]:\n",
    "with tqdm(total=len(u_map)) as pbar:\n",
    "    buf = []\n",
    "    buf2 = []\n",
    "    for i in u_map.keys():\n",
    "        ds = [d for d,f in u_map[i]]\n",
    "        threshold = ((max(ds) - min(ds)).days.real * (4./5.))\n",
    "        min_d = min(ds)\n",
    "        fl = []\n",
    "        m = {}\n",
    "        for d,f in u_map[i]:\n",
    "            if (d - min_d).days.real > threshold:\n",
    "                fl.append(f)\n",
    "            if d not in m:\n",
    "                m[d] = []\n",
    "            m[d].append(f)\n",
    "        for k,v in m.items():    \n",
    "            buf2.append(len(set(v)))\n",
    "#         print len(u_map[i]), len(fl), len(set(fl))\n",
    "        buf.append(len(set(fl)))\n",
    "        #     print i, min(ds), max(ds), len(ds)\n",
    "#         ck(ds)\n",
    "        pbar.update(1)\n",
    "# d6 = [d for d,f in u_map[6]]\n",
    "# ck(d6)\n",
    "    \n",
    "#     print ck(ds)\n",
    "# print u_map[33]\n",
    "print np.mean(buf), np.std(buf)\n",
    "print np.mean(buf2), np.std(buf2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print min(dates), max(dates),  max(dates)- min(dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print min(dates)\n",
    "m = min(dates)\n",
    "print max(dates)\n",
    "a = dates[0]\n",
    "print dir(a - min(dates))\n",
    "print a\n",
    "# def normalize_date(min_date, max_date, date):\n",
    "# for k in u_map.keys()[:10]:\n",
    "#     print len(u_map[k])\n",
    "ds = [(d-m).total_seconds() / (60*60*24) for d in dates]\n",
    "# import numpy as np\n",
    "print np.mean(ds)\n",
    "print np.min(ds), np.max(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from os.path import join\n",
    "import os\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from constants import MAX_SEQUENCE_LENGTH, MAX_NUM_WORDS, EMBEDDING_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "def quote_title_abstract(xml_path):\n",
    "    with open(xml_path, 'r') as f:\n",
    "        data = f.read()\n",
    "    soup = BS(data)\n",
    "    title, abstract = soup.find('title').text, soup.find('abstract').text\n",
    "    return title.strip(), abstract.strip()\n",
    "\n",
    "# text preprocessing\n",
    "data_path = join('./','kaggle/')\n",
    "xml_dir = join(data_path, 't2-doc')\n",
    "xml_list = [f for f in os.listdir(xml_dir) if f.endswith('.xml')]\n",
    "# print(len(xml_list))\n",
    "\n",
    "\n",
    "texts = []\n",
    "\n",
    "for xml in xml_list:\n",
    "    path = join(xml_dir,xml)\n",
    "    title, abstract = quote_title_abstract(path)\n",
    "    text = title + '' + abstract\n",
    "    texts.append(text)\n",
    "#     texts.append(title)\n",
    "#     texts.append(abstract)\n",
    "print('read all %d xml files.' % len(xml_list))\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NUM_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=True, split=' ', char_level=False, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "xml_id_map = {}\n",
    "for i,xml in enumerate(xml_list):\n",
    "    node_id = int(xml.replace('.xml',''))\n",
    "    xml_id_map[node_id] = data[i,:]\n",
    "\n",
    "\n",
    "print('Preparing embedding matrix.')\n",
    "embeddings_index = {}\n",
    "# with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r', encoding='utf8') as f:\n",
    "with open(os.path.join('./','glove', 'glove.6B.%dd.txt' % EMBEDDING_DIM), 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
