{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "dir_path = './sdml_final_1'\n",
    "input_list  = ['prep_final_test_win_feature.pickle',\n",
    "               'prep_final_train_bid_data.pickle',\n",
    "               'prep_final_train_win_data.pickle']\n",
    "with open(join(dir_path, 'prep_final_train_win_data.pickle'), 'rb') as f:\n",
    "    win_dic = pickle.load(f)\n",
    "with open(join(dir_path, 'prep_final_train_bid_data.pickle'), 'rb') as f:\n",
    "    bid_dic = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# eps = 10**-8\n",
    "eps = 10**-2\n",
    "\n",
    "class Regression_model(nn.Module):\n",
    "    def __init__(self, app_dim, device_dim, imp_dim, imp_posi_dim, dm1, dm2, drop):\n",
    "        super(Regression_model, self).__init__()\n",
    "        self.app = nn.Embedding(app_dim, dm1)\n",
    "        self.device = nn.Embedding(device_dim, dm1)\n",
    "        self.imp = nn.Embedding(imp_dim, dm1)\n",
    "        self.imp_posi = nn.Embedding(imp_posi_dim, dm1)\n",
    "        self.weekdays = nn.Embedding(7, dm1)\n",
    "        self.hours = nn.Embedding(24, dm1)\n",
    "        self.linears = nn.ModuleList()\n",
    "        self.norms = nn.ModuleList()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        for i in range(10):\n",
    "            if i == 0:\n",
    "                linear = nn.Linear(6*dm1 + 2, dm2)\n",
    "            else:\n",
    "                linear = nn.Linear(dm2, dm2)\n",
    "            self.linears.append(linear)\n",
    "            self.norms.append(nn.BatchNorm1d(dm2))\n",
    "        self.out = nn.Linear(dm2, 1)\n",
    "        self.sigma = nn.Linear(dm2, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, width, height, app, device, imp, imp_posi, weekdays, hours):\n",
    "        app = self.app(app).squeeze(1)\n",
    "        device = self.device(device).squeeze(1)\n",
    "        imp = self.imp(imp).squeeze(1)\n",
    "        imp_posi = self.imp_posi(imp_posi).squeeze(1)\n",
    "        weekdays = self.weekdays(weekdays).squeeze(1)\n",
    "        hours = self.hours(hours).squeeze(1)\n",
    "        x = torch.cat([width, height, app, device, imp, imp_posi, weekdays, hours], dim=-1)\n",
    "        x = self.drop(x)\n",
    "        for i, (norm, linear) in enumerate(zip(self.norms, self.linears)):\n",
    "            if i == 0:\n",
    "                x = F.relu(linear(x))\n",
    "            else:\n",
    "                x = F.relu(linear(x)) + x\n",
    "            x = norm(x)\n",
    "            x = self.drop(x)\n",
    "        y = self.out(x)\n",
    "        sigma = F.relu(self.sigma(x))\n",
    "        sigma = torch.clamp(sigma, 5, 200)\n",
    "        return y, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dic(rate, dic):\n",
    "    N = len(dic['price'])\n",
    "    \n",
    "    for k in dic.keys():\n",
    "        assert dic[k].shape[0] == N\n",
    "#         print 'categorical %s, dim size : %d' % (k, np.max(dic[k]+1))\n",
    "    idx = np.random.permutation(N)\n",
    "    c = int(N*rate)\n",
    "    print 'split to %d : %d' % (c, N-c)\n",
    "    train_idx, val_idx = idx[c:], idx[:c]\n",
    "    train_dic = {k: dic[k][train_idx, :] for k in dic.keys()}\n",
    "    val_dic = {k: dic[k][val_idx, :] for k in dic.keys()}\n",
    "    return train_dic, val_dic\n",
    "\n",
    "def batch_boostrap_generator(dic, batch):\n",
    "    while True:\n",
    "        N = len(dic['price'])\n",
    "        rand_idx = np.random.permutation(N)\n",
    "        ran = N // batch_size + 1 if N % batch_size != 0 else N // batch_size\n",
    "        for i in range(ran):\n",
    "            idx = rand_idx[i*batch: (i+1)*batch] if (i+1)*batch < len(rand_idx) \\\n",
    "            else rand_idx[i*batch:]\n",
    "            app = dic['app_type'][idx, :]\n",
    "            device = dic['device_type'][idx, :]\n",
    "            imp = dic['imp_type'][idx, :]\n",
    "            imp_posi = dic['imp_position'][idx, :]\n",
    "            weekdays = dic['weekdays'][idx, :]\n",
    "            hours = dic['hours'][idx, :]\n",
    "            width = dic['width'][idx, :]\n",
    "            height = dic['height'][idx, :]\n",
    "            price = dic['price'][idx, :]\n",
    "            yield (width, height, app, device, imp, imp_posi, weekdays, hours, price)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split to 1858597 : 16727378\n",
      "split to 1645380 : 14808421\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "def win_loss(x, y, sigma, distribution):\n",
    "    d = distribution(y, sigma)\n",
    "    loss = -d.log_prob(x)    \n",
    "    return loss\n",
    "def bid_loss(x, y, sigma, distribution):\n",
    "    d = distribution(y, sigma)\n",
    "    loss = -torch.log(1-d.cdf(x))\n",
    "    return loss\n",
    "def calc_loss(data, is_win, distribution, use_cuda):\n",
    "    data = list(data)\n",
    "    for i, v in enumerate(data):\n",
    "        if v.dtype == np.int32:\n",
    "            data[i] = torch.LongTensor(v)\n",
    "        elif v.dtype == np.float32:\n",
    "            data[i] = torch.FloatTensor(v)\n",
    "        if use_cuda:\n",
    "            data[i] = data[i].cuda()\n",
    "        \n",
    "    width, height, app, device, imp, imp_posi, weekdays, hours, price = data\n",
    "    out, sigma = model(width, height, app, device, imp, imp_posi, weekdays, hours)\n",
    "#     print out[0,0], sigma[0,0]\n",
    "    if is_win:\n",
    "        loss = win_loss(out, price, sigma, distribution)\n",
    "#         print loss[0,0]\n",
    "#         wef\n",
    "    else:\n",
    "        loss = bid_loss(out, price, sigma, distribution)\n",
    "        loss = 0\n",
    "    return loss\n",
    "\n",
    "def train(model, opt, train_g, val_g, distribution, use_cuda):\n",
    "    price_opt, sigma_opt = opt\n",
    "    win_g, bid_g = train_g\n",
    "    win_data, bid_data = next(win_g), next(bid_g)\n",
    "    \n",
    "#     price phase\n",
    "    win_loss = calc_loss(win_data, True, distribution, use_cuda)\n",
    "    bid_loss = calc_loss(bid_data, False, distribution, use_cuda)\n",
    "    loss1 = torch.mean(win_loss + bid_loss)\n",
    "    price_opt.zero_grad()\n",
    "    loss1.backward()\n",
    "    price_opt.step()\n",
    "#     sigma phase\n",
    "    win_loss = calc_loss(win_data, True, distribution, use_cuda)\n",
    "    bid_loss = calc_loss(bid_data, False, distribution, use_cuda)\n",
    "    loss2 = torch.mean(win_loss + bid_loss)\n",
    "    sigma_opt.zero_grad()\n",
    "    loss2.backward()\n",
    "    sigma_opt.step()\n",
    "#     validation\n",
    "    with torch.no_grad():\n",
    "        win_g, bid_g = val_g\n",
    "        win_loss = calc_loss(next(win_g), True, distribution, use_cuda)\n",
    "        bid_loss = calc_loss(next(bid_g), False, distribution, use_cuda)\n",
    "        val_loss = torch.mean(win_loss + bid_loss)\n",
    "    return (loss1+loss2)/2, val_loss\n",
    "    \n",
    "batch_size = 8192\n",
    "train_win_dic, val_win_dic = split_dic(0.1, win_dic)\n",
    "train_bid_dic, val_bid_dic = split_dic(0.1, bid_dic)\n",
    "\n",
    "win_g = batch_boostrap_generator(train_win_dic, batch_size)\n",
    "bid_g = batch_boostrap_generator(train_bid_dic, batch_size)\n",
    "train_g = (win_g, bid_g)\n",
    "\n",
    "win_g = batch_boostrap_generator(val_win_dic, batch_size)\n",
    "bid_g = batch_boostrap_generator(val_bid_dic, batch_size)\n",
    "val_g = (win_g, bid_g)\n",
    "\n",
    "\n",
    "dm1 = 64\n",
    "dm2 = 512\n",
    "drop = 0.15\n",
    "# batch_size = 4\n",
    "model = Regression_model(3, 5, 3, 10, dm1, dm2, drop)\n",
    "\n",
    "price_params = []\n",
    "sigma_params = []\n",
    "for m in model.modules():\n",
    "    for p in m.parameters():\n",
    "        if m != model.out:\n",
    "            sigma_params.append(p)\n",
    "        if m != model.sigma:\n",
    "            price_params.append(p)\n",
    "# price_opt = torch.optim.Adadelta(price_params)\n",
    "# sigma_opt = torch.optim.Adadelta(sigma_params)\n",
    "price_opt = torch.optim.Adam(price_params)\n",
    "sigma_opt = torch.optim.Adam(sigma_params)\n",
    "opt = price_opt, sigma_opt\n",
    "\n",
    "distribution = torch.distributions.normal.Normal\n",
    "train_q = deque(maxlen=100)\n",
    "val_q = deque(maxlen=100)\n",
    "c = 0\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1233 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch statr : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [08:26<00:00,  2.38it/s, loss : 58441.30289, val loss : 56767.20570]\n",
      "  0%|          | 0/1233 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch statr : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 339/1233 [02:19<06:06,  2.44it/s, loss : 57066.25063, val loss : 58528.07918]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1233/1233 [08:26<00:00,  2.44it/s, loss : 57939.17012, val loss : 57688.69582]\n",
      "  0%|          | 0/1233 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch statr : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 587/1233 [04:01<04:32,  2.37it/s, loss : 58473.59664, val loss : 57451.42145]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1233/1233 [08:26<00:00,  2.44it/s, loss : 57240.61930, val loss : 56743.90621]\n",
      "  0%|          | 0/1233 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch statr : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 795/1233 [05:27<02:59,  2.44it/s, loss : 56884.89570, val loss : 58824.07387]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 1233/1233 [08:25<00:00,  2.44it/s, loss : 58052.51547, val loss : 58100.65535]\n",
      "  0%|          | 0/1233 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch statr : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 51/1233 [00:20<08:04,  2.44it/s, loss : 57069.44348, val loss : 58503.87555]"
     ]
    }
   ],
   "source": [
    "N = len(test_dic['price'])\n",
    "ran = N // batch_size + 1 if N % batch_size != 0 else N // batch_size\n",
    "pre_loss = 0\n",
    "pre_train = 0 \n",
    "model.train()\n",
    "for epoch in count():\n",
    "    print 'epoch statr : %d' % epoch\n",
    "    with tqdm(total=ran) as pbar:\n",
    "        for i in range(ran):\n",
    "            loss, val_loss = train(model, opt, train_g, val_g, distribution, use_cuda)\n",
    "            train_q.append(loss.item())\n",
    "            val_q.append(val_loss.item())\n",
    "            loss = np.mean(train_q)\n",
    "            val_loss = np.mean(val_q)\n",
    "            pbar.set_postfix_str('loss : %.5f, val loss : %.5f' % (loss, val_loss))\n",
    "            pbar.update(1)\n",
    "            if i % 10 == 0:\n",
    "                if val_loss > pre_loss and loss < pre_train:\n",
    "                    c += 1\n",
    "                else:\n",
    "                    c = 0\n",
    "                pre_loss = val_loss\n",
    "                pre_train = loss\n",
    "                if c > 5 and val_loss < 50:\n",
    "                    c = 0\n",
    "                    print 'end of training'\n",
    "                    weffwe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1233/1233 [02:17<00:00,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "def test_generator(dic, batch):\n",
    "    rand_idx = np.arange(len(dic['price']))\n",
    "    N = len(dic['price'])\n",
    "    ran = N // batch_size + 1 if N % batch_size != 0 else N // batch_size\n",
    "    for i in range(ran):\n",
    "        idx = rand_idx[i*batch: (i+1)*batch] if (i+1)*batch < len(rand_idx) \\\n",
    "        else rand_idx[i*batch:]\n",
    "        app = dic['app_type'][idx, :]\n",
    "        device = dic['device_type'][idx, :]\n",
    "        imp = dic['imp_type'][idx, :]\n",
    "        imp_posi = dic['imp_position'][idx, :]\n",
    "        weekdays = dic['weekdays'][idx, :]\n",
    "        hours = dic['hours'][idx, :]\n",
    "        width = dic['width'][idx, :]\n",
    "        height = dic['height'][idx, :]\n",
    "        price = dic['price'][idx, :]\n",
    "        yield (width, height, app, device, imp, imp_posi, weekdays, hours, price)\n",
    "        \n",
    "def test(model, data, use_cuda):\n",
    "    data = list(data)\n",
    "    for i, v in enumerate(data):\n",
    "        if v.dtype == np.int32:\n",
    "            data[i] = torch.LongTensor(v)\n",
    "        elif v.dtype == np.float32:\n",
    "            data[i] = torch.FloatTensor(v)\n",
    "        if use_cuda:\n",
    "            data[i] = data[i].cuda()\n",
    "        \n",
    "    width, height, app, device, imp, imp_posi, weekdays, hours, price = data\n",
    "    out, sigma = model(width, height, app, device, imp, imp_posi, weekdays, hours)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "dir_path = './sdml_final_1'\n",
    "with open(join(dir_path, 'prep_final_test_win_feature.pickle'), 'rb') as f:\n",
    "    test_dic = pickle.load(f)\n",
    "batch_size = 8192\n",
    "N = len(test_dic['price'])\n",
    "ran = N // batch_size + 1 if N % batch_size != 0 else N // batch_size\n",
    "    \n",
    "test_g = test_generator(test_dic, batch_size)\n",
    "use_cuda = True\n",
    "model.eval()\n",
    "with open('./submission.csv', 'w') as f_out:\n",
    "    f_out.write('id,win_price\\n')\n",
    "    with tqdm(total=ran) as pbar:\n",
    "        with torch.no_grad():\n",
    "            for i in range(ran):\n",
    "                data = next(test_g)\n",
    "                out = test(model, data, use_cuda)\n",
    "                for j in range(out.shape[0]):\n",
    "                    v = out[j,0].item()\n",
    "                    f_out.write('%d,%f\\n' % (i*batch_size+j+1, v))\n",
    "                pbar.update(1)\n",
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10093506\n",
      "1232\n",
      "10100736\n"
     ]
    }
   ],
   "source": [
    "print len(test_dic['price'])\n",
    "N = len(test_dic['price']) // batch_size\n",
    "print N\n",
    "print (N+1)*batch_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.12(virtualenv)",
   "language": "python",
   "name": "python2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
