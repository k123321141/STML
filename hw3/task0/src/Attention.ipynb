{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667085/667085 [00:01<00:00, 493977.86it/s]\n",
      " 12%|█▏        | 2271/18338 [00:00<00:02, 7512.29it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "with open('../hw_train.csv', 'rb') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "# count vocabulary and max sequence length\n",
    "lyrics_map = {}\n",
    "bos_token = 's'.decode('utf8')\n",
    "eos_token = 'e'.decode('utf8')\n",
    "line_token = 'l'.decode('utf8')\n",
    "none_lyrics_words = [u'編詞', u'作曲', u'作詞', u'編曲', u'監製']\n",
    "vocab = [bos_token, eos_token, line_token]\n",
    "lens = []\n",
    "c = 0 \n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        l = l.decode('utf8')\n",
    "        no, l = l.strip().split(',')\n",
    "        no = int(no)\n",
    "        if no not in lyrics_map:\n",
    "            lyrics_map[no] = []\n",
    "        lyrics_map[no].append(l)\n",
    "        pbar.update(1)\n",
    "# split into Q,K pair\n",
    "with tqdm(total=len(lyrics_map)) as pbar:\n",
    "    for k,v in lyrics_map.items():\n",
    "        line_num = len(v)\n",
    "        V = [bos_token, ]\n",
    "        Q = [bos_token, ]\n",
    "        for i,l in enumerate(v):\n",
    "            exist_none_layrics_word = False\n",
    "            for nw in none_lyrics_words:\n",
    "                if nw in l:\n",
    "                    exist_none_layrics_word = True\n",
    "                    break\n",
    "            if exist_none_layrics_word:\n",
    "                continue\n",
    "            \n",
    "            for ws in l.strip().split(' '):\n",
    "                for w in ws:\n",
    "                    if i <= line_num//2:\n",
    "                        V.append(w)\n",
    "                    else:\n",
    "                        Q.append(w)\n",
    "                    vocab.append(w)\n",
    "            if i <= line_num//2:\n",
    "                V.append(line_token)\n",
    "            else:\n",
    "                Q.append(line_token)\n",
    "        V.append(eos_token)\n",
    "        Q.append(eos_token)\n",
    "        lens.append(len(Q))\n",
    "        lens.append(len(V))\n",
    "        lyrics_map[k] = (Q, V)\n",
    "        pbar.update(1)\n",
    "            \n",
    "print 'vocabulary size : %d, max len : %.0f, mean : %.0f, std : %.0f' % (len(set(vocab)), np.max(lens), np.mean(lens), np.std(lens))\n",
    "print 'lyrics number : %d' % len(lyrics_map)\n",
    "# print np.max(lens), np.mean(lens), np.std(lens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from constants import MAX_SEQ_LEN, VOCAB_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "texts = vocab\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_DIM, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=False, split=' ', char_level=True, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "idx = np.random.permutation(len(lyrics_map))\n",
    "val_num = len(lyrics_map)//10\n",
    "train_idx, val_idx = idx[val_num:], idx[:val_num]\n",
    "train_u_map = {k:lyrics_map[k] for k in lyrics_map.keys()[val_num:]}\n",
    "val_u_map = {k:lyrics_map[k] for k in lyrics_map.keys()[:val_num]}\n",
    "def boostrap_generator(lyrics_map, max_seq_len, tokenizer):\n",
    "    while True:\n",
    "        keys = lyrics_map.keys()\n",
    "        for idx in np.random.permutation(len(lyrics_map)):\n",
    "            no = keys[idx]\n",
    "            Q, K = lyrics_map[no]\n",
    "            Q = np.array([tokenizer.word_index[w] for w in Q], dtype=np.int32).reshape(1,-1)\n",
    "            K = np.array([tokenizer.word_index[w] for w in K], dtype=np.int32).reshape(1,-1)\n",
    "            yield Q, K\n",
    "    \n",
    "bat = 4\n",
    "G = boostrap_generator(train_u_map, MAX_SEQ_LEN, tokenizer)\n",
    "q, k = next(G)\n",
    "    \n",
    "print q.shape, k.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "from constants import VOCAB_DIM, D_MODEL, MAX_SEQ_LEN\n",
    "# construct neuron network\n",
    "class Seq2seq_att(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, num_lay):\n",
    "        super(Seq2seq_att, self).__init__()\n",
    "#         for construct cache positional encoding matrix.\n",
    "        self.emb = nn.Embedding(VOCAB_DIM+1, dm, padding_idx=0)\n",
    "        self.encoder = nn.GRU(dm, dm, num_lay, batch_first=True)\n",
    "        self.decoder = nn.GRU(dm, dm, num_lay, batch_first=True)\n",
    "        self.linear = nn.Linear(dm, VOCAB_DIM+1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        K = self.emb(K)\n",
    "        Q = self.emb(Q)\n",
    "        en_out, hn = self.encoder(K) \n",
    "        Q, _ = self.decoder(Q, hn)\n",
    "    \n",
    "        batch, q_len, _ = Q.size()\n",
    "        batch, k_len, _ = K.size()\n",
    "        att_in = torch.cat([en_out, Q], dim=1)\n",
    "        Q_mask = self.Q_mask_matrix(batch, q_len)\n",
    "        K_mask = torch.zeros([batch, q_len, k_len], dtype=torch.uint8).cuda()\n",
    "        \n",
    "        mask = torch.cat([K_mask, Q_mask], dim=-1)\n",
    "        out = self.dot_attention(Q, att_in, att_in, mask=mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y = self.linear(out)\n",
    "        return y\n",
    "    def Q_mask_matrix(self, batch, Q_len):\n",
    "#         ByteTensor\n",
    "        mask = torch.zeros([1, Q_len, Q_len], dtype=torch.uint8, requires_grad=False)\n",
    "        for i in range(Q_len):\n",
    "            mask[0,i,i+1:] = 1\n",
    "        return mask.repeat(batch,1, 1).cuda()\n",
    "    \n",
    "    def dot_attention(self, Q, K, V, mask):\n",
    "        assert Q.size()[-1] == K.size()[-1]\n",
    "        assert len(Q.size()) == 3 and len(K.size()) == 3 and len(V.size()) == 3\n",
    "        out = torch.matmul(Q,K.permute(0,2,1))\n",
    "        if mask is not None:\n",
    "            out.masked_fill_(mask, -float('inf'))\n",
    "        return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "\n",
    "\n",
    "bat = 7\n",
    "lay_num = 2\n",
    "model = Seq2seq_att(D_MODEL, lay_num).cuda()\n",
    "# print(o.size())\n",
    "q, k = next(G)\n",
    "q = torch.LongTensor(q).cuda()\n",
    "k = torch.LongTensor(k).cuda()\n",
    "# Q = torch.randint(VOCAB_DIM+1, [bat, MAX_SEQ_LEN]).cuda()\n",
    "# K = torch.randint(VOCAB_DIM+1, [bat, MAX_SEQ_LEN]).cuda()\n",
    "\n",
    "o = model(q, k)\n",
    "print(o.size())\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print tokenizer.index_word[6456]\n",
    "print \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, val_loss, acc, val_acc, train_pred, train_label, val_pred, val_label, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f<split>%s<split>%s<split>%s<split>%s\\n' % (n_iter, loss, val_loss, acc, val_acc, train_pred, train_label, val_pred, val_label)\n",
    "    log_file_stream.write(log_text.encode('utf8'))\n",
    "    if n_iter % 10 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "def normal_acc(pred, label):\n",
    "    pred = torch.argmax(pred, dim=-1)\n",
    "    mask = torch.ones_like(label, dtype=torch.uint8)\n",
    "#     \n",
    "    acc = pred == label\n",
    "    acc = torch.sum(acc).item() / float(torch.sum(mask).item())\n",
    "    \n",
    "    return acc\n",
    "def seq2text(output, index_word):\n",
    "    assert len(output.size()) == 1\n",
    "    seq = output\n",
    "    s = ''\n",
    "    for i in seq:\n",
    "        i = int(i.item())\n",
    "        if i == 0:\n",
    "            continue\n",
    "        w = index_word[i]\n",
    "        s += w\n",
    "    return s\n",
    "    \n",
    "def rev_mask(m):\n",
    "    out = torch.ones_like(m, dtype=torch.uint8, requires_grad=False)\n",
    "    out.masked_fill_(m, 0)\n",
    "    return out\n",
    "        \n",
    "acc_q = deque(maxlen=100)\n",
    "loss_q = deque(maxlen=10)\n",
    "\n",
    "val_acc_q = deque(maxlen=100)\n",
    "val_loss_q = deque(maxlen=10)\n",
    "\n",
    "t = time.time()\n",
    "best_loss = float('inf')\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "G = boostrap_generator(train_u_map, MAX_SEQ_LEN, tokenizer)\n",
    "val_G = boostrap_generator(val_u_map, MAX_SEQ_LEN, tokenizer)\n",
    "# criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "weight = torch.ones([VOCAB_DIM+1,], dtype=torch.float)\n",
    "weight[tokenizer.word_index[line_token]] = 100.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weight.cuda())\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "print 'start training.'\n",
    "with open('log-att.txt', 'w') as f:\n",
    "    with open('best-att.txt', 'w') as best_log:\n",
    "        iters = 100000000\n",
    "        with tqdm(total=iters) as pbar:\n",
    "            for it in range(iters):\n",
    "                optimizer.zero_grad()\n",
    "                model.train()\n",
    "                q, k = next(G)\n",
    "                q = torch.LongTensor(q).cuda()\n",
    "                k = torch.LongTensor(k).cuda()\n",
    "\n",
    "                q.requires_grad_(False)\n",
    "                k.requires_grad_(False)\n",
    "\n",
    "\n",
    "                output = model(q, k)\n",
    "                y = q[:,1:]\n",
    "#                 loss = torch.sum(criterion(output, y) * pad_mask[:,1:, :]) \n",
    "                loss = criterion(output[:,:-1,:,].permute(0,2,1), y) \n",
    "                label = y\n",
    "\n",
    "                pred = output[:,:-1,:,]\n",
    "                acc = normal_acc(pred, label)\n",
    "                acc_q.append(acc)\n",
    "                train_pred = seq2text(torch.argmax(output[0,:,:], dim=-1), tokenizer.index_word)\n",
    "                train_label= seq2text(k[0,:], tokenizer.index_word)\n",
    "                loss.backward()\n",
    "                loss_q.append(loss.item())\n",
    "                \n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    q, k = next(val_G)\n",
    "                    q = torch.LongTensor(q).cuda()\n",
    "                    k = torch.LongTensor(k).cuda()\n",
    "                    \n",
    "                    q.requires_grad_(False)\n",
    "                    k.requires_grad_(False)\n",
    "                    \n",
    "                    output = model(q, k)\n",
    "                    y = q[:,1:]\n",
    "                    val_loss = criterion(output[:,:-1,:,].permute(0,2,1), y) \n",
    "                    label = y\n",
    "\n",
    "                    pred = output[:,:-1,:,]\n",
    "                    val_acc = normal_acc(pred, label)\n",
    "                    val_acc_q.append(val_acc)\n",
    "                    val_pred = seq2text(torch.argmax(output[0,:,:], dim=-1), tokenizer.index_word)\n",
    "                    val_label= seq2text(k[0,:], tokenizer.index_word)\n",
    "                    val_loss_q.append(val_loss.item())\n",
    "\n",
    "\n",
    "                \n",
    "                acc = np.mean(acc_q)\n",
    "                val_acc = np.mean(val_acc_q)\n",
    "                loss = np.mean(acc_q)\n",
    "                val_loss = np.mean(val_acc_q)\n",
    "                \n",
    "#                     pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f \\t %.3f, %.3f, %.3f, %.3f' % (acc, val_acc, loss.item(), val_loss.item(), a,b,c,d), refresh=False)\n",
    "                pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f' % (acc, val_acc, loss.item(), val_loss.item()), refresh=False)\n",
    "                pbar.update(batch_size)\n",
    "                dump_log(model, (it+1)*batch_size, loss, val_loss, acc, val_acc, train_pred, train_label, val_pred, val_label, f,'./tmp-att.pt')\n",
    "                if val_loss.item() < best_loss and it > 100:\n",
    "                    torch.save(model, './best-att.pt')\n",
    "                    best_loss = val_loss\n",
    "                    best_log.write('%d\\t%.5f\\n' % ((it+1)*batch_size, best_loss))\n",
    "                    best_log.flush()\n",
    "                if it % 2000 == 0 and it >= 100: \n",
    "                    print 'train pred : %s\\ntrain label : %s' % (train_pred, train_label)\n",
    "                    print 'validation pred : %s\\nvalidation label : %s' % (val_pred, val_label)\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
