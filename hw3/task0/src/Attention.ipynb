{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 667085/667085 [00:01<00:00, 493923.79it/s]\n",
      "100%|██████████| 18338/18338 [00:02<00:00, 7375.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size : 6450, max len : 4016, min len : 5, mean : 180, std : 117\n",
      "lyrics number : 18223\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from constants import MAX_SEQ_LEN\n",
    "\n",
    "with open('../hw_train.csv', 'rb') as f:\n",
    "    ls = f.readlines()[1:]\n",
    "# count vocabulary and max sequence length\n",
    "lyrics_map = {}\n",
    "bos_token = 's'.decode('utf8')\n",
    "eos_token = 'e'.decode('utf8')\n",
    "line_token = 'l'.decode('utf8')\n",
    "oov_token = 'o'.decode('utf8')\n",
    "none_lyrics_words = [u'編詞', u'作曲', u'作詞', u'編曲', u'監製']\n",
    "vocab = [bos_token, eos_token, line_token, oov_token]\n",
    "lens = []\n",
    "c = 0 \n",
    "with tqdm(total=len(ls)) as pbar:\n",
    "    for l in ls:\n",
    "        l = l.decode('utf8')\n",
    "        no, l = l.strip().split(',')\n",
    "        no = int(no)\n",
    "        if no not in lyrics_map:\n",
    "            lyrics_map[no] = []\n",
    "        lyrics_map[no].append(l)\n",
    "        pbar.update(1)\n",
    "# split into Q,K pair\n",
    "with tqdm(total=len(lyrics_map)) as pbar:\n",
    "    for k,v in lyrics_map.items():\n",
    "        line_num = len(v)\n",
    "        V = [bos_token, ]\n",
    "        Q = [bos_token, ]\n",
    "        for i,l in enumerate(v):\n",
    "            if len(l) <= 1:\n",
    "                continue\n",
    "            exist_none_layrics_word = False\n",
    "            for nw in none_lyrics_words:\n",
    "                if nw in l:\n",
    "                    exist_none_layrics_word = True\n",
    "                    break\n",
    "            if exist_none_layrics_word:\n",
    "                continue\n",
    "            \n",
    "            for ws in l.strip().split(' '):\n",
    "                for w in ws:\n",
    "                    if i <= line_num//2:\n",
    "                        V.append(w)\n",
    "                    else:\n",
    "                        Q.append(w)\n",
    "                    vocab.append(w)\n",
    "            if i <= line_num//2:\n",
    "                V.append(line_token)\n",
    "            else:\n",
    "                Q.append(line_token)\n",
    "        \n",
    "        V.append(eos_token)\n",
    "        Q.append(eos_token)\n",
    "        if len(Q) == 2 or len(V) == 2:\n",
    "            lyrics_map.pop(k)\n",
    "            pbar.update(1)\n",
    "            continue\n",
    "        lens.append(len(Q))\n",
    "        lens.append(len(V))\n",
    "        lyrics_map[k] = (Q, V)\n",
    "        pbar.update(1)\n",
    "            \n",
    "print 'vocabulary size : %d, max len : %.0f, min len : %.0f, mean : %.0f, std : %.0f' % (len(set(vocab)), np.max(lens), np.min(lens), np.mean(lens), np.std(lens))\n",
    "print 'lyrics number : %d' % len(lyrics_map)\n",
    "# print np.max(lens), np.mean(lens), np.std(lens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6450 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pickle\n",
    "from constants import MAX_SEQ_LEN, VOCAB_DIM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "texts = vocab\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_DIM, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ ',\n",
    "                                   lower=False, split=' ', char_level=True, oov_token=oov_token)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "with open('tokenizer','wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 232) (1, 206)\n"
     ]
    }
   ],
   "source": [
    "from constants import MAX_SEQ_LEN\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "idx = np.random.permutation(len(lyrics_map))\n",
    "val_num = len(lyrics_map)//10\n",
    "train_idx, val_idx = idx[val_num:], idx[:val_num]\n",
    "train_u_map = {k:lyrics_map[k] for k in lyrics_map.keys()[val_num:]}\n",
    "val_u_map = {k:lyrics_map[k] for k in lyrics_map.keys()[:val_num]}\n",
    "def boostrap_generator(lyrics_map, max_seq_len, tokenizer):\n",
    "    while True:\n",
    "        keys = lyrics_map.keys()\n",
    "        for idx in np.random.permutation(len(lyrics_map)):\n",
    "            no = keys[idx]\n",
    "            Q, K = lyrics_map[no]\n",
    "            Q = np.array([tokenizer.word_index[w] if w in tokenizer.word_index else tokenizer.word_index[oov_token] for w in Q], dtype=np.int32).reshape(1,-1)\n",
    "            K = np.array([tokenizer.word_index[w] if w in tokenizer.word_index else tokenizer.word_index[oov_token] for w in K], dtype=np.int32).reshape(1,-1)\n",
    "            yield Q, K\n",
    "    \n",
    "bat = 4\n",
    "G = boostrap_generator(train_u_map, MAX_SEQ_LEN, tokenizer)\n",
    "q, k = next(G)\n",
    "    \n",
    "print q.shape, k.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__slotnames__', '__str__', '__subclasshook__', '__weakref__', '_all_weights', '_apply', '_backend', '_backward_hooks', '_buffers', '_flat_weights', '_forward_hooks', '_forward_pre_hooks', '_get_name', '_load_from_state_dict', '_load_state_dict_pre_hooks', '_modules', '_named_members', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_slow_forward', '_state_dict_hooks', '_tracing_name', '_version', 'add_module', 'all_weights', 'apply', 'batch_first', 'bias', 'bias_hh_l0', 'bias_hh_l1', 'bias_ih_l0', 'bias_ih_l1', 'bidirectional', 'buffers', 'check_forward_args', 'children', 'cpu', 'cuda', 'double', 'dropout', 'dump_patches', 'eval', 'extra_repr', 'flatten_parameters', 'float', 'forward', 'half', 'hidden_size', 'input_size', 'load_state_dict', 'mode', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_layers', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'reset_parameters', 'share_memory', 'state_dict', 'to', 'train', 'training', 'type', 'weight_hh_l0', 'weight_hh_l1', 'weight_ih_l0', 'weight_ih_l1', 'zero_grad']\n",
      "torch.Size([1, 167, 6451])\n",
      "2054195\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.init import xavier_normal_\n",
    "from constants import VOCAB_DIM, D_MODEL, MAX_SEQ_LEN\n",
    "# construct neuron network\n",
    "class Seq2seq_att(nn.Module):\n",
    "\n",
    "    def __init__(self, dm, num_lay):\n",
    "        super(Seq2seq_att, self).__init__()\n",
    "#         for construct cache positional encoding matrix.\n",
    "        self.emb = nn.Embedding(VOCAB_DIM+1, dm, padding_idx=0)\n",
    "        self.encoder = nn.GRU(dm, dm, num_lay, batch_first=True)\n",
    "        self.decoder = nn.GRU(dm, dm, num_lay, batch_first=True)\n",
    "        self.linear = nn.Linear(dm, VOCAB_DIM+1)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        K = self.emb(K)\n",
    "        Q = self.emb(Q)\n",
    "        en_out, hn = self.encoder(K) \n",
    "        Q, _ = self.decoder(Q, hn)\n",
    "        batch, q_len, _ = Q.size()\n",
    "        batch, k_len, _ = K.size()\n",
    "        att_in = torch.cat([en_out, Q], dim=1)\n",
    "        Q_mask = self.Q_mask_matrix(batch, q_len)\n",
    "        K_mask = torch.zeros([batch, q_len, k_len], dtype=torch.uint8).cuda()\n",
    "        \n",
    "        mask = torch.cat([K_mask, Q_mask], dim=-1)\n",
    "        out = self.dot_attention(Q, att_in, att_in, mask=mask)\n",
    "        \n",
    "        \n",
    "        \n",
    "        y = self.linear(out)\n",
    "        return y\n",
    "    def Q_mask_matrix(self, batch, Q_len):\n",
    "#         ByteTensor\n",
    "        mask = torch.zeros([1, Q_len, Q_len], dtype=torch.uint8, requires_grad=False)\n",
    "        for i in range(Q_len):\n",
    "            mask[0,i,i+1:] = 1\n",
    "        return mask.repeat(batch,1, 1).cuda()\n",
    "    \n",
    "    def dot_attention(self, Q, K, V, mask):\n",
    "        assert Q.size()[-1] == K.size()[-1]\n",
    "        assert len(Q.size()) == 3 and len(K.size()) == 3 and len(V.size()) == 3\n",
    "        out = torch.matmul(Q,K.permute(0,2,1))\n",
    "        if mask is not None:\n",
    "            out.masked_fill_(mask, -float('inf'))\n",
    "        return torch.matmul(F.softmax(out, dim=-1), V)\n",
    "\n",
    "\n",
    "bat = 7\n",
    "lay_num = 2\n",
    "model = Seq2seq_att(D_MODEL, lay_num).cuda()\n",
    "# print(o.size())\n",
    "q, k = next(G)\n",
    "q = torch.LongTensor(q).cuda()\n",
    "k = torch.LongTensor(k).cuda()\n",
    "# Q = torch.randint(VOCAB_DIM+1, [bat, MAX_SEQ_LEN]).cuda()\n",
    "# K = torch.randint(VOCAB_DIM+1, [bat, MAX_SEQ_LEN]).cuda()\n",
    "\n",
    "o = model(q, k)\n",
    "print(o.size())\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/100000000 [00:01<20449:13:17,  1.36it/s, acc : 0.000, val_acc : 0.008, loss : 0.000, val_loss : 0.008]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/100000000 [00:01<10671:23:54,  2.60it/s, acc : 0.053, val_acc : 0.063, loss : 0.053, val_loss : 0.063]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/100000000 [00:01<7924:50:12,  3.51it/s, acc : 0.067, val_acc : 0.067, loss : 0.067, val_loss : 0.067]/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type Seq2seq_att. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "  0%|          | 12/100000000 [00:01<6037:04:24,  4.60it/s, acc : 0.074, val_acc : 0.071, loss : 0.074, val_loss : 0.071]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 14/100000000 [00:01<4699:43:36,  5.91it/s, acc : 0.083, val_acc : 0.073, loss : 0.083, val_loss : 0.073]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/100000000 [00:01<2873:03:50,  9.67it/s, acc : 0.100, val_acc : 0.084, loss : 0.100, val_loss : 0.084]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/100000000 [00:02<2042:07:10, 13.60it/s, acc : 0.111, val_acc : 0.089, loss : 0.111, val_loss : 0.089]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n",
      "hn torch.Size([2, 1, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-960dab7da1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mtrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mtrain_label\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mseq2text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m                 \u001b[0mloss_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/tensor.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/k123/env/python2.7.15/lib/python2.7/site-packages/torch/autograd/__init__.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "import time\n",
    "def dump_log(model, n_iter, loss, val_loss, acc, val_acc, train_pred, train_label, val_pred, val_label, log_file_stream, tmp_model_path):\n",
    "    log_text = '%.7d<split>%.5f<split>%.5f<split>%.5f<split>%.5f<split>%s<split>%s<split>%s<split>%s\\n' % (n_iter, loss, val_loss, acc, val_acc, train_pred, train_label, val_pred, val_label)\n",
    "    log_file_stream.write(log_text.encode('utf8'))\n",
    "    if n_iter % 10 == 0 :\n",
    "        log_file_stream.flush()\n",
    "        torch.save(model, tmp_model_path)\n",
    "def normal_acc(pred, label):\n",
    "    pred = torch.argmax(pred, dim=-1)\n",
    "    mask = torch.ones_like(label, dtype=torch.uint8)\n",
    "#     \n",
    "    acc = pred == label\n",
    "    acc = torch.sum(acc).item() / float(torch.sum(mask).item())\n",
    "    \n",
    "    return acc\n",
    "def seq2text(output, index_word):\n",
    "    assert len(output.size()) == 1\n",
    "    seq = output\n",
    "    s = ''\n",
    "    for i in seq:\n",
    "        i = int(i.item())\n",
    "        if i == 0:\n",
    "            continue\n",
    "        if i not in index_word:\n",
    "            w = oov_token\n",
    "        else:\n",
    "            w = index_word[i]\n",
    "        s += w\n",
    "    return s\n",
    "    \n",
    "def rev_mask(m):\n",
    "    out = torch.ones_like(m, dtype=torch.uint8, requires_grad=False)\n",
    "    out.masked_fill_(m, 0)\n",
    "    return out\n",
    "def scheduled_sampling_rate(acc):\n",
    "    return (acc)/3.\n",
    "def scheduled_sampling_label(output, label, rate):\n",
    "    assert len(label.size()) == 2 and len(output.size()) == 2\n",
    "    ret = torch.zeros_like(label)\n",
    "    for b in range(label.size()[0]):\n",
    "        for i in range(label.size()[1]):\n",
    "            r = np.random.random()\n",
    "            if r < rate:\n",
    "                ret[b,i] = output[b,i]\n",
    "            else:\n",
    "                ret[b,i] = label[b,i]\n",
    "    return ret\n",
    "                \n",
    "acc_q = deque(maxlen=100)\n",
    "loss_q = deque(maxlen=10)\n",
    "acc = 0\n",
    "val_acc_q = deque(maxlen=100)\n",
    "val_loss_q = deque(maxlen=10)\n",
    "\n",
    "t = time.time()\n",
    "best_loss = float('inf')\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "G = boostrap_generator(train_u_map, MAX_SEQ_LEN, tokenizer)\n",
    "val_G = boostrap_generator(val_u_map, MAX_SEQ_LEN, tokenizer)\n",
    "# criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "weight = torch.ones([VOCAB_DIM+1,], dtype=torch.float)\n",
    "weight[tokenizer.word_index[line_token]] = 100.\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss(weight=weight.cuda())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "print 'start training.'\n",
    "with open('log-att.txt', 'w') as f:\n",
    "    with open('best-att.txt', 'w') as best_log:\n",
    "        iters = 100000000\n",
    "        with tqdm(total=iters) as pbar:\n",
    "            for it in range(iters):\n",
    "                optimizer.zero_grad()\n",
    "                model.train()\n",
    "                q, k = next(G)\n",
    "                q = torch.LongTensor(q).cuda()\n",
    "                k = torch.LongTensor(k).cuda()\n",
    "\n",
    "                q.requires_grad_(False)\n",
    "                k.requires_grad_(False)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    output = model(q, k)\n",
    "                q = scheduled_sampling_label(torch.argmax(output, dim=-1), q[:,:-1], scheduled_sampling_rate(acc))\n",
    "            \n",
    "\n",
    "                output = model(q, k)\n",
    "                y = q[:,1:]\n",
    "#                 loss = torch.sum(criterion(output, y) * pad_mask[:,1:, :]) \n",
    "                loss = criterion(output[:,:-1,:].permute(0,2,1), y) \n",
    "                label = y\n",
    "\n",
    "                pred = output[:,:-1,:,]\n",
    "                acc = normal_acc(pred, label)\n",
    "                acc_q.append(acc)\n",
    "                train_pred = seq2text(torch.argmax(output[0,:,:], dim=-1), tokenizer.index_word)\n",
    "                train_label= seq2text(k[0,:], tokenizer.index_word)\n",
    "                loss.backward()\n",
    "                loss_q.append(loss.item())\n",
    "                \n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    q, k = next(val_G)\n",
    "                    q = torch.LongTensor(q).cuda()\n",
    "                    k = torch.LongTensor(k).cuda()\n",
    "                    \n",
    "                    q.requires_grad_(False)\n",
    "                    k.requires_grad_(False)\n",
    "                    \n",
    "                    output = model(q, k)\n",
    "                    y = q[:,1:]\n",
    "                    val_loss = criterion(output[:,:-1,:,].permute(0,2,1), y) \n",
    "                    label = y\n",
    "\n",
    "                    pred = output[:,:-1,:,]\n",
    "                    val_acc = normal_acc(pred, label)\n",
    "                    val_acc_q.append(val_acc)\n",
    "                    val_pred = seq2text(torch.argmax(output[0,:,:], dim=-1), tokenizer.index_word)\n",
    "                    val_label= seq2text(k[0,:], tokenizer.index_word)\n",
    "                    val_loss_q.append(val_loss.item())\n",
    "\n",
    "\n",
    "                \n",
    "                acc = np.mean(acc_q)\n",
    "                val_acc = np.mean(val_acc_q)\n",
    "                loss = np.mean(acc_q)\n",
    "                val_loss = np.mean(val_acc_q)\n",
    "                \n",
    "#                     pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f \\t %.3f, %.3f, %.3f, %.3f' % (acc, val_acc, loss.item(), val_loss.item(), a,b,c,d), refresh=False)\n",
    "                pbar.set_postfix_str('acc : %.3f, val_acc : %.3f, loss : %.3f, val_loss : %.3f' % (acc, val_acc, loss.item(), val_loss.item()), refresh=False)\n",
    "                pbar.update(batch_size)\n",
    "                dump_log(model, (it+1)*batch_size, loss, val_loss, acc, val_acc, train_pred, train_label, val_pred, val_label, f,'./tmp-att.pt')\n",
    "                if val_loss.item() < best_loss and it > 100:\n",
    "                    torch.save(model, './best-att.pt')\n",
    "                    best_loss = val_loss\n",
    "                    best_log.write('%d\\t%.5f\\n' % ((it+1)*batch_size, best_loss))\n",
    "                    best_log.flush()\n",
    "                if it % 2000 == 0 and it >= 100: \n",
    "                    print 'train pred : %s\\ntrain label : %s' % (train_pred, train_label)\n",
    "                    print 'validation pred : %s\\nvalidation label : %s' % (val_pred, val_label)\n",
    "\n",
    "# Train model\n",
    "print(\"Optimization Finished!\")\n",
    "# print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print output.shape\n",
    "print y.shape\n",
    "print q.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7.15 (virtualenv)",
   "language": "python",
   "name": "python2.7.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
