{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 651339 sentence pairs\n",
      "Trimmed to 651293 sentence pairs\n",
      "Indexing words...\n",
      "[u'SOS \\u4f46 \\u6211 \\u671d\\u5915 \\u5728 \\u6000\\u5ff5 EOS v r v n v NOP ian NOE 5 NOR', u'SOS \\u543b\\u543b \\u4f60 \\u56de\\u5934 \\u8bdd \\u518d\\u89c1 EOS']\n",
      "SOS 当 你 看着 我 EOS r v r r NOP e NOE 4 NOR\n",
      "SOS 我 不知 该 如何 EOS\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = True\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2 # Count SOS and EOS\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "def normalize_string(s):\n",
    "#     s = unicode_to_ascii(s.lower().strip())\n",
    "    s = s.strip()\n",
    "#     print s,'start'\n",
    "    s = re.sub(u\"([.!?])\", u\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(u\"[^\\u4e00-\\u9fffa-zA-Z.!?0-9]+\", r\" \", s)\n",
    "#     print s,'end'\n",
    "    return s\n",
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../data/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s.decode('utf8')) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs\n",
    "MAX_LENGTH = 60\n",
    "\n",
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "#         p[1].startswith(good_prefixes)\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]\n",
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
    "# input_lang, output_lang, pairs = prepare_data('eng', 'cmn', True)\n",
    "input_lang, output_lang, pairs = prepare_data('r1', 'r2', True)\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))\n",
    "a,b = random.choice(pairs)\n",
    "print a\n",
    "print b\n",
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "#     print('var =', var)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttnDecoderRNN(\n",
       "  (embedding): Embedding(90467, 512)\n",
       "  (gru): GRU(1024, 512, num_layers=2, dropout=0.05)\n",
       "  (out): Linear(in_features=1024, out_features=90467, bias=True)\n",
       "  (attn): Attn(\n",
       "    (attn): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "#             print hidden.shape, energy.shape, energy.transpose(0,1).shape, 'lolo'\n",
    "            energy = torch.mm(hidden,energy.transpose(0,1))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, word_input, last_context, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), 2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((rnn_output, context), 1)), dim=-1)\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights\n",
    "\n",
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_context, decoder_hidden, encoder_outputs)\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni.item()])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "    return decoded_words\n",
    "encoder = torch.load('./models/encoder-att.pl')\n",
    "decoder = torch.load('./models/decoder-att.pl')\n",
    "encoder.eval()\n",
    "decoder.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/70000 [00:01<1:04:10, 18.17it/s]/home/k123/miniconda2/envs/python-conda/lib/python2.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6\n",
      "  \n",
      " 32%|███▏      | 22295/70000 [24:38<48:00, 16.56it/s]  IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 70000/70000 [1:13:03<00:00, 15.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "c = 0\n",
    "def testing(sentence):\n",
    "        \n",
    "    while True:\n",
    "        try:\n",
    "            output_words = evaluate(sentence)\n",
    "            break\n",
    "        except KeyError,e:\n",
    "            global c\n",
    "            c += 1\n",
    "            sub = input_lang.index2word[np.random.randint(input_lang.n_words)]\n",
    "            sentence = sentence.replace(e.message,sub)\n",
    "    s = ''\n",
    "    for y in output_words:\n",
    "        s += y + ' '\n",
    "    return s.replace('<EOS>','').replace('SOS','').replace('EOS','').strip()\n",
    "path = '../hw3_1/all/test.csv'\n",
    "out_path = '../output.csv'\n",
    "with open(path,'r') as f:\n",
    "    ls = f.readlines()\n",
    "\n",
    "with open(out_path,'w') as f:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        with tqdm(total=len(ls)) as pbar:\n",
    "            for l in ls:\n",
    "                l = l.decode('utf8').strip()\n",
    "                s = testing(l)+'\\n'\n",
    "                f.write(s.encode('utf8'))\n",
    "                pbar.update(1)\n",
    "print 'done', c\n",
    "s = testing(u\"SOS 所以 我 好 愿意 EOS v r l NOP i NOE 3 NOR\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    \n",
    "#     print('>', pair[0])\n",
    "#     print('=', pair[1])\n",
    "#     print('<', output_sentence)\n",
    "#     print('')\n",
    "    print '>', pair[0]\n",
    "    print '=', pair[1]\n",
    "    print '<', output_sentence\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SOS 后悔 在 失去 之后 EOS d v ns n NOP iu NOE 4 NOR\n",
      "= SOS 仿佛 看 江水 东流 EOS\n",
      "< SOS <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(u\"SOS 所以 我 好 愿意 EOS v r l NOP i NOE 3 NOR\")\n",
    "print len(output_words), attentions.shape\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    show_attention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"elle a cinq ans de moins que moi .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"elle est trop petit .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"je ne crains pas de mourir .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "* Try with a different dataset\n",
    "    * Another language pair\n",
    "    * Human &rarr; Machine (e.g. IOT commands)\n",
    "    * Chat &rarr; Response\n",
    "    * Question &rarr; Answer\n",
    "* Replace the embedding pre-trained word embeddings such as word2vec or GloVe\n",
    "* Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
    "* If you use a translation file where pairs have two of the same phrase (`I am test \\t I am test`), you can use this as an autoencoder. Try this:\n",
    "    * Train as an autoencoder\n",
    "    * Save only the Encoder network\n",
    "    * Train a new Decoder for translation from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2.7.14 (conda)",
   "language": "python",
   "name": "python-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
