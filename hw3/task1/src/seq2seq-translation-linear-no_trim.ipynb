{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Translation with a Sequence to Sequence Network and Attention\n",
    "\n",
    "In this project we will be teaching a neural network to translate from French to English.\n",
    "\n",
    "```\n",
    "[KEY: > input, = target, < output]\n",
    "\n",
    "> il est en train de peindre un tableau .\n",
    "= he is painting a picture .\n",
    "< he is painting a picture .\n",
    "\n",
    "> pourquoi ne pas essayer ce vin delicieux ?\n",
    "= why not try that delicious wine ?\n",
    "< why not try that delicious wine ?\n",
    "\n",
    "> elle n est pas poete mais romanciere .\n",
    "= she is not a poet but a novelist .\n",
    "< she not not a poet but a novelist .\n",
    "\n",
    "> vous etes trop maigre .\n",
    "= you re too skinny .\n",
    "< you re all alone .\n",
    "```\n",
    "\n",
    "... to varying degrees of success.\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence to sequence network](http://arxiv.org/abs/1409.3215), in which two recurrent neural networks work together to transform one sequence to another. An encoder network condenses an input sequence into a single vector, and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "To improve upon this model we'll use an [attention mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder learn to focus over a specific range of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sequence to Sequence model\n",
    "\n",
    "A [Sequence to Sequence network](http://arxiv.org/abs/1409.3215), or seq2seq network, or [Encoder Decoder network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting of two separate RNNs called the **encoder** and **decoder**. The encoder reads an input sequence one item at a time, and outputs a vector at each step. The final output of the encoder is kept as the **context** vector. The decoder uses this context vector to produce a sequence of outputs one step at a time.\n",
    "\n",
    "![](https://i.imgur.com/tVtHhNp.png)\n",
    "\n",
    "When using a single RNN, there is a one-to-one relationship between inputs and outputs. We would quickly run into problems with different sequence orders and lengths that are common during translation. Consider the simple sentence \"Je ne suis pas le chat noir\" &rarr; \"I am not the black cat\". Many of the words have a pretty direct translation, like \"chat\" &rarr; \"cat\". However the differing grammars cause words to be in different orders, e.g. \"chat noir\" and \"black cat\". There is also the \"ne ... pas\" &rarr; \"not\" construction that makes the two sentences have different lengths.\n",
    "\n",
    "With the seq2seq model, by encoding many inputs into one vector, and decoding from one vector into many outputs, we are freed from the constraints of sequence order and length. The encoded sequence is represented by a single vector, a single point in some N dimensional space of sequences. In an ideal case, this point can be considered the \"meaning\" of the sequence.\n",
    "\n",
    "This idea can be extended beyond sequences. Image captioning tasks take an [image as input, and output a description](https://arxiv.org/abs/1411.4555) of the image (img2seq). Some image generation tasks take a [description as input and output a generated image](https://arxiv.org/abs/1511.02793) (seq2img). These models can be referred to more generally as \"encoder decoder\" networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Mechanism\n",
    "\n",
    "The fixed-length vector carries the burden of encoding the the entire \"meaning\" of the input sequence, no matter how long that may be. With all the variance in language, this is a very hard problem. Imagine two nearly identical sentences, twenty words long, with only one word different. Both the encoders and decoders must be nuanced enough to represent that change as a very slightly different point in space.\n",
    "\n",
    "The **attention mechanism** [introduced by Bahdanau et al.](https://arxiv.org/abs/1409.0473) addresses this by giving the decoder a way to \"pay attention\" to parts of the input, rather than relying on a single vector. For every step the decoder can select a different part of the input sentence to consider.\n",
    "\n",
    "![](https://i.imgur.com/5y6SCvU.png)\n",
    "\n",
    "Attention is calculated with another feedforward layer in the decoder. This layer will use the current input and hidden state to create a new vector, which is the same size as the input sequence (in practice, a fixed maximum length). This vector is processed through softmax to create *attention weights*, which are multiplied by the encoders' outputs to create a new context vector, which is then used to predict the next output.\n",
    "\n",
    "![](https://i.imgur.com/K1qMPxs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "\n",
    "You will need [PyTorch](http://pytorch.org/) to build and train the models, and [matplotlib](https://matplotlib.org/) for plotting training and visualizing attention outputs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will also define a constant to decide whether to use the GPU (with CUDA specifically) or the CPU. **If you don't have a GPU, set this to `False`**. Later when we create tensors, this variable will be used to decide whether we keep them on CPU or move them to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data files\n",
    "\n",
    "The data for this project is a set of many thousands of English to French translation pairs.\n",
    "\n",
    "[This question on Open Data Stack Exchange](http://opendata.stackexchange.com/questions/3888/dataset-of-sentences-translated-into-many-languages) pointed me to the open translation site http://tatoeba.org/ which has downloads available at http://tatoeba.org/eng/downloads - and better yet, someone did the extra work of splitting language pairs into individual text files here: http://www.manythings.org/anki/\n",
    "\n",
    "The English to French pairs are too big to include in the repo, so download `fra-eng.zip`, extract the text file in there, and rename it to `data/eng-fra.txt` before continuing (for some reason the zipfile is named backwards). The file is a tab separated list of translation pairs:\n",
    "\n",
    "```\n",
    "I am cold.    J'ai froid.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the character encoding used in the character-level RNN tutorials, we will be representing each word in a language as a one-hot vector, or giant vector of zeros except for a single one (at the index of the word). Compared to the dozens of characters that might exist in a language, there are many many more words, so the encoding vector is much larger. We will however cheat a bit and trim the data to only use a few thousand words per language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing words\n",
    "\n",
    "We'll need a unique index per word to use as the inputs and targets of the networks later. To keep track of all this we will use a helper class called `Lang` which has word &rarr; index (`word2index`) and index &rarr; word (`index2word`) dictionaries, as well as a count of each word `word2count` to use to later replace rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_token = 2\n",
    "MAX_VOCAB_DIM = 50000\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.default_vocab = {0: \"<SOS>\", 1: \"<EOS>\", 2:'<UNK>'}\n",
    "        self.index2word = self.default_vocab.copy()\n",
    "        self.n_words = 3 # Count SOS and EOS\n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def trim_vocab(self, max_vocab_dim):\n",
    "        start_idx = len(self.default_vocab)\n",
    "        kv = np.array([(k,v) for k,v in self.word2count.items() if k not in self.default_vocab])\n",
    "        sorted_count_idx = np.flip(np.argsort(np.array(kv[:,1], dtype=np.int32)), axis=0)\n",
    "        self.index2word = self.default_vocab.copy()\n",
    "        self.word2index = {v:k for k,v in self.index2word.items()}\n",
    "        i = start_idx\n",
    "        \n",
    "        for word in np.array(kv[:,0])[sorted_count_idx[:max_vocab_dim-len(self.default_vocab)]]:\n",
    "            self.index2word[i] = word\n",
    "            self.word2index[word] = i\n",
    "            i += 1\n",
    "        self.n_words = len(self.index2word)\n",
    "        self.word2count = {k:v for k,v in self.word2count.items() if k in self.word2index}\n",
    "            \n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and decoding files\n",
    "\n",
    "The files are all in Unicode, to simplify we will turn Unicode characters to ASCII, make everything lowercase, and trim most punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427\n",
    "# def unicode_to_ascii(s):\n",
    "#     return ''.join(\n",
    "#         c for c in unicodedata.normalize('NFD', s)\n",
    "#         if unicodedata.category(c) != 'Mn'\n",
    "#     )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "#     s = unicode_to_ascii(s.lower().strip())\n",
    "    s = s.strip()\n",
    "#     print s,'start'\n",
    "    s = re.sub(u\"([.!?])\", u\" \\1\", s)\n",
    "#     s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(u\"[^\\u4e00-\\u9fffa-zA-Z.!?0-9]+\", r\" \", s)\n",
    "#     print s,'end'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the data file we will split the file into lines, and then split lines into pairs. The files are all English &rarr; Other Language, so if we want to translate from Other Language &rarr; English I added the `reverse` flag to reverse the pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('../data/%s-%s.txt' % (lang1, lang2)).read().strip().split('\\n')\n",
    "    \n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalize_string(s.decode('utf8')) for s in l.split('\\t')] for l in lines]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering sentences\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train something quickly, we'll trim the data set to only relatively short and simple sentences. Here the maximum length is 10 words (that includes punctuation) and we're filtering to sentences that translate to the form \"I am\" or \"He is\" etc. (accounting for apostrophes being removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 60\n",
    "\n",
    "good_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \"\n",
    ")\n",
    "\n",
    "def filter_pair(p):\n",
    "#     return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "#         p[1].startswith(good_prefixes)\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full process for preparing the data is:\n",
    "\n",
    "* Read text file and split into lines, split lines into pairs\n",
    "* Normalize text, filter by length and content\n",
    "* Make word lists from sentences in pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 651339 sentence pairs\n",
      "Trimmed to 651293 sentence pairs\n",
      "Indexing words...\n",
      "[u'SOS \\u730e\\u6237\\u5ea7 \\u6709 \\u4e00\\u628a \\u5f13\\u7bad EOS r v d v v uj v NOP ian NOE 7 NOR', u'SOS \\u4ed6 \\u662f\\u5426 \\u4e5f \\u80fd \\u770b \\u7684 \\u89c1 EOS']\n",
      "SOS 请 爱 我 EOS r a v d v n uj n NOP e NOE 8 NOR\n",
      "SOS 我 奢望 爱 就 像 吞象 的 蛇 EOS\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "# input_lang, output_lang, pairs = prepare_data('eng', 'fra', True)\n",
    "# input_lang, output_lang, pairs = prepare_data('eng', 'cmn', True)\n",
    "input_lang, output_lang, pairs = prepare_data('r1', 'r2', True)\n",
    "input_lang.trim_vocab(MAX_VOCAB_DIM)\n",
    "output_lang.trim_vocab(MAX_VOCAB_DIM)\n",
    "\n",
    "# Print an example pair\n",
    "print(random.choice(pairs))\n",
    "a,b = random.choice(pairs)\n",
    "print a\n",
    "print b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning training data into Tensors/Variables\n",
    "\n",
    "To train we need to turn the sentences into something the neural network can understand, which of course means numbers. Each sentence will be split into words and turned into a Tensor, where each word is replaced with the index (from the Lang indexes made earlier). While creating these tensors we will also append the EOS token to signal that the sentence is over.\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)\n",
    "\n",
    "A Tensor is a multi-dimensional array of numbers, defined with some type e.g. FloatTensor or LongTensor. In this case we'll be using LongTensor to represent an array of integer indexes.\n",
    "\n",
    "Trainable PyTorch modules take Variables as input, rather than plain Tensors. A Variable is basically a Tensor that is able to keep track of the graph state, which is what makes autograd (automatic calculation of backwards gradients) possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a list of indexes, one for each word in the sentence\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return [lang.word2index[word] if word in lang.word2index else UNK_token for word in sentence.split(' ')]\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indexes_from_sentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "#     print('var =', var)\n",
    "    if USE_CUDA: var = var.cuda()\n",
    "    return var\n",
    "\n",
    "def variables_from_pair(pair):\n",
    "    input_variable = variable_from_sentence(input_lang, pair[0])\n",
    "    target_variable = variable_from_sentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(d_model, pos):\n",
    "    assert d_model % 2 == 0\n",
    "    pos = torch.tensor(pos, dtype=torch.float32, requires_grad=False)\n",
    "    pe = torch.zeros([1,d_model], dtype=torch.float32, requires_grad=False)\n",
    "    for i in range(d_model//2):\n",
    "        a = torch.tensor(10000, dtype=torch.float32, requires_grad=False)\n",
    "        b = torch.tensor(2.*i/float(d_model), dtype=torch.float32, requires_grad=False)\n",
    "        c = pos / torch.pow(a, b)\n",
    "        pe[0, 2*i] = torch.sin(c)\n",
    "        pe[0, 2*i+1] = torch.cos(c)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for every word from the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.d_model = hidden_size\n",
    "        self.init_pos_mat(MAX_LENGTH*2)\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "#         self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        \n",
    "    def forward(self, word_inputs, hidden):\n",
    "        # Note: we run this all at once (over the whole input sequence)\n",
    "        seq_len = len(word_inputs)\n",
    "        embedded = self.embedding(word_inputs).view(seq_len, 1, -1)\n",
    "        embedded = embedded + self.get_pos_mat(seq_len).view(seq_len, 1, -1)\n",
    "#         embedded = F.relu(self.linear(embedded))\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return embedded, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        if USE_CUDA: hidden = hidden.cuda()\n",
    "        return hidden\n",
    "    \n",
    "    def init_pos_mat(self, cache_length):\n",
    "        print('init postional matrix with length : %d ' % cache_length)\n",
    "        self.positional_matrix = torch.cat([positional_encoding(self.d_model, i) for i in range(0,cache_length)], dim=0)\n",
    "        self.positional_matrix.requires_grad = False\n",
    "        self.positional_matrix = self.positional_matrix.cuda()\n",
    "            \n",
    "        \n",
    "    def get_pos_mat(self, length):\n",
    "        if length > self.positional_matrix.shape[0]:\n",
    "            print('input sequence length reach positional matrix maximum length. %d ' % length)\n",
    "            ret = torch.cat([positional_encoding(self.d_model, i) for i in range(length)], dim=0)\n",
    "            ret.requires_grad = False\n",
    "            print('Increase positional matrix maximum length. %d ' % length)\n",
    "            self.positional_matrix = ret\n",
    "            self.positional_matrix = self.positional_matrix.cuda()\n",
    "            return ret\n",
    "        else:\n",
    "            return self.positional_matrix[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 120 \n",
      "torch.Size([3, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "encoder_test = EncoderRNN(10, 10, 2)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "print encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Bahdanau et al. model\n",
    "\n",
    "The attention model in [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) is described as the following series of equations.\n",
    "\n",
    "Each decoder output is conditioned on the previous outputs and some $\\mathbf x$, where $\\mathbf x$ consists of the current hidden state (which takes into account previous outputs) and the attention \"context\", which is calculated below. The function $g$ is a fully-connected layer with a nonlinear activation, which takes as input the values $y_{i-1}$, $s_i$, and $c_i$ concatenated.\n",
    "\n",
    "$$\n",
    "p(y_i \\mid \\{y_1,...,y_{i-1}\\},\\mathbf{x}) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "The current hidden state $s_i$ is calculated by an RNN $f$ with the last hidden state $s_{i-1}$, last decoder output value $y_{i-1}$, and context vector $c_i$.\n",
    "\n",
    "In the code, the RNN will be a `nn.GRU` layer, the hidden state $s_i$ will be called `hidden`, the output $y_i$ called `output`, and context $c_i$ called `context`.\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "The context vector $c_i$ is a weighted sum of all encoder outputs, where each weight $a_{ij}$ is the amount of \"attention\" paid to the corresponding encoder output $h_j$.\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} a_{ij} h_j\n",
    "$$\n",
    "\n",
    "... where each weight $a_{ij}$ is a normalized (over all steps) attention \"energy\" $e_{ij}$ ...\n",
    "\n",
    "$$\n",
    "a_{ij} = \\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T} exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "... where each attention energy is calculated with some function $a$ (such as another linear layer) using the last hidden state $s_{i-1}$ and that particular encoder output $h_j$:\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the Bahdanau et al. model\n",
    "\n",
    "In summary our decoder should consist of four main parts - an embedding layer turning an input word into a vector; a layer to calculate the attention energy per encoder output; a RNN layer; and an output layer.\n",
    "\n",
    "The decoder's inputs are the last RNN hidden state $s_{i-1}$, last output $y_{i-1}$, and all encoder outputs $h_*$.\n",
    "\n",
    "* embedding layer with inputs $y_{i-1}$\n",
    "    * `embedded = embedding(last_rnn_output)`\n",
    "* attention layer $a$ with inputs $(s_{i-1}, h_j)$ and outputs $e_{ij}$, normalized to create $a_{ij}$\n",
    "    * `attn_energies[j] = attn_layer(last_hidden, encoder_outputs[j])`\n",
    "    * `attn_weights = normalize(attn_energies)`\n",
    "* context vector $c_i$ as an attention-weighted average of encoder outputs\n",
    "    * `context = sum(attn_weights * encoder_outputs)`\n",
    "* RNN layer(s) $f$ with inputs $(s_{i-1}, y_{i-1}, c_i)$ and internal hidden state, outputting $s_i$\n",
    "    * `rnn_input = concat(embedded, context)`\n",
    "    * `rnn_output, rnn_hidden = rnn(rnn_input, last_hidden)`\n",
    "* an output layer $g$ with inputs $(y_{i-1}, s_i, c_i)$, outputting $y_i$\n",
    "    * `output = out(embedded, rnn_output, context)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Luong et al. model(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025) by Luong et al. describe a few more attention models that offer improvements and simplifications. They describe a few \"global attention\" models, the distinction between them being the way the attention scores are calculated.\n",
    "\n",
    "The general form of the attention calculation relies on the target (decoder) side hidden state and corresponding source (encoder) side state, normalized over all states to get values summing to 1:\n",
    "\n",
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "The specific \"score\" function that compares two states is either *dot*, a simple dot product between the states; *general*, a a dot product between the decoder hidden state and a linear transform of the encoder state; or *concat*, a dot product between a new parameter $v_a$ and a linear transform of the states concatenated together.\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s & dot \\\\\n",
    "h_t ^\\top \\textbf{W}_a \\bar h_s & general \\\\\n",
    "v_a ^\\top \\textbf{W}_a [ h_t ; \\bar h_s ] & concat\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The modular definition of these scoring functions gives us an opportunity to build specific attention module that can switch between the different score methods. The input to this module is always the hidden state (of the decoder RNN) and set of encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size, max_length=MAX_LENGTH):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_len = len(encoder_outputs)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(seq_len)) # B x 1 x S\n",
    "        if USE_CUDA: attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        for i in range(seq_len):\n",
    "            attn_energies[i] = self.score(hidden, encoder_outputs[i])\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x 1 x seq_len\n",
    "        return F.softmax(attn_energies, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "#             print hidden.shape, energy.shape, energy.transpose(0,1).shape, 'lolo'\n",
    "            energy = torch.mm(hidden,energy.transpose(0,1))\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a decoder that plugs this Attn module in after the RNN to calculate attention weights, and apply those weights to the encoder outputs to get a context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LinearAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(LinearAttnDecoderRNN, self).__init__()\n",
    "#         \n",
    "        self.d_model = hidden_size\n",
    "        self.init_pos_mat(MAX_LENGTH*2)\n",
    "        # Keep parameters for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, n_layers, dropout=dropout_p)\n",
    "#         self.linear = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.lin1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size * 2 , output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, last_context, last_hidden, encoder_emb, decoder_seq):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        seq_len, _ = decoder_seq.size()\n",
    "#         decoder_emb = self.embedding(decoder_seq).view(seq_len, 1, -1)\n",
    "        decoder_emb = self.embedding(decoder_seq).view(seq_len, 1, -1) + self.get_pos_mat(seq_len).view(seq_len, 1, -1)\n",
    "#         decoder_emb = F.relu(self.linear(decoder_emb))\n",
    "        word_embedded = decoder_emb[-1,:].view(1, 1, -1)\n",
    "        \n",
    "        seq_embedded = torch.cat([encoder_emb,decoder_emb], dim=0)\n",
    "        rnn_input = torch.cat((word_embedded, last_context.unsqueeze(0)), dim=2)\n",
    "        rnn_output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Combine embedded input word and last context, run through RNN\n",
    "#         hidden = F.relu(self.lin1(torch.cat([word_embedded, last_context], 1)))\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs; apply to encoder outputs\n",
    "        attn_weights = self.attn(rnn_output.squeeze(0), seq_embedded)\n",
    "        context = attn_weights.bmm(seq_embedded.transpose(0, 1)) # B x 1 x N\n",
    "        \n",
    "        word_embedded = word_embedded.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        \n",
    "        # Final output layer (next word prediction) using the RNN hidden state and context vector\n",
    "        output = F.log_softmax(self.out(torch.cat([context,word_embedded], dim=-1)), dim=-1)\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, context, hidden, attn_weights\n",
    "    \n",
    "    #     To speed up the positional encoding by construct an cache matrix. \n",
    "    def init_pos_mat(self, cache_length):\n",
    "        print('init postional matrix with length : %d ' % cache_length)\n",
    "        self.positional_matrix = torch.cat([positional_encoding(self.d_model, i) for i in range(0,cache_length)], dim=0)\n",
    "        self.positional_matrix.requires_grad = False\n",
    "        self.positional_matrix = self.positional_matrix.cuda()\n",
    "            \n",
    "        \n",
    "    def get_pos_mat(self, length):\n",
    "        if length > self.positional_matrix.shape[0]:\n",
    "            print('input sequence length reach positional matrix maximum length. %d ' % length)\n",
    "            ret = torch.cat([positional_encoding(self.d_model, i) for i in range(length)], dim=0)\n",
    "            ret.requires_grad = False\n",
    "            print('Increase positional matrix maximum length. %d ' % length)\n",
    "            self.positional_matrix = ret\n",
    "            self.positional_matrix = self.positional_matrix.cuda()\n",
    "            return ret\n",
    "        else:\n",
    "            return self.positional_matrix[:length]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the models\n",
    "\n",
    "To make sure the Encoder and Decoder model are working (and working together) we'll do a quick test with fake word inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 120 \n",
      "init postional matrix with length : 120 \n",
      "EncoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(10, 10, num_layers=2)\n",
      ")\n",
      "LinearAttnDecoderRNN(\n",
      "  (embedding): Embedding(10, 10)\n",
      "  (gru): GRU(20, 10, num_layers=2, dropout=0.1)\n",
      "  (out): Linear(in_features=20, out_features=10, bias=True)\n",
      "  (attn): Attn(\n",
      "    (attn): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "(torch.Size([1, 10]), torch.Size([1, 10]), torch.Size([1, 1, 6]), torch.Size([4, 1]))\n",
      "(torch.Size([1, 10]), torch.Size([1, 10]), torch.Size([1, 1, 7]), torch.Size([5, 1]))\n",
      "(torch.Size([1, 10]), torch.Size([1, 10]), torch.Size([1, 1, 8]), torch.Size([6, 1]))\n"
     ]
    }
   ],
   "source": [
    "encoder_test = EncoderRNN(10, 10, 2)\n",
    "decoder_test = LinearAttnDecoderRNN('general', 10, 10, 2)\n",
    "print(encoder_test)\n",
    "print(decoder_test)\n",
    "\n",
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "encoder_embs, encoder_hidden  = encoder_test(word_input, encoder_hidden)\n",
    "word_inputs = Variable(torch.LongTensor([[1], [2], [3]]))\n",
    "decoder_hidden = encoder_hidden\n",
    "decoder_context = Variable(torch.zeros(1, decoder_test.hidden_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "    decoder_context = decoder_context.cuda()\n",
    "\n",
    "for i in range(3):\n",
    "    decoder_output, decoder_context, decoder_hidden, decoder_attn = decoder_test(decoder_context, decoder_hidden, encoder_embs, word_inputs)\n",
    "    next_out = torch.topk(decoder_output, k=1, dim=1)[1]\n",
    "    word_inputs = torch.cat([word_inputs, next_out], dim=0)\n",
    "    print(decoder_output.size(), decoder_context.size(), decoder_attn.size(), word_inputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Defining a training iteration\n",
    "\n",
    "To train we first run the input sentence through the encoder word by word, and keep track of every output and the latest hidden state. Next the decoder is given the last hidden state of the decoder as its first hidden state, and the `<SOS>` token as its first input. From there we iterate to predict a next token from the decoder.\n",
    "\n",
    "### Teacher Forcing and Scheduled Sampling\n",
    "\n",
    "\"Teacher Forcing\", or maximum likelihood sampling, means using the real target outputs as each next input when training. The alternative is using the decoder's own guess as the next input. Using teacher forcing may cause the network to converge faster, but [when the trained network is exploited, it may exhibit instability](http://minds.jacobs-university.de/sites/default/files/uploads/papers/ESNTutorialRev.pdf).\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with coherent grammar but wander far from the correct translation - you could think of it as having learned how to listen to the teacher's instructions, without learning how to venture out on its own.\n",
    "\n",
    "The solution to the teacher-forcing \"problem\" is known as [Scheduled Sampling](https://arxiv.org/abs/1506.03099), which simply alternates between using the target values and predicted values when training. We will randomly choose to use teacher forcing with an if statement while training - sometimes we'll feed use real target as the input (ignoring the decoder's output), sometimes we'll use the decoder's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "\n",
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_embs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = decoder_seq = Variable(torch.LongTensor([[SOS_token]]))\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "#     decoder_hidden = encoder_hidden # Use last hidden state from encoder to start decoder\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_seq = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        \n",
    "        # Teacher forcing: Use the ground-truth target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_context, decoder_hidden, encoder_embs, decoder_seq)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            decoder_input = target_variable[di] # Next target is next input\n",
    "            decoder_seq = torch.cat([decoder_seq, decoder_input.view(1,1)], dim=0)\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use network's own prediction as the next input\n",
    "        for di in range(target_length):\n",
    "#             print 'context, embs, seq', decoder_context.shape, encoder_embs.shape, decoder_seq.shape\n",
    "            \n",
    "            decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_context, decoder_hidden, encoder_embs, decoder_seq)\n",
    "            loss += criterion(decoder_output, target_variable[di])\n",
    "            \n",
    "            # Get most likely word index (highest value) from output\n",
    "            topv, topi = decoder_output.data.topk(3)\n",
    "            for i in range(3):\n",
    "                ni = topi[0][i]\n",
    "                if ni != UNK_token:\n",
    "                    break\n",
    "        \n",
    "            decoder_input = Variable(torch.LongTensor([ni])) # Chosen word is next input\n",
    "            if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targets)\n",
    "            if ni == EOS_token: break\n",
    "            decoder_seq = torch.cat([decoder_seq, decoder_input.view(1,1)], dim=0)\n",
    "#     encoder_outputs = torch.cat([encoder_outputs, decoder_rnn_output], dim=0)\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    return loss.data.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally helper functions to print time elapsed and estimated time remaining, given the current time and progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    if percent == 0:\n",
    "        es = 0.\n",
    "    else:\n",
    "        es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training\n",
    "\n",
    "With everything in place we can actually initialize a network and start training.\n",
    "\n",
    "To start, we initialize models, optimizers, and a loss function (criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init postional matrix with length : 120 \n",
      "init postional matrix with length : 120 \n"
     ]
    }
   ],
   "source": [
    "attn_model = 'general'\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "dropout_p = 0.05\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers)\n",
    "decoder = LinearAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout_p=dropout_p)\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "learning_rate = 0.0001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then set up variables for plotting and tracking progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "n_epochs = 500000\n",
    "plot_every = 20\n",
    "print_every = 1000\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print_every\n",
    "plot_loss_total = 0 # Reset every plot_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually train, we call the train function many times, printing a summary as we go.\n",
    "\n",
    "*Note:* If you run this notebook you can train, interrupt the kernel, evaluate, and continue training later. You can comment out the lines above where the encoder and decoder are initialized (so they aren't reset) or simply run the notebook starting from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1002/500001 [01:11<11:05:40, 12.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 56s (- 969m 39s) (1000 0%) 5.4455\n",
      "> SOS 今晚 这样 下去 EOS m n nr uj NOP e NOE 4 NOR\n",
      "= SOS 一个 人 安静 的 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2002/500001 [02:25<10:38:36, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 11s (- 793m 30s) (2000 0%) 5.5726\n",
      "> SOS 用 脆弱 的 花蕊 EOS v v v f uj v NOP ei NOE 6 NOR\n",
      "= SOS 想 抗拒 绽放 后 的 枯萎 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3001/500001 [03:41<11:45:55, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4m 27s (- 737m 49s) (3000 0%) 5.5620\n",
      "> SOS 心里 拔 也 拔 不掉 的 一根 刺 EOS m d r v m NOP i NOE 5 NOR\n",
      "= SOS 一辈子 就 这么 耍赖 一次 EOS\n",
      "< SOS 我 的 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4001/500001 [04:55<9:53:14, 13.93it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 41s (- 705m 28s) (4000 0%) 5.4503\n",
      "> SOS 实现 实现 EOS d v NOP i NOE 2 NOR\n",
      "= SOS 似曾 相惜 EOS\n",
      "< SOS 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5000/500001 [06:10<11:59:33, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6m 56s (- 686m 55s) (5000 1%) 5.3753\n",
      "> SOS 别人 都 长大 了 EOS d v r m n v p t NOP en NOE 8 NOR\n",
      "= SOS 只 剩下 我 一个 人 住 在 凌晨 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6002/500001 [07:27<11:19:32, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8m 13s (- 677m 10s) (6000 1%) 5.4943\n",
      "> SOS 习惯 的 方式 慵懒 或 放纵 EOS v t uj r d d NOP ian NOE 6 NOR\n",
      "= SOS 让 过去 的 自己 暂时 先 EOS\n",
      "< SOS 我 的 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7001/500001 [08:43<10:00:13, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9m 28s (- 667m 39s) (7000 1%) 5.3587\n",
      "> SOS 听 过 天空 拒绝 飞鸟 EOS d v ug r NOP i NOE 4 NOR\n",
      "= SOS 没 听 过 你 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8002/500001 [09:58<9:38:38, 14.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10m 43s (- 659m 49s) (8000 1%) 5.3748\n",
      "> SOS 等待 愈 浓郁 神化 的 约会 EOS n n r a NOP i NOE 4 NOR\n",
      "= SOS 意象 色彩 多麼 精致 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9001/500001 [11:14<11:07:10, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12m 0s (- 655m 5s) (9000 1%) 5.5272\n",
      "> SOS 笑 我 太 不 值得 EOS p v m a NOP ong NOE 4 NOR\n",
      "= SOS 为了 换取 一点点 虚荣 EOS\n",
      "< SOS 你 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10002/500001 [12:31<9:05:17, 14.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13m 17s (- 651m 19s) (10000 2%) 5.4384\n",
      "> SOS 明天 起 开始 学习 EOS m n uj n d v r NOP i NOE 7 NOR\n",
      "= SOS 一个 人 的 世界 不再 需要 你 EOS\n",
      "< SOS 我 的 你 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11001/500001 [13:47<10:43:49, 12.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14m 33s (- 647m 11s) (11000 2%) 5.4617\n",
      "> SOS 时间 从不 停歇 EOS v r t NOP ian NOE 3 NOR\n",
      "= SOS 催促 我们 向前 EOS\n",
      "< SOS 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 12001/500001 [15:04<10:33:22, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15m 49s (- 643m 50s) (12000 2%) 5.4447\n",
      "> SOS 虽然 雨 的 步态 转 柔 EOS c r d v r a uj v uz n n uj n NOP in NOE 13 NOR\n",
      "= SOS 但是 你 仍然 听见 它 清朗 的 带 着 金属 韵律 的 步音 EOS\n",
      "< SOS 我 的 你 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13001/500001 [16:19<10:03:03, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17m 5s (- 640m 11s) (13000 2%) 5.5180\n",
      "> SOS 后悔 告诉 你 价钱 EOS r v v d v y NOP a NOE 6 NOR\n",
      "= SOS 你 欢喜 吃 就 吃 啦 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 14002/500001 [17:33<8:48:59, 15.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18m 19s (- 636m 10s) (14000 2%) 5.2961\n",
      "> SOS 见招拆招 时间 不 早 EOS m n n d r d a NOP ao NOE 7 NOR\n",
      "= SOS 两 人 行程 约 哪里 才 好 EOS\n",
      "< SOS 我 的 的 <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15001/500001 [18:46<8:38:23, 15.59it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19m 32s (- 631m 49s) (15000 3%) 5.1425\n",
      "> SOS 鹰 显然 是 想 挣扎 着 重返 蓝天 EOS d NOP u NOE 1 NOR\n",
      "= SOS 欲 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 16001/500001 [19:58<9:41:10, 13.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 44s (- 627m 14s) (16000 3%) 5.0384\n",
      "> SOS 後 悔 時 的 淚水 又 特別 讓 人 覺得 無力 疲憊 EOS a v d v a r v c r v ul r NOP ui NOE 12 NOR\n",
      "= SOS 舊 愛 還 是 最美 有時 分手 不是 誰 負 了 誰 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 17001/500001 [21:10<10:37:11, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21m 56s (- 623m 23s) (17000 3%) 5.0506\n",
      "> SOS 会 不会 随 落叶 变得 腐朽 EOS v c d v NOP ou NOE 4 NOR\n",
      "= SOS 放手 或 不 放手 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 18002/500001 [22:22<9:54:23, 13.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 7s (- 619m 22s) (18000 3%) 5.0981\n",
      "> SOS 我 只 想 为 你 再 努力 一点 EOS r v p n uj n p nr NOP ian NOE 8 NOR\n",
      "= SOS 我 要 把 梦想 的 歌唱 给 蓝天 EOS\n",
      "< SOS 我 的 你 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19002/500001 [23:33<8:19:52, 16.04it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24m 18s (- 615m 26s) (19000 3%) 5.0431\n",
      "> SOS 只有 我 深爱 的 这 季节 听到 EOS r n v v uj n NOP in NOE 6 NOR\n",
      "= SOS 我 泪水 涌出 划落 的 声音 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 20001/500001 [24:44<8:56:12, 14.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25m 30s (- 612m 2s) (20000 4%) 4.9986\n",
      "> SOS 数落 我 改不掉 习惯 怎么办 EOS t f m NOP iao NOE 3 NOR\n",
      "= SOS 明明 上 一秒 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 21002/500001 [25:55<9:20:05, 14.25it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26m 41s (- 608m 49s) (21000 4%) 5.0133\n",
      "> SOS 他 几乎 全用 在 自费 进修 英语 和 买 书上 EOS nr v v NOP ao NOE 3 NOR\n",
      "= SOS 杜厦 留心 到 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 22001/500001 [27:08<10:21:44, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27m 54s (- 606m 14s) (22000 4%) 5.1688\n",
      "> SOS 当再 经过 雪路 已 找 不到 EOS t r c uj n NOP i NOE 5 NOR\n",
      "= SOS 旧日 那 无言 的 女子 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 23001/500001 [28:22<11:15:32, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29m 7s (- 604m 8s) (23000 4%) 5.2522\n",
      "> SOS 满海 飘荡 找 不到 岸 EOS r ns zg v NOP ang NOE 4 NOR\n",
      "= SOS 这 夕阳 仍 盼望 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 24001/500001 [29:35<10:23:56, 12.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30m 20s (- 601m 49s) (24000 4%) 5.2216\n",
      "> SOS 青春 伴 着 青涩 EOS v NOP e NOE 1 NOR\n",
      "= SOS 合 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 25003/500001 [30:47<9:46:10, 13.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31m 33s (- 599m 31s) (25000 5%) 5.1629\n",
      "> SOS 是 你 抚养 我 长大 EOS v r v m n NOP ua NOE 5 NOR\n",
      "= SOS 陪 我 说 第一句 话 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 26001/500001 [31:59<9:04:21, 14.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32m 44s (- 596m 58s) (26000 5%) 5.1222\n",
      "> SOS 月亮 吗 出来 呀 照照 你 脸上 EOS e e y e NOP ai NOE 4 NOR\n",
      "= SOS 哎呀 嘿 呀 哎 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 27001/500001 [33:12<9:35:14, 13.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33m 58s (- 595m 4s) (27000 5%) 5.2615\n",
      "> SOS 我们 走 在 这条 漫长 的 路上 EOS i f uj n NOP u NOE 4 NOR\n",
      "= SOS 真不知道 前方 的 路 EOS\n",
      "< SOS 我 的 你 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 28003/500001 [34:24<9:27:20, 13.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35m 10s (- 592m 55s) (28000 5%) 5.1590\n",
      "> SOS 光 透过 玻璃窗 氤氲 在 脸上 EOS r n v uz r v r v uj n NOP ang NOE 10 NOR\n",
      "= SOS 那 光影 带 着 我 走向 你 沉醉 的 脸庞 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 29001/500001 [35:36<8:43:03, 15.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36m 22s (- 590m 46s) (29000 5%) 5.1196\n",
      "> SOS 我们 虽 不 在 同一个 地方 EOS v d uj n NOP ang NOE 4 NOR\n",
      "= SOS 没有 相同 的 主张 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30002/500001 [36:48<9:16:02, 14.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37m 34s (- 588m 33s) (30000 6%) 5.1031\n",
      "> SOS 心中 爱 你 的 那 盏灯 EOS p l d p r v NOP ie NOE 6 NOR\n",
      "= SOS 在 无意间 已 被 你 浇灭 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 31002/500001 [37:59<10:19:06, 12.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38m 45s (- 586m 20s) (31000 6%) 5.0582\n",
      "> SOS 也许 该 找 个 朋友 像 你 一样 不停 的 吵闹 EOS r r v v NOP ao NOE 4 NOR\n",
      "= SOS 为何 我 拼命 寻找 EOS\n",
      "< SOS 我 的 你 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 32001/500001 [39:10<10:30:19, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39m 56s (- 584m 10s) (32000 6%) 5.0615\n",
      "> SOS 努力 了 一阵 EOS d a uv v v NOP ou NOE 5 NOR\n",
      "= SOS 终于 疲乏 地 垂 下头 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 33002/500001 [40:24<9:19:05, 13.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41m 9s (- 582m 31s) (33000 6%) 5.1788\n",
      "> SOS 她 更 迷惑 了 EOS d v v f uj v n v n v ns c n NOP i NOE 13 NOR\n",
      "= SOS 不 知道 爱 上 的 是 雪景 是 诗词 是 中国 还是 男子 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 34001/500001 [41:35<9:44:45, 13.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42m 21s (- 580m 30s) (34000 6%) 4.9310\n",
      "> SOS 该 走 了 曾经 属于 我 的 情绪 竟然 都 失控 EOS d v v ul NOP e NOE 4 NOR\n",
      "= SOS 不得不 结束 了忘 了 EOS\n",
      "< SOS 我 我 的 的 的 的 <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 35002/500001 [42:49<8:06:07, 15.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43m 34s (- 579m 0s) (35000 7%) 5.2163\n",
      "> SOS 上网 也 混混 EOS v d v NOP un NOE 3 NOR\n",
      "= SOS 上床 也 混 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 36002/500001 [44:01<9:05:50, 14.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44m 47s (- 577m 17s) (36000 7%) 5.1427\n",
      "> SOS 每个 月 她 总会 有 那么 几天 EOS r v v n r d v v NOP an NOE 8 NOR\n",
      "= SOS 我 不能 有 杂念 什么 都 不能 干 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 37001/500001 [45:14<8:16:06, 15.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46m 0s (- 575m 41s) (37000 7%) 5.0916\n",
      "> SOS 我 知 过分 中意 枉 捉 错用 神 EOS v r d nr d yg q NOP en NOE 7 NOR\n",
      "= SOS 明知 你 最 後 亦 冇 份 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 38001/500001 [46:27<9:30:58, 13.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47m 12s (- 573m 59s) (38000 7%) 5.0973\n",
      "> SOS 我 梦见 我 穿堂 过 巷地 在 巴黎 EOS r p nr v ul nr n NOP ing NOE 7 NOR\n",
      "= SOS 我 在 香榭丽舍 买 了 蓝 风铃 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 39002/500001 [47:40<10:51:03, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48m 25s (- 572m 26s) (39000 7%) 5.0658\n",
      "> SOS 收割 我 的 爱 EOS z n d v t NOP ai NOE 5 NOR\n",
      "= SOS 鲜红 镰刀 正 挥舞 下来 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 40002/500001 [48:54<9:52:41, 12.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49m 40s (- 571m 11s) (40000 8%) 5.0990\n",
      "> SOS 我 真的 好 想 找 一条 船 EOS v d v r q nr NOP an NOE 6 NOR\n",
      "= SOS 能 远远 离开 这 片 沙滩 EOS\n",
      "< SOS 我 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 41002/500001 [50:09<9:40:55, 13.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50m 54s (- 569m 59s) (41000 8%) 5.1421\n",
      "> SOS 少林寺 美丽 辉煌 EOS m uj ns NOP i NOE 3 NOR\n",
      "= SOS 千年 的 古寺 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 42001/500001 [51:22<9:15:52, 13.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52m 8s (- 568m 32s) (42000 8%) 4.9880\n",
      "> SOS 学到 足 关帝 座镇 华容 EOS v a ng v v n NOP ong NOE 6 NOR\n",
      "= SOS 恨 满 胸 裹 蒸 粽 EOS\n",
      "< SOS 我 的 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 43001/500001 [52:34<9:57:49, 12.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53m 20s (- 566m 50s) (43000 8%) 5.0114\n",
      "> SOS 片片 回忆 活下去 EOS r n z n v NOP u NOE 5 NOR\n",
      "= SOS 任 时光 匆匆 流 去 EOS\n",
      "< SOS 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 44002/500001 [53:46<9:15:38, 13.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54m 32s (- 565m 11s) (44000 8%) 5.0146\n",
      "> SOS 一整夜 站立 在 窗口 EOS v v r i v r NOP o NOE 6 NOR\n",
      "= SOS 还好 有 这 万家灯火 陪 我 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 45002/500001 [55:01<8:35:05, 14.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55m 46s (- 563m 59s) (45000 9%) 5.0982\n",
      "> SOS 不是 我 跟 你 EOS n a NOP iao NOE 2 NOR\n",
      "= SOS 俗尘 渺渺 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 46002/500001 [56:14<9:40:41, 13.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57m 0s (- 562m 39s) (46000 9%) 5.1398\n",
      "> SOS 话到嘴边 又 咽下 EOS n r v p r v NOP iao NOE 6 NOR\n",
      "= SOS 妈妈 我 想 对 您 笑 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 47001/500001 [57:26<8:53:54, 14.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58m 11s (- 560m 54s) (47000 9%) 4.8779\n",
      "> SOS 你们 在 哪里 我 的 朋友 兄弟 EOS ns uj n v v r NOP i NOE 6 NOR\n",
      "= SOS 四海 的 朋友 感谢 有 你 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 48003/500001 [58:37<8:12:41, 15.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59m 23s (- 559m 12s) (48000 9%) 4.9694\n",
      "> SOS 害怕 的 你 不用 担心 我 哪里 都 不会 去 EOS v r uj v an uz r v r s NOP in NOE 10 NOR\n",
      "= SOS 没有 任何 的 委屈 温暖 着 你 直到 你 心 EOS\n",
      "< SOS 我 的 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 49002/500001 [59:48<9:25:32, 13.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60m 34s (- 557m 31s) (49000 9%) 5.0264\n",
      "> SOS 慈母 手中 线 游子 身上 衣 EOS v n a d v NOP ui NOE 5 NOR\n",
      "= SOS 临行 密密缝 意恐 迟迟 归 EOS\n",
      "< SOS 我 的 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 49999/500001 [1:01:00<8:26:28, 14.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61m 45s (- 555m 52s) (50000 10%) 5.0203\n",
      "> SOS 年轻 的 朋友 用 你 的 勇气 回答 我 EOS nr f s uj n d v v r NOP e NOE 9 NOR\n",
      "= SOS 阳光 下 心里 的 歌唱 不 出来 是 为什么 EOS\n",
      "< SOS 我 的 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 51002/500001 [1:02:13<8:58:55, 13.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62m 59s (- 554m 30s) (51000 10%) 5.0691\n",
      "> SOS 和 飘香 的 烟 EOS r t d v r m NOP ian NOE 6 NOR\n",
      "= SOS 每个 星期 都 盼望 这 一天 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 52001/500001 [1:03:25<8:59:13, 13.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64m 10s (- 552m 55s) (52000 10%) 4.9947\n",
      "> SOS 两 人 愿意 没 惆怅 EOS m n an v d n n p d NOP iang NOE 9 NOR\n",
      "= SOS 三 人 痛苦 恋爱 不再 问 事实 与 真相 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 53001/500001 [1:04:36<8:59:54, 13.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65m 22s (- 551m 22s) (53000 10%) 4.9895\n",
      "> SOS 鲜活 的 人呐 EOS d n c a NOP i NOE 4 NOR\n",
      "= SOS 从未 情欲 而 得体 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 54001/500001 [1:05:47<9:52:42, 12.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66m 33s (- 549m 43s) (54000 10%) 5.0043\n",
      "> SOS 你 靠 在 我 的 肩 你 的 笑 那么 甜 EOS d d ug ul m m NOP ian NOE 6 NOR\n",
      "= SOS 仿佛 已 过 了 好多好多 年 EOS\n",
      "< SOS 我 你 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 55001/500001 [1:07:04<10:14:06, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67m 49s (- 548m 49s) (55000 11%) 5.3808\n",
      "> SOS 让 往事 像 雾气 慢慢 地 蒸发 EOS v nr d d v f NOP ia NOE 6 NOR\n",
      "= SOS 等到 麻木 也许 就 放得 下 EOS\n",
      "< SOS 我 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 56002/500001 [1:08:20<9:52:05, 12.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69m 5s (- 547m 51s) (56000 11%) 5.4027\n",
      "> SOS 突然 好想你 EOS ad a uj v NOP i NOE 4 NOR\n",
      "= SOS 突然 锋利 的 回忆 EOS\n",
      "< SOS 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 57001/500001 [1:09:35<9:30:06, 12.95it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70m 20s (- 546m 45s) (57000 11%) 5.3789\n",
      "> SOS 萨嘎 拉 贝勒 佳纳 EOS d nrt ns NOP a NOE 3 NOR\n",
      "= SOS 尤 哈拉 佳雅达他嘎达雅 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 58001/500001 [1:10:52<9:40:33, 12.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71m 37s (- 545m 52s) (58000 11%) 5.3189\n",
      "> SOS 只 想 把 你 抱进 我 胸怀 EOS t d v v NOP ai NOE 4 NOR\n",
      "= SOS 夏天 就是 要 恋爱 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 59001/500001 [1:12:08<9:12:23, 13.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72m 53s (- 544m 52s) (59000 11%) 5.3638\n",
      "> SOS 这 绿岛 像 一只 船 EOS p m t v y v NOP ao NOE 6 NOR\n",
      "= SOS 在 月 夜里 摇 啊 摇 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 60001/500001 [1:13:23<9:26:53, 12.94it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74m 9s (- 543m 47s) (60000 12%) 5.3519\n",
      "> SOS 为 祖国 贡献 出 青春 和 力量 EOS nr zg NOP a NOE 2 NOR\n",
      "= SOS 啊啊啊 啊 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 61002/500001 [1:14:40<8:34:16, 14.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75m 26s (- 542m 55s) (61000 12%) 5.3869\n",
      "> SOS 为什么 又 是 下 一个 路口 EOS c r d v NOP ou NOE 4 NOR\n",
      "= SOS 但是 我们 依然 牵着手 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 62001/500001 [1:15:56<9:08:01, 13.32it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76m 42s (- 541m 53s) (62000 12%) 5.3803\n",
      "> SOS 俯身 向 大地 背 朝着 蓝天 EOS s n uj n y r d uj n NOP ian NOE 9 NOR\n",
      "= SOS 心中 点点 的 云 啊 我 默默 的 念 EOS\n",
      "< SOS 我 你 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 63000/500001 [1:17:12<9:58:47, 12.16it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77m 58s (- 540m 50s) (63000 12%) 5.3711\n",
      "> SOS 我 不要 只是 你 的 小 木偶 EOS r r q r p r v NOP ong NOE 7 NOR\n",
      "= SOS 每天 每 分 每秒 被 你 操纵 EOS\n",
      "< SOS 我 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 64001/500001 [1:18:29<8:51:24, 13.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79m 15s (- 539m 56s) (64000 12%) 5.3166\n",
      "> SOS 两个 人 的 一座 城 EOS r p ns r p ns NOP ong NOE 6 NOR\n",
      "= SOS 你 在 城西 他 在 城东 EOS\n",
      "< SOS 我 的 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 65001/500001 [1:19:45<9:07:28, 13.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80m 30s (- 538m 48s) (65000 13%) 5.3185\n",
      "> SOS 一 是 婴儿 哭啼 二 是 学 游戏 EOS m v ns n m v d v r NOP i NOE 9 NOR\n",
      "= SOS 三 是 青春 物语 四 是 碰巧 遇见 你 EOS\n",
      "< SOS 我 我 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 66001/500001 [1:21:01<9:41:04, 12.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81m 47s (- 537m 48s) (66000 13%) 5.3354\n",
      "> SOS 有 一天 你 会 懂得 怎么样 去 忘记 他 EOS r v v v r uj n y NOP a NOE 8 NOR\n",
      "= SOS 你 会 遇到 爱 你 的 人 啊 EOS\n",
      "< SOS 我 的 你 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 67002/500001 [1:22:18<9:43:25, 12.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83m 4s (- 536m 50s) (67000 13%) 5.4196\n",
      "> SOS 喜欢 你 说不出口 EOS a v p r v NOP ai NOE 5 NOR\n",
      "= SOS 好 想 跟 你 表白 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 68001/500001 [1:23:35<10:10:23, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84m 20s (- 535m 51s) (68000 13%) 5.2939\n",
      "> SOS 点燃 EOS a uj n NOP eng NOE 3 NOR\n",
      "= SOS 潮湿 的 风 EOS\n",
      "< SOS 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 69002/500001 [1:24:51<8:28:16, 14.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85m 37s (- 534m 47s) (69000 13%) 5.3063\n",
      "> SOS 有 办法 没 出轨 先 怨 后悔 EOS d v r r v r v a NOP ei NOE 8 NOR\n",
      "= SOS 都 是 谁 你 怪 他 看 般配 EOS\n",
      "< SOS 我 的 你 的 你 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 70002/500001 [1:26:07<10:12:21, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86m 53s (- 533m 43s) (70000 14%) 5.3098\n",
      "> SOS 海边 的 晚风 吹 EOS v v r v NOP ui NOE 4 NOR\n",
      "= SOS 好像 是 你 相随 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 71000/500001 [1:27:23<8:57:22, 13.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88m 8s (- 532m 36s) (71000 14%) 5.2632\n",
      "> SOS 下课铃 狂响 只有 先生 慢动作 EOS n v uz v n uj n NOP e NOE 7 NOR\n",
      "= SOS 苹果 忙 着 配合 离心力 的 守则 EOS\n",
      "< SOS 我 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 72002/500001 [1:28:38<8:42:09, 13.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89m 24s (- 531m 27s) (72000 14%) 5.2333\n",
      "> SOS 那 是 说书 人口 中 海市蜃楼 EOS nrt NOP an NOE 1 NOR\n",
      "= SOS 特曼 EOS\n",
      "< SOS 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 73001/500001 [1:29:55<10:17:44, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90m 40s (- 530m 24s) (73000 14%) 5.3715\n",
      "> SOS 献给 有 梦 的 嗯 嘛 EOS r n a e y e y e y y NOP a NOE 10 NOR\n",
      "= SOS 我 嘴唇 温热 嗯 嘛 嗯 嘛 嗯 嘛 嘛 EOS\n",
      "< SOS 你 的 你 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 74002/500001 [1:31:11<8:54:48, 13.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91m 56s (- 529m 19s) (74000 14%) 5.3043\n",
      "> SOS 离开 的 理由 我 只 需要 一个 EOS c f v v m m d v c p r NOP i NOE 11 NOR\n",
      "= SOS 无论 以后 会 有 多少 个 不 舍 但是 为 你 EOS\n",
      "< SOS 我 的 你 的 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 75000/500001 [1:32:29<9:46:04, 12.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93m 15s (- 528m 25s) (75000 15%) 5.3302\n",
      "> SOS 我 交给 你 你 一起 EOS n v a n NOP i NOE 4 NOR\n",
      "= SOS 收腹 兼 挺挺 胸肌 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 76002/500001 [1:33:47<9:07:56, 12.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94m 32s (- 527m 28s) (76000 15%) 5.3161\n",
      "> SOS 经过 一排排 凋零 的 树 EOS v ns c a NOP u NOE 4 NOR\n",
      "= SOS 还有 青春 和 孤独 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 77001/500001 [1:35:03<9:40:37, 12.14it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95m 48s (- 526m 21s) (77000 15%) 5.3401\n",
      "> SOS 我 的 鼻梁 已经 被 晒伤 我 的 头皮 也 开始 发烫 EOS r v r n z n d mq v f uj a n NOP ie NOE 13 NOR\n",
      "= SOS 我 爱 这 油箱 满满的 感觉 随时 一脚 甩掉 身后 的 烦恼 世界 EOS\n",
      "< SOS 我 你 的 你 的 的 的 的 <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 78002/500001 [1:36:20<13:46:23,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97m 6s (- 525m 22s) (78000 15%) 5.2215\n",
      "> SOS 等待 谢幕 的 掌声 EOS m n v uz n NOP in NOE 5 NOR\n",
      "= SOS 第九号 协奏曲 吟唱 着 伤心 EOS\n",
      "< SOS 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 79002/500001 [1:37:36<8:32:29, 13.69it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98m 21s (- 524m 11s) (79000 15%) 5.2674\n",
      "> SOS 不禁 要 承认 我 EOS v t r v uj n NOP en NOE 6 NOR\n",
      "= SOS 像 往日 我 恨透 的 罪人 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 80001/500001 [1:38:51<8:23:59, 13.89it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99m 37s (- 523m 1s) (80000 16%) 5.3104\n",
      "> SOS 后者 是 一位 饱经沧桑 的 艺术家 发自 心灵 的 叹息 EOS d v nr NOP e NOE 3 NOR\n",
      "= SOS 都 是 莫扎特 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 81001/500001 [1:40:07<9:20:47, 12.45it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100m 52s (- 521m 50s) (81000 16%) 5.1889\n",
      "> SOS 蓝蓝的 夜 蓝蓝的 梦 EOS v r p r uj n f v v NOP ai NOE 9 NOR\n",
      "= SOS 请 你 从 我 的 梦 中 走 出来 EOS\n",
      "< SOS 我 在 等 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 82002/500001 [1:41:23<9:16:16, 12.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102m 8s (- 520m 41s) (82000 16%) 5.2994\n",
      "> SOS 听 那 风声 听 那 雨声 你 的 歌声 EOS r v uj m ns a v NOP en NOE 7 NOR\n",
      "= SOS 我们 轮回 的 一生 新旧 难 分 EOS\n",
      "< SOS 我 是 我 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 83001/500001 [1:42:38<9:34:41, 12.09it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103m 24s (- 519m 32s) (83000 16%) 5.3122\n",
      "> SOS 都 爱 过 有 的 人 爱 的 像是 一个 观众 EOS d n uj n n d v NOP ong NOE 7 NOR\n",
      "= SOS 最 基本 的 感情 戏 也 看不懂 EOS\n",
      "< SOS 我 的 的 的 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 84001/500001 [1:43:55<10:06:23, 11.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104m 40s (- 518m 24s) (84000 16%) 5.3479\n",
      "> SOS 失去 多久 才 够 接受 爱 的 残酷 EOS r r v v ul r n NOP u NOE 7 NOR\n",
      "= SOS 为何 你 要 放弃 了 这 全部 EOS\n",
      "< SOS 我 的 你 的 你 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 85001/500001 [1:45:12<9:57:50, 11.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105m 57s (- 517m 21s) (85000 17%) 5.2676\n",
      "> SOS 我会 把 心情 整理 一遍 EOS t n v v d a NOP uan NOE 6 NOR\n",
      "= SOS 明天 用心 去 想 就 遥远 EOS\n",
      "< SOS 我 的 我 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 87002/500001 [1:47:45<7:49:18, 14.67it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108m 31s (- 515m 10s) (87000 17%) 5.3484\n",
      "> SOS 叩 叩 EOS vg vg vg vg NOP ou NOE 4 NOR\n",
      "= SOS 叩 叩 叩 叩 EOS\n",
      "< SOS 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 88002/500001 [1:49:03<9:39:11, 11.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109m 48s (- 514m 6s) (88000 17%) 5.2489\n",
      "> SOS 向 快乐 出发 别 害怕 EOS a d v s a uj n NOP ia NOE 7 NOR\n",
      "= SOS 幸福 就 像 天边 灿烂 的 晚霞 EOS\n",
      "< SOS 我 你 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 89002/500001 [1:50:19<8:34:42, 13.31it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111m 4s (- 512m 57s) (89000 17%) 5.3506\n",
      "> SOS 这 一年 夏天 有 最 温暖 的 目光 EOS n uj s r v uj nr NOP uang NOE 7 NOR\n",
      "= SOS 记忆 的 远方 我 披戴 的 荣光 EOS\n",
      "< SOS 我 的 我 的 的 EOS EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 90001/500001 [1:51:36<8:39:00, 13.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112m 22s (- 511m 53s) (90000 18%) 5.2737\n",
      "> SOS 不 知道 你 是不是 在乎 EOS a v p m v n NOP u NOE 6 NOR\n",
      "= SOS 明 知道 在 一起 是 错误 EOS\n",
      "< SOS 我 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 92001/500001 [1:54:34<11:52:29,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115m 20s (- 511m 29s) (92000 18%) 5.2383\n",
      "> SOS 是 你 的 体贴 让 我 再次 热烈 EOS v r uj m a vn n NOP ue NOE 7 NOR\n",
      "= SOS 是 你 的 万种 柔情 融化 冰雪 EOS\n",
      "< SOS 我 的 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 93002/500001 [1:56:04<10:10:18, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116m 50s (- 511m 18s) (93000 18%) 5.2909\n",
      "> SOS 今天 又 不见 EOS d v v NOP ian NOE 3 NOR\n",
      "= SOS 只 剩下 怀念 EOS\n",
      "< SOS 我 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 94001/500001 [1:57:34<10:50:43, 10.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118m 20s (- 511m 6s) (94000 18%) 5.3103\n",
      "> SOS 梦醒 推门 他 依旧 不 在 EOS v n n r n ns NOP iang NOE 6 NOR\n",
      "= SOS 闻 茶香 思念 那 油菜花 香 EOS\n",
      "< SOS 我 的 的 的 的 EOS <EOS>\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 94089/500001 [1:57:41<8:29:30, 13.28it/s] "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Begin!\n",
    "with tqdm(total=n_epochs + 1) as pbar:\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        # Get training data for this cycle\n",
    "        training_pair = variables_from_pair(random.choice(pairs))\n",
    "        input_variable = training_pair[0]\n",
    "        target_variable = training_pair[1]\n",
    "        # Run the train function\n",
    "        loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "\n",
    "        # Keep track of loss\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch == 0: continue\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = '%s (%d %d%%) %.4f' % (time_since(start, float(epoch) / n_epochs), epoch, float(epoch) / n_epochs * 100, print_loss_avg)\n",
    "            print(print_summary)\n",
    "            evaluate_randomly()\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting training loss\n",
    "\n",
    "Plotting is done with matplotlib, using the array `plot_losses` that was created while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(encoder,'./models/encoder-linear.pl')\n",
    "# torch.save(decoder,'./models/decoder-linear.pl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = torch.load('./models/encoder-linear.pl')\n",
    "# decoder = torch.load('./models/decoder-linear.pl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the network\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets. Instead we always feed the decoder's predictions back to itself. Every time it predicts a word, we add it to the output string. If it predicts the EOS token we stop there. We also store the decoder's attention outputs for each step to display later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = variable_from_sentence(input_lang, sentence)\n",
    "    input_length = input_variable.size()[0]\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_embs, encoder_hidden  = encoder(input_variable.cuda(), encoder_hidden.cuda())\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = decoder_seq = Variable(torch.LongTensor([[SOS_token]])) # SOS\n",
    "    decoder_context = Variable(torch.zeros(1, decoder.hidden_size))\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_seq = decoder_input.cuda()\n",
    "        decoder_context = decoder_context.cuda()\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, 2*max_length)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_context, decoder_hidden, decoder_attention = decoder(decoder_context, decoder_hidden, encoder_embs, decoder_seq)\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(3)\n",
    "        for i in range(3):\n",
    "            ni = topi[0][i]\n",
    "            if ni != UNK_token:\n",
    "                break\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni.item()])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([[ni]]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "        decoder_seq = torch.cat([decoder_seq, decoder_input], dim=0)\n",
    "        \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_embs)+di+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the input, target, and output to make some subjective quality judgements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    pair = random.choice(pairs)\n",
    "    \n",
    "    output_words, decoder_attn = evaluate(pair[0])\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    \n",
    "#     print('>', pair[0])\n",
    "#     print('=', pair[1])\n",
    "#     print('<', output_sentence)\n",
    "#     print('')\n",
    "    print '>', pair[0]\n",
    "    print '=', pair[1]\n",
    "    print '<', output_sentence\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SOS 你 总 流连 在 我 脑海 EOS v r v v NOP ai NOE 4 NOR\n",
      "= SOS 眷恋 你 恋成 依赖 EOS\n",
      "< 搭建 大好河山 受不起 尽而 多位 配有 夜未央 造物 右脑 物资 旧地 共同 估 弹断 下世纪 奇花异草 轉過 缘何 必需品 名堂 内个 拼夜 纤夫 引诱 躲不开 旧地 共同 右脑 第几 油菜花 待风 八朵 那丽桑 先验 见招拆招 浪接浪 螳螂 守恒 诃 会晤 看得出来 香露 滴滴 同行者 沙丘 二分之一 迦罗帝夷 看轻 青筋 旧地 炮弹 不矜不伐 制胜 不矜不伐 制胜 北岛 路有平 自怨 要诀 明显\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_randomly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing attention\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine looking where the network is focused most at each time step.\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output displayed as a matrix, with the columns being input steps and rows being output steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 torch.Size([6, 21])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5eb103fbd0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_words, attentions = evaluate(u\"SOS 所以 我 好 愿意 EOS v r l NOP i NOE 3 NOR\")\n",
    "print len(output_words), attentions.shape\n",
    "plt.matshow(attentions.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better viewing experience we will do the extra work of adding axes and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "zhfont = matplotlib.font_manager.FontProperties(fname=\"/home/k123/simhei.ttf\")\n",
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') + ['<EOS>'] + output_words, rotation=90, fontproperties=zhfont)\n",
    "    ax.set_yticklabels([''] + output_words, fontproperties=zhfont)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    print 'input =', input_sentence\n",
    "    print 'output =', ' '.join(output_words)\n",
    "    print attentions.shape\n",
    "    show_attention(input_sentence, output_words, attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = SOS 男 我 的 爱 EOS b r uj n NOP ai NOE 4 NOR\n",
      "output = SOS 女 我 的 爱 EOS <EOS>\n",
      "torch.Size([7, 23])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pair = random.choice(pairs)\n",
    "evaluate_and_show_attention(pair[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = elle a cinq ans de moins que moi .\n",
      "output = SOS 下 个 上 中 谁 能 EOS <EOS>\n",
      "torch.Size([9, 19])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_and_show_attention(\"elle a cinq ans de moins que moi .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"elle est trop petit .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"je ne crains pas de mourir .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"c est un jeune directeur plein de talent .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "* Try with a different dataset\n",
    "    * Another language pair\n",
    "    * Human &rarr; Machine (e.g. IOT commands)\n",
    "    * Chat &rarr; Response\n",
    "    * Question &rarr; Answer\n",
    "* Replace the embedding pre-trained word embeddings such as word2vec or GloVe\n",
    "* Try with more layers, more hidden units, and more sentences. Compare the training time and results.\n",
    "* If you use a translation file where pairs have two of the same phrase (`I am test \\t I am test`), you can use this as an autoencoder. Try this:\n",
    "    * Train as an autoencoder\n",
    "    * Save only the Encoder network\n",
    "    * Train a new Decoder for translation from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2.7.14 (conda)",
   "language": "python",
   "name": "python-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
